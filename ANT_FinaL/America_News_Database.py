import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import concurrent.futures
import threading
import os
import gc
from datetime import datetime, timedelta
import sys

# ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnicodeEncodeError (‡∏Ç‡πâ‡∏≤‡∏°‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö)
sys.stdout.reconfigure(encoding="utf-8", errors="ignore")

# ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå (‡∏õ‡∏£‡∏±‡∏ö `..` ‡∏ï‡∏≤‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå)
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..")) 

# ‚úÖ URL ‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß
base_url = "https://www.investing.com/news/stock-market-news"
output_filename = os.path.join(BASE_DIR, "Investing_Folder", "USA_News.csv")

# ‚úÖ Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Chrome instance
driver_lock = threading.Lock()

# ‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô"
yesterday = (datetime.now() - timedelta(days=1)).date()

def init_driver():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Chrome driver instance ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"""
    with driver_lock:
        options = uc.ChromeOptions()
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--blink-settings=imagesEnabled=false")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-extensions")
        driver = uc.Chrome(options=options)
    return driver

def close_popup(driver):
    """‡∏õ‡∏¥‡∏î popup ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ"""
    try:
        close_button = WebDriverWait(driver, 3).until(
            EC.element_to_be_clickable((By.XPATH, "//svg[@data-test='sign-up-dialog-close-button']"))
        )
        close_button.click()
        print("‚úÖ Popup ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    except Exception:
        pass

def scrape_news(driver):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"""
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    articles = soup.find_all('article', {'data-test': 'article-item'})
    news_list = []
    
    for article in articles:
        title_tag = article.find('a', {'data-test': 'article-title-link'})
        title = title_tag.get_text(strip=True) if title_tag else 'No Title'
        link = title_tag.get("href", "No Link")  # ‚úÖ ‡πÉ‡∏ä‡πâ .get() ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô KeyError

        description_tag = article.find('p', {'data-test': 'article-description'})
        description = description_tag.get_text(strip=True) if description_tag else 'No Description'

        date_tag = article.find('time', {'data-test': 'article-publish-date'})
        date_str = date_tag.get("datetime", "No Date")  # ‚úÖ ‡πÉ‡∏ä‡πâ .get() ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô KeyError

        news_list.append({'title': title, 'link': link, 'description': description, 'date': date_str})

    return news_list

def safe_quit(driver):
    """ ‡∏õ‡∏¥‡∏î WebDriver ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ `WinError 6` """
    if driver:
        try:
            driver.quit()
            if driver.service:
                driver.service.stop()  # ‚úÖ ‡∏õ‡∏¥‡∏î WebDriver service
            del driver
            gc.collect()  # ‚úÖ ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
            print("‚úÖ WebDriver ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: WebDriver ‡∏õ‡∏¥‡∏î‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: {e}")

scraped_pages = set()  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ global ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡πâ‡∏≥

def scrape_page(page):
    """Scrape ‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"""
    global scraped_pages

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
    if page in scraped_pages:
        print(f"‚ö†Ô∏è ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page} ‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡πâ‡∏≤‡∏°...")
        return []

    driver = None
    try:
        driver = init_driver()
        driver.get(f"{base_url}/{page}" if page > 1 else base_url)

        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "article")))
        close_popup(driver)
        news = scrape_news(driver)

        print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page} ‡πÑ‡∏î‡πâ {len(news)} ‡∏Ç‡πà‡∏≤‡∏ß")
        scraped_pages.add(page)  # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡πÅ‡∏•‡πâ‡∏ß
        return news

    except Exception as e:
        print(f"‚ùå Error ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page}: {e}")
        return []

    finally:
        if driver:
            safe_quit(driver)

def save_to_csv(data, filename, write_header=False):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á CSV ‡πÇ‡∏î‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥"""
    if not data:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà ‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV")
        return
    
    df_new = pd.DataFrame(data)

    # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥
    if os.path.exists(filename):
        df_existing = pd.read_csv(filename)
        
        # ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô KeyError
        if 'link' in df_existing.columns:
            existing_links = set(df_existing['link'].astype(str))
            df_new = df_new[~df_new['link'].astype(str).isin(existing_links)]
        else:
            existing_links = set()

        print(f"üõë ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ {len(existing_links)} ‡∏Ç‡πà‡∏≤‡∏ß, ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")

    # ‚úÖ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    if not df_new.empty:
        mode = 'w' if write_header else 'a'
        header = True if write_header else False
        df_new.to_csv(filename, index=False, encoding='utf-8', mode=mode, header=header)
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß {len(df_new)} ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏á CSV (mode={mode})")
    else:
        print("‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")


def clean_csv(filename):
    """‡∏•‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å CSV"""
    if not os.path.exists(filename):
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå CSV ‡πÉ‡∏´‡πâ clean")
        return

    if not os.path.exists(filename):
        print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå {filename} ‡πÉ‡∏´‡πâ clean")
        return

    df = pd.read_csv(filename)

    if df.empty:
        print("‚ö†Ô∏è ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤, ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ clean")
        return

    df['date'] = pd.to_datetime(df['date'], errors='coerce')

    # ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô clean
    total_before_clean = len(df)

    # ‚úÖ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏Å‡∏•‡∏∏‡πà‡∏°
    df_yesterday = df[df['date'].dt.date == yesterday]  # ‡∏Ç‡πà‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô
    df_old_news = df[df['date'].dt.date < yesterday]    # ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô
    df_today_news = df[df['date'].dt.date > yesterday]  # ‡∏Ç‡πà‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ

    # ‚úÖ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏•‡∏ö
    deleted_news = len(df_old_news) + len(df_today_news)

    # ‚úÖ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
    total_after_clean = len(df_yesterday)

    df_yesterday.to_csv(filename, index=False)

    print(f"\nüîç **Clean CSV Summary** üîç")
    print(f"üìä ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô Clean: {total_before_clean} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"üóëÔ∏è ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏•‡∏ö (‡πÄ‡∏Å‡πà‡∏≤ + ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ): {deleted_news} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (‡∏Ç‡∏≠‡∏á {yesterday}): {total_after_clean} ‡∏Ç‡πà‡∏≤‡∏ß")

def main():
    if os.path.exists(output_filename):
        os.remove(output_filename)  # ‚úÖ ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô

    batch_size = 5  # ‚úÖ ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô thread ‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Chrome crash
    max_pages = 7499
    all_news = []
    is_first_save = True
    stop_scraping = False  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° flag ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß
    total_articles = 0  # ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

    with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
        futures = []
        for page in range(1, max_pages + 1):
            if stop_scraping:
                break  # ‚úÖ ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô

            futures.append(executor.submit(scrape_page, page))
            
            if len(futures) == batch_size or page == max_pages:
                for future in concurrent.futures.as_completed(futures):
                    result = future.result()
                    all_news.extend(result)

                    for item in result:
                        try:
                            news_date = datetime.strptime(item['date'], "%Y-%m-%d %H:%M:%S").date()
                        except (ValueError, TypeError):
                            news_date = None  # ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà

                        if news_date and news_date < yesterday:
                            print(f"‚èπÔ∏è ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ {yesterday}, ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡∏ô‡∏ó‡∏µ")
                            save_to_csv(all_news, output_filename, write_header=is_first_save)
                            stop_scraping = True
                            break

                save_to_csv(all_news, output_filename, write_header=is_first_save)
                total_articles += len(all_news)
                is_first_save = False
                all_news = []
                futures = []

    clean_csv(output_filename)

if __name__ == "__main__":
    main()
