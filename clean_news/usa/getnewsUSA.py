import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import concurrent.futures
import threading
import os
import gc
from datetime import datetime, timedelta
import sys

# ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnicodeEncodeError (‡∏Ç‡πâ‡∏≤‡∏°‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö)
sys.stdout.reconfigure(encoding="utf-8", errors="ignore")


# ‚úÖ ‡πÉ‡∏ä‡πâ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÅ‡∏ó‡∏ô BASE_DIR
CURRENT_DIR = os.getcwd()

# ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå "Investing_Folder" ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
News_FOLDER = os.path.join(CURRENT_DIR, "News")
os.makedirs(News_FOLDER, exist_ok=True)

# ‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå CSV
output_filename = os.path.join(News_FOLDER, "USA_News.csv")

print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà: {output_filename}")

# ‚úÖ URL ‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß
base_url = "https://www.investing.com/news/stock-market-news"

# ‚úÖ Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Chrome instance
driver_lock = threading.Lock()

def get_latest_news_date_from_csv():

    last_news_file = os.path.join("Combined_News.csv")
    df = pd.read_csv(last_news_file)
    latest_date = df['date'].max()
    if latest_date:
        latest_date = datetime.strptime(latest_date, "%Y-%m-%d %H:%M:%S").date()
        print(f"üóìÔ∏è ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {latest_date}")
        return latest_date
    else:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 7 ‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô")
        return datetime.now().date() - timedelta(days=7)  # ‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 7 ‡∏ß‡∏±‡∏ô

def init_driver():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Chrome driver instance ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"""
    with driver_lock:
        options = uc.ChromeOptions()
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--blink-settings=imagesEnabled=false")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-extensions")
        driver = uc.Chrome(options=options)
    return driver

def close_popup(driver):
    """‡∏õ‡∏¥‡∏î popup ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ"""
    try:
        close_button = WebDriverWait(driver, 3).until(
            EC.element_to_be_clickable((By.XPATH, "//svg[@data-test='sign-up-dialog-close-button']"))
        )
        close_button.click()
        print("‚úÖ Popup ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    except Exception:
        pass

def scrape_news(driver):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"""
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    articles = soup.find_all('article', {'data-test': 'article-item'})
    news_list = []
    
    for article in articles:
        title_tag = article.find('a', {'data-test': 'article-title-link'})
        title = title_tag.get_text(strip=True) if title_tag else 'No Title'
        link = title_tag.get("href", "No Link")

        description_tag = article.find('p', {'data-test': 'article-description'})
        description = description_tag.get_text(strip=True) if description_tag else 'No Description'

        date_tag = article.find('time', {'data-test': 'article-publish-date'})
        date_str = date_tag.get("datetime", "No Date")

        news_list.append({'title': title, 'link': link, 'description': description, 'date': date_str})

    return news_list

def safe_quit(driver):
    """‡∏õ‡∏¥‡∏î WebDriver ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"""
    if driver:
        try:
            driver.quit()
            if driver.service:
                driver.service.stop()
            del driver
            gc.collect()
            print("‚úÖ WebDriver ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: WebDriver ‡∏õ‡∏¥‡∏î‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: {e}")

scraped_pages = set()

def scrape_page(page):
    """Scrape ‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"""
    global scraped_pages

    if page in scraped_pages:
        print(f"‚ö†Ô∏è ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page} ‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡πâ‡∏≤‡∏°...")
        return []

    driver = None
    try:
        driver = init_driver()
        driver.get(f"{base_url}/{page}" if page > 1 else base_url)

        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "article")))
        close_popup(driver)
        news = scrape_news(driver)

        print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page} ‡πÑ‡∏î‡πâ {len(news)} ‡∏Ç‡πà‡∏≤‡∏ß")
        scraped_pages.add(page)
        return news

    except Exception as e:
        print(f"‚ùå Error ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page}: {e}")
        return []

    finally:
        if driver:
            safe_quit(driver)

def save_to_csv(data, filename, write_header=False):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á CSV ‡πÇ‡∏î‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥"""
    if not data:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà ‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV")
        return
    
    df_new = pd.DataFrame(data)

    if os.path.exists(filename):
        df_existing = pd.read_csv(filename)
        if 'link' in df_existing.columns:
            existing_links = set(df_existing['link'].astype(str))
            df_new = df_new[~df_new['link'].astype(str).isin(existing_links)]
        else:
            existing_links = set()

        print(f"üõë ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ {len(existing_links)} ‡∏Ç‡πà‡∏≤‡∏ß, ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")

    if not df_new.empty:
        mode = 'w' if write_header else 'a'
        header = True if write_header else False
        df_new.to_csv(filename, index=False, encoding='utf-8', mode=mode, header=header)
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß {len(df_new)} ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏á CSV (mode={mode})")
    else:
        print("‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô Database"""
    latest_date = get_latest_news_date_from_csv()
    batch_size = 5
    max_pages = 7499
    all_news = []
    is_first_save = True
    stop_scraping = False
    total_articles = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
        futures = []
        for page in range(1, max_pages + 1):
            if stop_scraping:
                break

            futures.append(executor.submit(scrape_page, page))

            if len(futures) == batch_size or page == max_pages:
                for future in concurrent.futures.as_completed(futures):
                    result = future.result()
                    all_news.extend(result)

                    for item in result:
                        try:
                            news_date = datetime.strptime(item['date'], "%Y-%m-%d %H:%M:%S").date()
                        except (ValueError, TypeError):
                            news_date = None

                        if news_date and news_date <= latest_date:
                            print(f"‚èπÔ∏è ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ({latest_date}), ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡∏ô‡∏ó‡∏µ")
                            save_to_csv(all_news, output_filename, write_header=is_first_save)
                            save_to_csv(all_news, os.path.join("Combined_News.csv"), write_header=False)
                            stop_scraping = True
                            break

                save_to_csv(all_news, output_filename, write_header=is_first_save)
                save_to_csv(all_news, os.path.join("Combined_News.csv"), write_header=False)
                total_articles += len(all_news)
                is_first_save = False
                all_news = []
                futures = []

if __name__ == "__main__":
    main()
