import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import concurrent.futures
import threading
import os
import gc
from datetime import datetime, timedelta
import sys

# ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnicodeEncodeError (‡∏Ç‡πâ‡∏≤‡∏°‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö)
sys.stdout.reconfigure(encoding="utf-8", errors="ignore")


# ‚úÖ URL ‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß
base_url = 'https://www.investing.com/news/stock-market-news'
output_filename = "D:/Stock_Project/AI-and-API-Stock-Project/Investing_Folder/investing_news.csv"

# ‚úÖ Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Chrome instance
driver_lock = threading.Lock()

# ‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô"
yesterday = (datetime.now() - timedelta(days=1)).date()

def init_driver():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Chrome driver instance ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"""
    with driver_lock:
        options = uc.ChromeOptions()
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--blink-settings=imagesEnabled=false")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-extensions")
        driver = uc.Chrome(options=options)
    return driver

def close_popup(driver):
    """‡∏õ‡∏¥‡∏î popup ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ"""
    try:
        close_button = WebDriverWait(driver, 3).until(
            EC.element_to_be_clickable((By.XPATH, "//svg[@data-test='sign-up-dialog-close-button']"))
        )
        close_button.click()
        print("‚úÖ Popup ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    except Exception:
        pass

def scrape_news(driver):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"""
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    articles = soup.find_all('article', {'data-test': 'article-item'})
    news_list = []
    
    for article in articles:
        title_tag = article.find('a', {'data-test': 'article-title-link'})
        title = title_tag.get_text(strip=True) if title_tag else 'No Title'
        link = title_tag['href'] if title_tag and 'href' in title_tag.attrs else 'No Link'
        description_tag = article.find('p', {'data-test': 'article-description'})
        description = description_tag.get_text(strip=True) if description_tag else 'No Description'
        date_tag = article.find('time', {'data-test': 'article-publish-date'})
        date_str = date_tag['datetime'] if date_tag and 'datetime' in date_tag.attrs else 'No Date'

        news_list.append({'title': title, 'link': link, 'description': description, 'date': date_str})
    
    return news_list

def safe_quit(driver):
    """ ‡∏õ‡∏¥‡∏î driver ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ `WinError 6`"""
    if driver:
        try:
            if hasattr(driver, "service") and driver.service.process:
                driver.quit()
                del driver  # ‚úÖ ‡∏•‡∏ö object ‡∏Ç‡∏≠‡∏á WebDriver
                gc.collect()  # ‚úÖ ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
                print("‚úÖ WebDriver ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
            else:
                print("‚ö†Ô∏è WebDriver ‡∏õ‡∏¥‡∏î‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏¥‡∏î‡πÑ‡∏î‡πâ")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: WebDriver ‡∏õ‡∏¥‡∏î‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: {e}")

def scrape_page(page):
    """Scrape ‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"""
    driver = None
    try:
        driver = init_driver()
        driver.get(f"{base_url}/{page}" if page > 1 else base_url)

        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "article")))
        close_popup(driver)
        news = scrape_news(driver)

        print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page} ‡πÑ‡∏î‡πâ {len(news)} ‡∏Ç‡πà‡∏≤‡∏ß")
        return news

    except Exception as e:
        print(f"‚ùå Error ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page}: {e}")
        return []

    finally:
        safe_quit(driver)

def save_to_csv(data, filename, write_header=False):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á CSV"""
    if not data:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà ‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV")
        return
    
    df = pd.DataFrame(data)
    mode = 'w' if write_header else 'a'
    header = True if write_header else False
    df.to_csv(filename, index=False, encoding='utf-8', mode=mode, header=header)
    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß {len(data)} ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏á CSV (mode={mode})")

def clean_csv(filename):
    """‡∏•‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ 1 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025 ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å CSV ‡πÅ‡∏•‡∏∞ Clean ‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö"""
    if not os.path.exists(filename):
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå CSV ‡πÉ‡∏´‡πâ clean")
        return

    df = pd.read_csv(filename)
    if df.empty:
        print("‚ö†Ô∏è ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤, ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ clean")
        return

    df['date'] = pd.to_datetime(df['date'], errors='coerce')

    # ‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ 1 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025
    cutoff_date = datetime(2025, 3, 1).date()
    df_valid_news = df[df['date'].dt.date >= cutoff_date]  # ‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 1 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ
    df_old_news = df[df['date'].dt.date < cutoff_date]  # ‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ 1 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°

    # ‚úÖ ‡∏•‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ ‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏à‡∏≤‡∏Å 'title' ‡πÅ‡∏•‡∏∞ 'date'
    df_valid_news = df_valid_news.drop_duplicates(subset=['title', 'date'], keep='first')

    # ‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡πÄ‡∏Å‡πà‡∏≤
    df_valid_news = df_valid_news.sort_values(by='date', ascending=False)

    # ‚úÖ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
    total_after_clean = len(df_valid_news)

    # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡πÉ‡∏´‡∏°‡πà
    df_valid_news.to_csv(filename, index=False)

    print(f"\nüîç **Clean CSV Summary** üîç")
    print(f"üìä ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô Clean: {len(df)} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"üóëÔ∏è ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏•‡∏ö (‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ {cutoff_date}): {len(df_old_news)} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"üßπ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏•‡∏ö: {len(df) - len(df_valid_news) - len(df_old_news)} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (‡∏Ç‡∏≠‡∏á {cutoff_date} ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ): {total_after_clean} ‡∏Ç‡πà‡∏≤‡∏ß")
    print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡πÄ‡∏Å‡πà‡∏≤!")

def main():
    if os.path.exists(output_filename):
        os.remove(output_filename)  # ‚úÖ ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô

    batch_size = 5  # ‚úÖ ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô thread ‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Chrome crash
    max_pages = 7499
    all_news = []
    is_first_save = True
    stop_scraping = False  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° flag ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß
    total_articles = 0  # ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    cutoff_date = datetime(2025, 3, 1).date()  # ‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏õ‡πá‡∏ô 1 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025

    with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
        futures = []
        for page in range(1, max_pages + 1):
            if stop_scraping:
                break  # ‚úÖ ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ 1 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025

            futures.append(executor.submit(scrape_page, page))
            
            if len(futures) == batch_size or page == max_pages:
                for future in concurrent.futures.as_completed(futures):
                    result = future.result()
                    all_news.extend(result)

                    # ‚úÖ ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÉ‡∏ô batch ‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ 1 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025 ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    for item in result:
                        try:
                            news_date = datetime.strptime(item['date'], "%Y-%m-%d %H:%M:%S").date()
                            if news_date < cutoff_date:
                                print(f"‚èπÔ∏è ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ {cutoff_date}, ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡∏ô‡∏ó‡∏µ")
                                
                                # ‚úÖ ‡∏•‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á CSV
                                df_all_news = pd.DataFrame(all_news)
                                df_all_news = df_all_news.drop_duplicates(subset=['title', 'date'], keep='first')

                                save_to_csv(df_all_news.to_dict(orient='records'), output_filename, write_header=is_first_save)
                                
                                stop_scraping = True
                                break

                        except ValueError:
                            pass

                # ‚úÖ ‡∏•‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å CSV
                df_all_news = pd.DataFrame(all_news)
                df_all_news = df_all_news.drop_duplicates(subset=['title', 'date'], keep='first')

                save_to_csv(df_all_news.to_dict(orient='records'), output_filename, write_header=is_first_save)
                
                total_articles += len(df_all_news)
                is_first_save = False
                all_news = []
                futures = []

    clean_csv(output_filename)

if __name__ == "__main__":
    main()
