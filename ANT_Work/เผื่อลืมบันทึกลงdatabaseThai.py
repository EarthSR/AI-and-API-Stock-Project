import os
import urllib.parse
import pandas as pd
import requests
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from concurrent.futures import ThreadPoolExecutor
import sys

# ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnicodeEncodeError (‡∏Ç‡πâ‡∏≤‡∏°‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö)
sys.stdout.reconfigure(encoding="utf-8", errors="ignore")


# üîπ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á
NEWS_CATEGORIES = {
    "Business": "https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQhidXNpbmVzcwxjaGFubmVsYWxpYXMBAV4BJA%3D%3D",
    "Investment": "https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQppbnZlc3RtZW50DGNoYW5uZWxhbGlhcwEBXgEk",
    "Motoring": "https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQhtb3RvcmluZwxjaGFubmVsYWxpYXMBAV4BJA%3D%3D",
    "General": "https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQdnZW5lcmFsDGNoYW5uZWxhbGlhcwEBXgEk"
}

# üîπ ‡πÑ‡∏ü‡∏•‡πå CSV
RAW_CSV_FILE = "D:/Stock_Project/AI-and-API-Stock-Project/BangkokPost_Folder/Thai_News.csv"
CLEAN_CSV_FILE = "D:/Stock_Project/AI-and-API-Stock-Project/BangkokPost_Folder/Thai_News.csv"

# üîπ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà 00:00:00)
yesterday_start = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)

# üîπ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Selenium Driver ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ChromeDriver ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
def setup_driver():
    options = Options()
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--headless=new')  # ‚úÖ ‡πÉ‡∏ä‡πâ headless mode (new API)

    # üîπ ‡∏£‡∏∞‡∏ö‡∏∏‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á ChromeDriver ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏°‡πà
    chromedriver_path = ChromeDriverManager().install()
    service = Service(chromedriver_path)

    return webdriver.Chrome(service=service, options=options)


# üîπ ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤
def parse_and_format_datetime(date_str):
    date_formats = [
        "%d %b %Y at %H:%M",
        "%Y-%m-%d %H:%M:%S",
        "%b %d, %Y at %H:%M",
        "%d/%m/%Y %H:%M:%S",
        "%B %d, %Y"
    ]

    for date_format in date_formats:
        try:
            parsed_date = datetime.strptime(date_str.strip(), date_format)
            return parsed_date.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            continue

    return None  # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ

# üîπ ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß
def fetch_news_content(real_link):
    try:
        response = requests.get(real_link, timeout=5)
        news_soup = BeautifulSoup(response.content, 'html.parser')

        date_tag = news_soup.find('div', class_='article-info--col')
        date = date_tag.find('p', string=lambda x: x and 'PUBLISHED :' in x).get_text(strip=True).replace('PUBLISHED :', '').strip() if date_tag else 'No Date'

        content_div = news_soup.find('div', class_='article-content')
        paragraphs = content_div.find_all('p') if content_div else []
        full_content = '\n'.join([p.get_text(strip=True) for p in paragraphs])

        return date, full_content.replace(',', '').replace('""', '')
    except requests.exceptions.RequestException:
        return 'No Date', 'Content not found'

# üîπ ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° Debug ‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
def scrape_all_news():
    global yesterday_start
    current_fake_today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    stop_date = datetime(2025, 3, 1, 0, 0, 0)  # ‚úÖ ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ñ‡∏∂‡∏á "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ" = 2 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025

    while current_fake_today >= stop_date:  # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å ">" ‡πÄ‡∏õ‡πá‡∏ô ">=" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2025 ‡∏î‡πâ‡∏ß‡∏¢
        print(f" [START] ‡∏´‡∏•‡∏≠‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡πà‡∏≤ '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ' ‡∏Ñ‡∏∑‡∏≠ {current_fake_today.strftime('%Y-%m-%d')}")

        yesterday_start = current_fake_today - timedelta(days=1)

        all_news_data = []
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {executor.submit(scrape_news_from_category, name, url): name for name, url in NEWS_CATEGORIES.items()}
            for future in futures:
                result = future.result()
                all_news_data.extend(result)

        print(f"üìä ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ {len(all_news_data)} ‡∏Ç‡πà‡∏≤‡∏ß")

        if len(all_news_data) > 0:
            df = pd.DataFrame(all_news_data)
            df.to_csv(RAW_CSV_FILE, mode='a', index=False, encoding='utf-8', header=not os.path.exists(RAW_CSV_FILE))
            print(f"[SAVED] ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_news_data)} ‡∏Ç‡πà‡∏≤‡∏ß‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!")
        else:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å! ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÇ‡∏´‡∏•‡∏î‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")

        clean_and_process_data()

        current_fake_today -= timedelta(days=1)  # ‚úÖ ‡∏•‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡∏á 1 ‡∏ß‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ã‡πâ‡∏≥

# üîπ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î
def scrape_news_from_category(category_name, url):
    print(f" [START] ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å {category_name}")

    driver = setup_driver()
    driver.get(url)
    news_data = []

    while True:
        try:
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'mk-listnew--title')))
        except Exception:
            break

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        articles = soup.find_all('div', class_='mk-listnew--title')

        if not articles:
            break

        for article in articles:
            try:
                title_tag = article.find('h3').find('a')
                title = title_tag.get_text(strip=True)
                link = title_tag['href']

                real_link = urllib.parse.parse_qs(urllib.parse.urlparse(link).query).get('href', [link])[0] if 'track/visitAndRedirect' in link else link
                date, full_content = fetch_news_content(real_link)
                formatted_datetime = parse_and_format_datetime(date)

                if not formatted_datetime:
                    continue

                news_datetime = datetime.strptime(formatted_datetime, "%Y-%m-%d %H:%M:%S")
                if news_datetime < yesterday_start:
                    print(f"[STOP] ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ {yesterday_start} ‚Üí ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á {category_name}")
                    driver.quit()
                    return news_data

                news_data.append({"title": title, "date": formatted_datetime, "link": real_link, "description": full_content})

            except Exception:
                continue

        next_page = soup.find('a', string='Next')
        if next_page and 'href' in next_page.attrs:
            driver.get(next_page['href'])
        else:
            break

    driver.quit()
    print(f"üìå ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å {category_name} ‡πÑ‡∏î‡πâ {len(news_data)} ‡∏Ç‡πà‡∏≤‡∏ß")  # ‚úÖ Debug ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
    return news_data

def clean_and_process_data():
    if not os.path.exists(RAW_CSV_FILE):
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå CSV ‡πÉ‡∏´‡πâ clean")
        return

    df = pd.read_csv(RAW_CSV_FILE, encoding='utf-8')
    print(f"üìä ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤: {len(df)} ‡∏Ç‡πà‡∏≤‡∏ß")  # ‚úÖ Debug ‡∏Å‡πà‡∏≠‡∏ô clean

    df['date'] = pd.to_datetime(df['date'], errors='coerce')

    # üîπ ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ 01/03/2025
    cutoff_date = datetime(2025, 3, 1, 0, 0, 0)
    df = df[df['date'] >= cutoff_date]

    # üîπ ‡∏•‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ ‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏à‡∏≤‡∏Å 'title' ‡πÅ‡∏•‡∏∞ 'date'
    df = df.drop_duplicates(subset=['title', 'date'], keep='first')

    # üîπ ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡πÄ‡∏Å‡πà‡∏≤
    df = df.sort_values(by='date', ascending=False)

    if len(df) > 0:
        df.to_csv(CLEAN_CSV_FILE, index=False, encoding='utf-8')
        print(f"‚úÖ [CLEANED] ‡∏•‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡πÄ‡∏Å‡πà‡∏≤!")
    else:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å!")

# üîπ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å
if __name__ == "__main__":
    scrape_all_news()
