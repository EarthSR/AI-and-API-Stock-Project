import importlib
import logging
import sys
import time
import numpy as np
import pandas as pd
import sqlalchemy
import os
import tensorflow as tf
import ta
from tensorflow.keras.models import load_model
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, RobustScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score
import joblib
import warnings
from datetime import datetime, timedelta
import mysql.connector
from dotenv import load_dotenv
from tensorflow.keras.optimizers import Adam
from sqlalchemy import text
from sklearn.utils.class_weight import compute_class_weight
# XGBoost imports
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.ensemble import RandomForestRegressor
import joblib
import logging
import warnings
import os
import xgboost as xgb
from sklearn.impute import SimpleImputer
from ta.momentum import RSIIndicator
from ta.trend import SMAIndicator, MACD
from ta.volatility import BollingerBands, AverageTrueRange
import pickle
import io
import traceback
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

with open('../LSTM_model/class_weights.pkl', 'rb') as f:
    class_weights_dict = pickle.load(f)

@tf.keras.utils.register_keras_serializable()
def quantile_loss(y_true, y_pred, quantile=0.5):
    error = y_true - y_pred
    return tf.keras.backend.mean(tf.keras.backend.maximum(quantile * error, (quantile - 1) * error))

def focal_weighted_binary_crossentropy(class_weights, gamma=2.0, alpha_pos=0.7):
    def loss(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        epsilon = 1e-7
        y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)
        
        weights = tf.where(y_true == 1, class_weights[1], class_weights[0])
        alpha = tf.where(y_true == 1, alpha_pos, 1 - alpha_pos)
        pt = tf.where(y_true == 1, y_pred, 1 - y_pred)
        focal_factor = tf.pow(1 - pt, gamma)
        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
        weighted_bce = bce * weights * alpha * focal_factor
        return tf.reduce_mean(weighted_bce)
    return loss

# ======================== RETRAIN TRACKING SYSTEM ========================
def save_retrain_log(model_name, chunk_idx, retrain_count, performance_metrics=None):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ retrain (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)"""
    log_file = f"retrain_log_{model_name}.csv"
    
    log_data = {
        'Timestamp': datetime.now(),
        'Model': model_name,
        'Chunk_Index': chunk_idx,
        'Retrain_Count': retrain_count,
        'Date': datetime.now().strftime('%Y-%m-%d'),
        'Time': datetime.now().strftime('%H:%M:%S')
    }
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° performance metrics ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if performance_metrics:
        log_data.update(performance_metrics)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
    try:
        if os.path.exists(log_file):
            existing_df = pd.read_csv(log_file)
            new_df = pd.concat([existing_df, pd.DataFrame([log_data])], ignore_index=True)
        else:
            new_df = pd.DataFrame([log_data])
        
        new_df.to_csv(log_file, index=False)
        print(f"            üìù Retrain log saved to {log_file}")
        
    except Exception as e:
        print(f"            ‚ö†Ô∏è Error saving retrain log: {e}")

def should_retrain_model(model_name, retrain_frequency_days=5):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£ retrain ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤"""
    last_trained_file = f"last_retrain_{model_name}.txt"
    
    if not os.path.exists(last_trained_file):
        return True, "No previous retrain record"
    
    try:
        with open(last_trained_file, "r") as f:
            last_trained_str = f.read().strip()
        
        last_trained_date = datetime.strptime(last_trained_str, "%Y-%m-%d")
        days_since_last_retrain = (datetime.now() - last_trained_date).days
        
        if days_since_last_retrain >= retrain_frequency_days:
            return True, f"Last retrain: {days_since_last_retrain} days ago"
        else:
            return False, f"Last retrain: {days_since_last_retrain} days ago (too recent)"
            
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading retrain date: {e}")
        return True, "Error reading retrain record"

def update_retrain_date(model_name):
    """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà retrain ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
    last_trained_file = f"last_retrain_{model_name}.txt"
    
    try:
        with open(last_trained_file, "w") as f:
            f.write(datetime.now().strftime("%Y-%m-%d"))
        print(f"            üìÖ Updated retrain date for {model_name}")
        
    except Exception as e:
        print(f"            ‚ö†Ô∏è Error updating retrain date: {e}")

def get_retrain_stats(model_name):
    """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ retrain (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)"""
    log_file = f"retrain_log_{model_name}.csv"
    
    if not os.path.exists(log_file):
        return "No retrain history found"
    
    try:
        df = pd.read_csv(log_file)
        
        stats = {
            'Total_Retrains': len(df),
            'First_Retrain': df['Date'].min(),
            'Last_Retrain': df['Date'].max(),
            'Total_Chunks_Processed': df['Chunk_Index'].nunique() if 'Chunk_Index' in df.columns else 0,
            'Avg_Retrains_Per_Day': len(df) / max(1, (pd.to_datetime(df['Date'].max()) - pd.to_datetime(df['Date'].min())).days + 1)
        }
        
        return stats
        
    except Exception as e:
        return f"Error reading retrain stats: {e}"
    
def best_practice_version(raw_price, current_price, direction_prob, model_uncertainty=None):
    """‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î - ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏à‡∏£‡∏¥‡∏á"""
    
    # ‡πÉ‡∏ä‡πâ probability ‡πÅ‡∏ó‡∏ô binary direction
    predicted_price = raw_price
    predicted_direction_prob = direction_prob
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì confidence interval ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ model uncertainty
    if model_uncertainty is not None:
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì confidence interval ‡∏à‡∏≤‡∏Å model uncertainty
        price_lower = raw_price - (2 * model_uncertainty)  # 95% CI
        price_upper = raw_price + (2 * model_uncertainty)
    else:
        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏õ‡πá‡∏ô proxy
        price_volatility = abs(raw_price - current_price) * 0.1
        price_lower = raw_price - price_volatility
        price_upper = raw_price + price_volatility
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á prediction object ‡∏ó‡∏µ‡πà‡∏°‡∏µ uncertainty
    prediction_result = {
        'predicted_price': predicted_price,
        'price_lower_bound': price_lower,
        'price_upper_bound': price_upper,
        'direction_probability': predicted_direction_prob,
        'price_change_percent': (predicted_price - current_price) / current_price * 100,
        'model_confidence': abs(direction_prob - 0.5) * 2,  # 0 = no confidence, 1 = full confidence
        'uncertainty_range': abs(price_upper - price_lower),
        'raw_prediction_used': True,
        'adjustments_made': False
    }
    
    return prediction_result
# ======================== WALK-FORWARD VALIDATION FUNCTION ========================

def walk_forward_validation_multi_task_batch(
    model,
    df,
    feature_columns,
    ticker_scalers,   # Dict ‡∏Ç‡∏≠‡∏á Scaler per Ticker
    ticker_encoder,
    market_encoder,
    seq_length=10,
    retrain_frequency=5,
    chunk_size = 200
):

    all_predictions = []
    chunk_metrics = []
    tickers = df['StockSymbol'].unique()

    for ticker in tickers:
        print(f"\nProcessing Ticker: {ticker}")
        df_ticker = df[df['StockSymbol'] == ticker].sort_values('Date').reset_index(drop=True)
        
        total_days = len(df_ticker)
        print(f"   üìä Total data available: {total_days} days")
        
        if total_days < chunk_size + seq_length:
            print(f"   ‚ö†Ô∏è Not enough data (need at least {chunk_size + seq_length} days), skipping...")
            continue
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ
        num_chunks = total_days // chunk_size
        remaining_days = total_days % chunk_size
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° partial chunk ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
        if remaining_days > seq_length:
            num_chunks += 1
            
        print(f"   üì¶ Number of chunks: {num_chunks} (chunk_size={chunk_size})")
        
        ticker_predictions = []
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * chunk_size
            end_idx = min(start_idx + chunk_size, total_days)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î chunk
            if (end_idx - start_idx) < seq_length + 1:
                print(f"      ‚ö†Ô∏è Chunk {chunk_idx + 1} too small ({end_idx - start_idx} days), skipping...")
                continue
                
            current_chunk = df_ticker.iloc[start_idx:end_idx].reset_index(drop=True)
            
            print(f"\n      üì¶ Processing Chunk {chunk_idx + 1}/{num_chunks}")
            print(f"         üìÖ Date range: {current_chunk['Date'].min()} to {current_chunk['Date'].max()}")
            print(f"         üìà Days: {len(current_chunk)} ({start_idx}-{end_idx})")
            
            # === Walk-Forward Validation ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Chunk ===
            chunk_predictions = []
            batch_features = []
            batch_tickers = []
            batch_market = []
            batch_price = []
            batch_dir = []
            predictions_count = 0

            for i in range(len(current_chunk) - seq_length):
                historical_data = current_chunk.iloc[i : i + seq_length]
                target_data = current_chunk.iloc[i + seq_length]

                t_id = historical_data['Ticker_ID'].iloc[-1]
                if t_id not in ticker_scalers:
                    print(f"         ‚ö†Ô∏è Ticker_ID {t_id} not found in scalers, skipping...")
                    continue

                scaler_f = ticker_scalers[t_id]['feature_scaler']
                scaler_p = ticker_scalers[t_id]['price_scaler']

                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° input features
                features = historical_data[feature_columns].values
                ticker_ids = historical_data['Ticker_ID'].values
                market_ids = historical_data['Market_ID'].values

                try:
                    features_scaled = scaler_f.transform(features)
                except Exception as e:
                    print(f"         ‚ö†Ô∏è Feature scaling error: {e}")
                    continue

                X_features = features_scaled.reshape(1, seq_length, len(feature_columns))
                X_ticker = ticker_ids.reshape(1, seq_length)
                X_market = market_ids.reshape(1, seq_length)

                # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
                try:
                    pred_output = model.predict([X_features, X_ticker, X_market], verbose=0)
                    pred_price_scaled = pred_output[0]
                    pred_dir_prob = pred_output[1]

                    predicted_price = scaler_p.inverse_transform(pred_price_scaled)[0][0]
                    predicted_dir = 1 if pred_dir_prob[0][0] >= 0.5 else 0

                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                    actual_price = target_data['Close']
                    future_date = target_data['Date']
                    last_close = historical_data.iloc[-1]['Close']
                    actual_dir = 1 if (target_data['Close'] > last_close) else 0

                    chunk_predictions.append({
                        'Ticker': ticker,
                        'Date': future_date,
                        'Chunk_Index': chunk_idx + 1,
                        'Position_in_Chunk': i + 1,
                        'Predicted_Price': predicted_price,
                        'Actual_Price': actual_price,
                        'Predicted_Dir': predicted_dir,
                        'Actual_Dir': actual_dir,
                        'Last_Close': last_close,
                        'Price_Change_Actual': actual_price - last_close,
                        'Price_Change_Predicted': predicted_price - last_close
                    })

                    predictions_count += 1

                    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mini-retrain
                    batch_features.append(X_features)
                    batch_tickers.append(X_ticker)
                    batch_market.append(X_market)

                    y_price_true_scaled = scaler_p.transform(np.array([[actual_price]], dtype=float))
                    batch_price.append(y_price_true_scaled)

                    y_dir_true = np.array([actual_dir], dtype=float)
                    batch_dir.append(y_dir_true)

                    # üîÑ Mini-retrain (Online Learning ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô chunk)
                    if (i+1) % retrain_frequency == 0 or (i == (len(current_chunk) - seq_length - 1)):
                        if len(batch_features) > 0:
                            try:
                                bf = np.concatenate(batch_features, axis=0)
                                bt = np.concatenate(batch_tickers, axis=0)
                                bm = np.concatenate(batch_market, axis=0)
                                bp = np.concatenate(batch_price, axis=0)
                                bd = np.concatenate(batch_dir, axis=0)

                                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡πà‡∏≠‡∏ô retrain
                                pre_retrain_loss = model.evaluate(
                                    [bf, bt, bm],
                                    {
                                        'price_output': bp,
                                        'direction_output': bd
                                    },
                                    verbose=0
                                )

                                # ‡∏ó‡∏≥ mini-retrain
                                history = model.fit(
                                    [bf, bt, bm],
                                    {
                                        'price_output': bp,
                                        'direction_output': bd
                                    },
                                    epochs=1,
                                    batch_size=len(bf),
                                    verbose=0,
                                    shuffle=False
                                )
                                
                                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏á retrain
                                post_retrain_loss = model.evaluate(
                                    [bf, bt, bm],
                                    {
                                        'price_output': bp,
                                        'direction_output': bd
                                    },
                                    verbose=0
                                )
                                
                                # ‡∏™‡∏£‡πâ‡∏≤‡∏á performance metrics
                                performance_metrics = {
                                    'Pre_Retrain_Loss': pre_retrain_loss[0] if isinstance(pre_retrain_loss, list) else pre_retrain_loss,
                                    'Post_Retrain_Loss': post_retrain_loss[0] if isinstance(post_retrain_loss, list) else post_retrain_loss,
                                    'Loss_Improvement': (pre_retrain_loss[0] - post_retrain_loss[0]) if isinstance(pre_retrain_loss, list) else (pre_retrain_loss - post_retrain_loss),
                                    'Batch_Size': len(bf),
                                    'Position_in_Chunk': i+1
                                }
                                
                                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å retrain log
                                model_name = model.name if hasattr(model, 'name') else 'Unknown_Model'
                                save_retrain_log(model_name, chunk_idx + 1, (i+1)//retrain_frequency + 1, performance_metrics)
                                
                                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà retrain
                                update_retrain_date(model_name)
                                
                                print(f"            üîÑ Mini-retrain at position {i+1} (batch size: {len(bf)})")
                                print(f"            üìä Loss: {pre_retrain_loss[0]:.4f} ‚Üí {post_retrain_loss[0]:.4f}" if isinstance(pre_retrain_loss, list) else f"            üìä Loss: {pre_retrain_loss:.4f} ‚Üí {post_retrain_loss:.4f}")
                                
                            except Exception as e:
                                print(f"            ‚ö†Ô∏è Mini-retrain error: {e}")

                            # ‡∏•‡πâ‡∏≤‡∏á batch
                            batch_features = []
                            batch_tickers = []
                            batch_market = []
                            batch_price = []
                            batch_dir = []
                            
                except Exception as e:
                    print(f"         ‚ö†Ô∏è Prediction error at position {i}: {e}")
                    continue

            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö chunk ‡∏ô‡∏µ‡πâ
            if chunk_predictions:
                chunk_df = pd.DataFrame(chunk_predictions)
                
                actual_prices = chunk_df['Actual_Price'].values
                pred_prices = chunk_df['Predicted_Price'].values
                actual_dirs = chunk_df['Actual_Dir'].values
                pred_dirs = chunk_df['Predicted_Dir'].values
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
                mae_val = mean_absolute_error(actual_prices, pred_prices)
                mse_val = mean_squared_error(actual_prices, pred_prices)
                rmse_val = np.sqrt(mse_val)
                r2_val = r2_score(actual_prices, pred_prices)
                dir_acc = accuracy_score(actual_dirs, pred_dirs)
                dir_f1 = f1_score(actual_dirs, pred_dirs, zero_division=0)
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì MAPE ‡πÅ‡∏•‡∏∞ SMAPE (safe calculation)
                try:
                    mape_val = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100
                except:
                    mape_val = 0
                    
                try:
                    smape_val = 100/len(actual_prices) * np.sum(2 * np.abs(pred_prices - actual_prices) / (np.abs(actual_prices) + np.abs(pred_prices)))
                except:
                    smape_val = 0

                chunk_metric = {
                    'Ticker': ticker,
                    'Chunk_Index': chunk_idx + 1,
                    'Chunk_Start_Date': current_chunk['Date'].min(),
                    'Chunk_End_Date': current_chunk['Date'].max(),
                    'Chunk_Days': len(current_chunk),
                    'Predictions_Count': predictions_count,
                    'MAE': mae_val,
                    'MSE': mse_val,
                    'RMSE': rmse_val,
                    'MAPE': mape_val,
                    'SMAPE': smape_val,
                    'R2_Score': r2_val,
                    'Direction_Accuracy': dir_acc,
                    'Direction_F1': dir_f1
                }
                
                chunk_metrics.append(chunk_metric)
                ticker_predictions.extend(chunk_predictions)
                
                print(f"         üìä Chunk results: {predictions_count} predictions")
                print(f"         üìà Direction accuracy: {dir_acc:.3f}")
                print(f"         üìà Price MAE: {mae_val:.3f}")
            
            # ‚úÖ ‡πÅ‡∏Ñ‡πà Mini-retrain (Online Learning) ‡∏Å‡πá‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡πÅ‡∏•‡πâ‡∏ß
            print(f"         ‚úÖ Chunk {chunk_idx + 1} completed with continuous online learning")
        
        all_predictions.extend(ticker_predictions)
        print(f"   ‚úÖ Completed {ticker}: {len(ticker_predictions)} total predictions from {num_chunks} chunks")

    # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    print(f"\nüìä Processing complete!")
    print(f"   Total predictions: {len(all_predictions)}")
    print(f"   Total chunks processed: {len(chunk_metrics)}")
    
    if len(all_predictions) == 0:
        print("‚ùå No predictions generated!")
        return pd.DataFrame(), {}

    predictions_df = pd.DataFrame(all_predictions)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å predictions
    predictions_df.to_csv('predictions_chunk_walkforward.csv', index=False)
    print("üíæ Saved predictions to 'predictions_chunk_walkforward.csv'")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å chunk metrics
    if chunk_metrics:
        chunk_metrics_df = pd.DataFrame(chunk_metrics)
        chunk_metrics_df.to_csv('chunk_metrics.csv', index=False)
        print("üíæ Saved chunk metrics to 'chunk_metrics.csv'")

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Overall Metrics ‡∏ï‡πà‡∏≠ Ticker
    print("\nüìä Calculating overall metrics...")
    overall_metrics = {}
    
    for ticker, group in predictions_df.groupby('Ticker'):
        actual_prices = group['Actual_Price'].values
        pred_prices = group['Predicted_Price'].values
        actual_dirs = group['Actual_Dir'].values
        pred_dirs = group['Predicted_Dir'].values

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
        mae_val = mean_absolute_error(actual_prices, pred_prices)
        mse_val = mean_squared_error(actual_prices, pred_prices)
        rmse_val = np.sqrt(mse_val)
        r2_val = r2_score(actual_prices, pred_prices)

        dir_acc = accuracy_score(actual_dirs, pred_dirs)
        dir_f1 = f1_score(actual_dirs, pred_dirs, zero_division=0)
        dir_precision = precision_score(actual_dirs, pred_dirs, zero_division=0)
        dir_recall = recall_score(actual_dirs, pred_dirs, zero_division=0)

        # Safe MAPE ‡πÅ‡∏•‡∏∞ SMAPE calculation
        try:
            mape_val = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100
        except:
            mape_val = 0
            
        try:
            smape_val = 100/len(actual_prices) * np.sum(2 * np.abs(pred_prices - actual_prices) / (np.abs(actual_prices) + np.abs(pred_prices)))
        except:
            smape_val = 0

        overall_metrics[ticker] = {
            'Total_Predictions': len(group),
            'Number_of_Chunks': len(group['Chunk_Index'].unique()),
            'MAE': mae_val,
            'MSE': mse_val,
            'RMSE': rmse_val,
            'MAPE': mape_val,
            'SMAPE': smape_val,
            'R2_Score': r2_val,
            'Direction_Accuracy': dir_acc,
            'Direction_F1_Score': dir_f1,
            'Direction_Precision': dir_precision,
            'Direction_Recall': dir_recall
        }

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å overall metrics
    overall_metrics_df = pd.DataFrame.from_dict(overall_metrics, orient='index')
    overall_metrics_df.to_csv('overall_metrics_per_ticker.csv')
    print("üíæ Saved overall metrics to 'overall_metrics_per_ticker.csv'")

    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    print(f"\nüéØ Summary:")
    print(f"   üìà Tickers processed: {len(predictions_df['Ticker'].unique())}")
    print(f"   üìà Average predictions per ticker: {len(predictions_df)/len(predictions_df['Ticker'].unique()):.1f}")
    print(f"   üìà Average chunks per ticker: {len(chunk_metrics)/len(predictions_df['Ticker'].unique()):.1f}")
    
    if chunk_metrics:
        avg_chunk_acc = np.mean([c['Direction_Accuracy'] for c in chunk_metrics])
        avg_chunk_mae = np.mean([c['MAE'] for c in chunk_metrics])
        print(f"   üìà Average chunk direction accuracy: {avg_chunk_acc:.3f}")
        print(f"   üìà Average chunk MAE: {avg_chunk_mae:.3f}")

    print(f"\nüìÅ Files generated:")
    print(f"   üìÑ predictions_chunk_walkforward.csv - All predictions with chunk info")
    print(f"   üìÑ chunk_metrics.csv - Performance metrics per chunk")  
    print(f"   üìÑ overall_metrics_per_ticker.csv - Overall performance per ticker")

    return predictions_df, overall_metrics

def create_unified_ticker_scalers(df, feature_columns, scaler_file_path="../LSTM_model/ticker_scalers.pkl"):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á ticker scalers ‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô + ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    """
    print("üîß Creating unified per-ticker scalers...")
    
    # ======== STEP 1: Data Cleaning (‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô) ========
    df_clean = clean_data_for_unified_scaling(df, feature_columns)
    
    # ======== STEP 2: Create Per-Ticker Scalers (‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô) ========
    ticker_scalers = {}
    unique_tickers = df_clean['StockSymbol'].unique()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Ticker_ID ‡∏Å‡∏±‡∏ö StockSymbol
    ticker_id_to_name = {}
    name_to_ticker_id = {}
    
    print("üìã Creating ticker mappings...")
    for ticker_name in unique_tickers:
        ticker_rows = df_clean[df_clean['StockSymbol'] == ticker_name]
        if len(ticker_rows) > 0:
            ticker_id = ticker_rows['Ticker_ID'].iloc[0]
            ticker_id_to_name[ticker_id] = ticker_name
            name_to_ticker_id[ticker_name] = ticker_id
            print(f"   Mapping: Ticker_ID {ticker_id} = {ticker_name}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î pre-trained scalers
    pre_trained_scalers = {}
    try:
        if os.path.exists(scaler_file_path):
            pre_trained_scalers = joblib.load(scaler_file_path)
            print(f"‚úÖ Loaded pre-trained scalers for {len(pre_trained_scalers)} tickers")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not load pre-trained scalers: {e}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á scalers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ ticker
    for ticker_name in unique_tickers:
        ticker_data = df_clean[df_clean['StockSymbol'] == ticker_name].copy()
        
        if len(ticker_data) < 30:
            print(f"   ‚ö†Ô∏è {ticker_name}: Not enough data ({len(ticker_data)} days), skipping...")
            continue
        
        ticker_id = name_to_ticker_id[ticker_name]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö pre-trained scaler
        if ticker_id in pre_trained_scalers:
            scaler_info = pre_trained_scalers[ticker_id]
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö structure ‡∏Ç‡∏≠‡∏á scaler
            required_keys = ['feature_scaler', 'price_scaler']
            if all(key in scaler_info for key in required_keys):
                try:
                    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö scaler
                    test_features = ticker_data[feature_columns].iloc[:5]
                    test_price = ticker_data[['Close']].iloc[:5]
                    
                    _ = scaler_info['feature_scaler'].transform(test_features.fillna(0))
                    _ = scaler_info['price_scaler'].transform(test_price)
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                    scaler_info.update({
                        'ticker_symbol': ticker_name,
                        'ticker': ticker_name,  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö compatibility
                        'data_points': len(ticker_data)
                    })
                    
                    ticker_scalers[ticker_id] = scaler_info
                    print(f"   ‚úÖ {ticker_name} (ID: {ticker_id}): Using pre-trained scaler")
                    continue
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è {ticker_name}: Pre-trained scaler failed ({e}), creating new one")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler ‡πÉ‡∏´‡∏°‡πà
        try:
            print(f"   üîß {ticker_name}: Creating new scaler...")
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° feature data
            features = ticker_data[feature_columns].copy()
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ inf ‡πÅ‡∏•‡∏∞ NaN ‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô
            features = handle_infinite_values(features)
            features = features.fillna(features.mean()).fillna(0)
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° price data
            price_data = ticker_data[['Close']].copy()
            price_data = handle_infinite_values(price_data)
            price_data = price_data.fillna(price_data.mean())
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á scalers
            feature_scaler = RobustScaler()
            price_scaler = RobustScaler()
            
            feature_scaler.fit(features)
            price_scaler.fit(price_data)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scaler ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata (‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô)
            ticker_scalers[ticker_id] = {
                'feature_scaler': feature_scaler,
                'price_scaler': price_scaler,
                'ticker': ticker_name,  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö compatibility ‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô
                'ticker_symbol': ticker_name,  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                'data_points': len(ticker_data)
            }
            
            print(f"   ‚úÖ {ticker_name} (ID: {ticker_id}): Created new scaler with {len(ticker_data)} data points")
            
        except Exception as e:
            print(f"   ‚ùå {ticker_name}: Error creating scaler - {e}")
            continue
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scalers
    try:
        os.makedirs(os.path.dirname(scaler_file_path), exist_ok=True)
        joblib.dump(ticker_scalers, scaler_file_path)
        print(f"üíæ Saved unified scalers to {scaler_file_path}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ
        print(f"\nüìä Unified Ticker Scalers Summary:")
        for t_id, scaler_info in ticker_scalers.items():
            ticker_name = scaler_info.get('ticker', 'Unknown')
            data_points = scaler_info.get('data_points', 'Unknown')
            print(f"   Ticker_ID {t_id}: {ticker_name} ({data_points} data points)")
            
    except Exception as e:
        print(f"‚ùå Error saving scalers: {e}")
    
    print(f"‚úÖ Created unified scalers for {len(ticker_scalers)} tickers")
    return ticker_scalers

def clean_data_for_unified_scaling(df, feature_columns):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á unified scalers"""
    print("üßπ Cleaning data for unified scaling...")
    
    df_clean = df.copy()
    
    # Map column names ‡∏à‡∏≤‡∏Å database format ‡πÄ‡∏õ‡πá‡∏ô training format
    column_mapping = {
        'Change_Percent': 'Change (%)',
        'P_BV_Ratio': 'P/BV Ratio',
        'TotalRevenue': 'Total Revenue',
        'QoQGrowth': 'QoQ Growth (%)',
        'EPS': 'Earnings Per Share (EPS)',
        'ROE': 'ROE (%)',
        'NetProfitMargin': 'Net Profit Margin (%)',
        'DebtToEquity': 'Debt to Equity',
        'PERatio': 'P/E Ratio',
        'Dividend_Yield': 'Dividend Yield (%)',
    }
    
    # Rename columns ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    for old_name, new_name in column_mapping.items():
        if old_name in df_clean.columns and new_name not in df_clean.columns:
            df_clean[new_name] = df_clean[old_name]
            print(f"   üîÑ Mapped {old_name} ‚Üí {new_name}")
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° feature columns
    for col in feature_columns:
        if col in df_clean.columns:
            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numeric
                if not pd.api.types.is_numeric_dtype(df_clean[col]):
                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
                
                # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ infinite values (‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô)
                df_clean[col] = handle_infinite_values_column(df_clean[col])
                
                print(f"   ‚úÖ Cleaned {col}: range {df_clean[col].min():.3f} - {df_clean[col].max():.3f}")
                
            except Exception as e:
                print(f"   ‚ùå Error cleaning {col}: {e}")
                df_clean[col] = 0.0
    
    return df_clean

def handle_infinite_values(data):
    """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ infinite values ‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô"""
    data_clean = data.copy()
    
    for col in data_clean.columns:
        col_data = data_clean[col]
        
        # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà +inf ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà inf
        pos_inf_mask = col_data == np.inf
        if pos_inf_mask.any():
            max_val = col_data[col_data != np.inf].max()
            if pd.notna(max_val):
                data_clean.loc[pos_inf_mask, col] = max_val
            else:
                data_clean.loc[pos_inf_mask, col] = 0
        
        # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà -inf ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà -inf
        neg_inf_mask = col_data == -np.inf
        if neg_inf_mask.any():
            min_val = col_data[col_data != -np.inf].min()
            if pd.notna(min_val):
                data_clean.loc[neg_inf_mask, col] = min_val
            else:
                data_clean.loc[neg_inf_mask, col] = 0
    
    return data_clean

def handle_infinite_values_column(series):
    """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ infinite values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö column ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"""
    series_clean = series.copy()
    
    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà +inf
    pos_inf_mask = series_clean == np.inf
    if pos_inf_mask.any():
        max_val = series_clean[series_clean != np.inf].max()
        if pd.notna(max_val):
            series_clean[pos_inf_mask] = max_val
        else:
            series_clean[pos_inf_mask] = 0
    
    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà -inf
    neg_inf_mask = series_clean == -np.inf
    if neg_inf_mask.any():
        min_val = series_clean[series_clean != -np.inf].min()
        if pd.notna(min_val):
            series_clean[neg_inf_mask] = min_val
        else:
            series_clean[neg_inf_mask] = 0
    
    return series_clean

def prepare_data_for_walk_forward(df, feature_columns):
    """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Walk-Forward Validation ‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô"""
    print("üìä Preparing data for Walk-Forward Validation...")
    
    df_prepared = df.copy()
    
    # ======== STEP 1: Calculate Technical Indicators (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ) ========
    df_prepared = ensure_technical_indicators(df_prepared)
    
    # ======== STEP 2: Handle Sentiment Mapping ========
    if 'Sentiment' in df_prepared.columns:
        # ‡πÅ‡∏õ‡∏•‡∏á text sentiment ‡πÄ‡∏õ‡πá‡∏ô numeric (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
        if df_prepared['Sentiment'].dtype == 'object':
            df_prepared['Sentiment'] = df_prepared['Sentiment'].map({
                'Positive': 1, 'Negative': -1, 'Neutral': 0
            })
            print("   üîÑ Mapped sentiment values to numeric")
    
    # ======== STEP 3: Create Target Variables ========
    print("   üéØ Creating target variables...")
    df_prepared = df_prepared.sort_values(['StockSymbol', 'Date']).reset_index(drop=True)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Direction ‡πÅ‡∏•‡∏∞ TargetPrice per ticker
    df_prepared['Direction'] = 0
    df_prepared['TargetPrice'] = np.nan
    
    for ticker in df_prepared['StockSymbol'].unique():
        ticker_mask = df_prepared['StockSymbol'] == ticker
        ticker_data = df_prepared[ticker_mask].copy()
        
        if len(ticker_data) > 1:
            # Direction: 1 ‡∏ñ‡πâ‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ
            direction = (ticker_data['Close'].shift(-1) > ticker_data['Close']).astype(int)
            target_price = ticker_data['Close'].shift(-1)
            
            df_prepared.loc[ticker_mask, 'Direction'] = direction
            df_prepared.loc[ticker_mask, 'TargetPrice'] = target_price
    
    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ target
    df_prepared = df_prepared.dropna(subset=['Direction', 'TargetPrice'])
    
    # ======== STEP 4: Ensure Encoders ========
    if 'Ticker_ID' not in df_prepared.columns:
        ticker_encoder = LabelEncoder()
        df_prepared['Ticker_ID'] = ticker_encoder.fit_transform(df_prepared['StockSymbol'])
        print("   üîÑ Created Ticker_ID encoding")
    
    if 'Market_ID' not in df_prepared.columns or df_prepared['Market_ID'].dtype == 'object':
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Market_ID
        us_stock = ['AAPL', 'NVDA', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'AVGO', 'TSM', 'AMD']
        thai_stock = ['ADVANC', 'TRUE', 'DITTO', 'DIF', 
                     'INSET', 'JMART', 'INET', 'JAS', 'HUMAN']
        
        df_prepared['Market'] = df_prepared['StockSymbol'].apply(
            lambda x: "US" if x in us_stock else "TH" if x in thai_stock else "OTHER"
        )
        
        market_encoder = LabelEncoder()
        df_prepared['Market_ID'] = market_encoder.fit_transform(df_prepared['Market'])
        print("   üîÑ Created Market_ID encoding")
    
    # ======== STEP 5: Handle Missing Technical Indicators ========
    stock_columns = [
        'RSI', 'EMA_12', 'EMA_26', 'MACD', 'MACD_Signal', 'Bollinger_High',
        'Bollinger_Low', 'ATR', 'Keltner_High', 'Keltner_Low', 'Keltner_Middle',
        'Chaikin_Vol', 'Donchian_High', 'Donchian_Low', 'PSAR', 'SMA_50', 'SMA_200'
    ]
    
    available_stock_cols = [col for col in stock_columns if col in df_prepared.columns]
    if available_stock_cols:
        print(f"   üîß Forward filling {len(available_stock_cols)} technical indicators...")
        # Forward fill per ticker
        for ticker in df_prepared['StockSymbol'].unique():
            ticker_mask = df_prepared['StockSymbol'] == ticker
            df_prepared.loc[ticker_mask, available_stock_cols] = \
                df_prepared.loc[ticker_mask, available_stock_cols].fillna(method='ffill')
    
    # Fill remaining NaN with 0
    df_prepared = df_prepared.fillna(0)
    
    print(f"   ‚úÖ Prepared data: {len(df_prepared)} rows, {len(df_prepared['StockSymbol'].unique())} tickers")
    return df_prepared

def ensure_technical_indicators(df):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° technical indicators ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
    df_with_indicators = df.copy()
    
    required_indicators = ['RSI', 'MACD', 'MACD_Signal', 'ATR', 'Bollinger_High', 'Bollinger_Low']
    missing_indicators = [ind for ind in required_indicators if ind not in df_with_indicators.columns]
    
    if missing_indicators:
        print(f"   üîß Adding missing technical indicators: {missing_indicators}")
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indicators ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ per ticker
        for ticker in df_with_indicators['StockSymbol'].unique():
            ticker_mask = df_with_indicators['StockSymbol'] == ticker
            ticker_data = df_with_indicators[ticker_mask].copy()
            
            if len(ticker_data) < 20:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 20 ‡∏ß‡∏±‡∏ô
                continue
            
            try:
                if 'RSI' in missing_indicators:
                    rsi = ta.momentum.RSIIndicator(ticker_data['Close'], window=14).rsi()
                    df_with_indicators.loc[ticker_mask, 'RSI'] = rsi
                
                if any(ind in missing_indicators for ind in ['MACD', 'MACD_Signal']):
                    ema_12 = ticker_data['Close'].ewm(span=12).mean()
                    ema_26 = ticker_data['Close'].ewm(span=26).mean()
                    macd = ema_12 - ema_26
                    macd_signal = macd.rolling(window=9).mean()
                    
                    if 'MACD' in missing_indicators:
                        df_with_indicators.loc[ticker_mask, 'MACD'] = macd
                    if 'MACD_Signal' in missing_indicators:
                        df_with_indicators.loc[ticker_mask, 'MACD_Signal'] = macd_signal
                
                if 'ATR' in missing_indicators and all(col in ticker_data.columns for col in ['High', 'Low']):
                    atr = ta.volatility.AverageTrueRange(
                        high=ticker_data['High'], 
                        low=ticker_data['Low'], 
                        close=ticker_data['Close'], 
                        window=14
                    ).average_true_range()
                    df_with_indicators.loc[ticker_mask, 'ATR'] = atr
                
                if any(ind in missing_indicators for ind in ['Bollinger_High', 'Bollinger_Low']):
                    bollinger = ta.volatility.BollingerBands(ticker_data['Close'], window=20, window_dev=2)
                    if 'Bollinger_High' in missing_indicators:
                        df_with_indicators.loc[ticker_mask, 'Bollinger_High'] = bollinger.bollinger_hband()
                    if 'Bollinger_Low' in missing_indicators:
                        df_with_indicators.loc[ticker_mask, 'Bollinger_Low'] = bollinger.bollinger_lband()
                        
            except Exception as e:
                print(f"      ‚ö†Ô∏è Error calculating indicators for {ticker}: {e}")
                continue
    
    return df_with_indicators

# ======================== INTEGRATION WITH MAIN SYSTEM ========================

def create_walk_forward_compatible_scalers(df, feature_columns):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á scalers ‡∏ó‡∏µ‡πà compatible ‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Walk-Forward Validation ‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô"""
    
    print("üîÑ Creating Walk-Forward compatible scalers...")
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    df_prepared = prepare_data_for_walk_forward(df, feature_columns)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á unified scalers
    ticker_scalers = create_unified_ticker_scalers(df_prepared, feature_columns)
    
    return ticker_scalers, df_prepared
# ======================== WalkForwardMiniRetrainManager Class ========================
class WalkForwardMiniRetrainManager:
    """
    Walk-Forward Validation + Retrain System ‡∏û‡∏£‡πâ‡∏≠‡∏° XGBoost Ensemble
    - ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô chunks
    - retrain ‡∏ó‡∏∏‡∏Å N ‡∏ß‡∏±‡∏ô
    - Continuous learning ‡πÅ‡∏ö‡∏ö incremental
    - ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
    """
    def __init__(self, 
                 lstm_model_path="../LSTM_model/best_v6_plus_minimal_tuning_v2_final_model.keras",
                 gru_model_path="../GRU_Model/best_v6_plus_minimal_tuning_v2_final_model.keras",
                 retrain_frequency=5,  
                 chunk_size=200,       
                 seq_length=10):
        
        self.lstm_model_path = lstm_model_path
        self.gru_model_path = gru_model_path
        self.retrain_frequency = retrain_frequency
        self.chunk_size = chunk_size
        self.seq_length = seq_length
        
        # ‡πÇ‡∏°‡πÄ‡∏î‡∏•
        self.lstm_model = None
        self.gru_model = None
        
        # Performance tracking
        self.all_predictions = []
        self.chunk_metrics = []
        
    def load_models_for_prediction(self, model_path=None, compile_model=False):
        """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU"""
        custom_objects = {
            "quantile_loss": quantile_loss,
            "focal_weighted_binary_crossentropy": focal_weighted_binary_crossentropy
        }
        try:
            if model_path:  # ‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
                print(f"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å {model_path}...")
                model = tf.keras.models.load_model(
                    model_path,
                    custom_objects=custom_objects,
                    safe_mode=False,
                    compile=compile_model  # ‡πÉ‡∏ä‡πâ compile_model ‡πÅ‡∏ó‡∏ô compile
                )
                print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                return model
            else:  # ‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU
                print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢...")
                self.lstm_model = tf.keras.models.load_model(
                    self.lstm_model_path,
                    custom_objects=custom_objects,
                    safe_mode=False,
                    compile=compile_model
                )
                self.gru_model = tf.keras.models.load_model(
                    self.gru_model_path,
                    custom_objects=custom_objects,
                    safe_mode=False,
                    compile=compile_model
                )
                print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                return True
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
            return None if model_path else False

# Fix 2: Enhanced data cleaning function
def clean_data_for_scalers(df, feature_columns):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á scalers"""
    print("üßπ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scalers...")
    
    df_clean = df.copy()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    for col in feature_columns:
        if col in df_clean.columns:
            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
                col_data = df_clean[col].astype(str)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÄ‡∏ä‡πà‡∏ô 49.0349.03)
                problematic_mask = col_data.str.contains(r'\d+\.\d+\d+\.\d+', regex=True, na=False)
                
                if problematic_mask.any():
                    print(f"   ‚ö†Ô∏è ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå {col}: {problematic_mask.sum()} ‡πÅ‡∏ñ‡∏ß")
                    
                    # ‡πÅ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô (‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
                    def extract_first_number(x):
                        try:
                            if pd.isna(x):
                                return 0.0
                            x_str = str(x)
                            # ‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
                            import re
                            match = re.search(r'^\d+\.?\d*', x_str)
                            if match:
                                return float(match.group())
                            else:
                                return 0.0
                        except:
                            return 0.0
                    
                    df_clean[col] = col_data.apply(extract_first_number)
                else:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numeric ‡∏õ‡∏Å‡∏ï‡∏¥
                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
                
                # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡πÅ‡∏•‡∏∞ inf ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                col_mean = df_clean[col].replace([np.inf, -np.inf], np.nan).mean()
                if pd.isna(col_mean):
                    col_mean = 0.0
                
                df_clean[col] = df_clean[col].replace([np.inf, -np.inf, np.nan], col_mean)
                
                print(f"   ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î {col}: {df_clean[col].dtype}, range: {df_clean[col].min():.3f} - {df_clean[col].max():.3f}")
                
            except Exception as e:
                print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î {col}: {e}")
                # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                df_clean[col] = 0.0
    
    return df_clean

# Fix 3: Enhanced create_ticker_scalers function
def create_ticker_scalers_fixed(df, feature_columns, scaler_file_path="../LSTM_model/ticker_scalers.pkl"):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á ticker scalers ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô"""
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô
    df_clean = clean_data_for_scalers(df, feature_columns)
    
    ticker_scalers = {}
    tickers = df_clean['StockSymbol'].unique()
    
    print("üîß Creating/loading individual scalers for each ticker...")
    
    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î scalers ‡πÄ‡∏Å‡πà‡∏≤
    pre_trained_scalers = {}
    try:
        if os.path.exists(scaler_file_path):
            pre_trained_scalers = joblib.load(scaler_file_path)
            print(f"‚úÖ Loaded pre-trained scalers from {scaler_file_path} for {len(pre_trained_scalers)} tickers")
        else:
            print(f"‚ö†Ô∏è No pre-trained scalers found at {scaler_file_path}, creating new scalers")
    except Exception as e:
        print(f"‚ùå Error loading pre-trained scalers: {e}, creating new scalers")
    
    for ticker in tickers:
        df_ticker = df_clean[df_clean['StockSymbol'] == ticker].copy()
        
        if len(df_ticker) < 30:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 30 ‡∏ß‡∏±‡∏ô
            print(f"   ‚ö†Ô∏è {ticker}: Not enough data ({len(df_ticker)} days), skipping...")
            continue
        
        ticker_id = df_ticker['Ticker_ID'].iloc[0]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö pre-trained scaler
        if ticker_id in pre_trained_scalers:
            scaler_info = pre_trained_scalers[ticker_id]
            if all(key in scaler_info for key in ['ticker_symbol', 'feature_scaler', 'price_scaler']):
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ scaler ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                try:
                    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö transform ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                    test_data = df_ticker[feature_columns].iloc[:5].fillna(df_ticker[feature_columns].mean())
                    _ = scaler_info['feature_scaler'].transform(test_data)
                    _ = scaler_info['price_scaler'].transform(df_ticker[['Close']].iloc[:5])
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° data_points ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏≤‡∏î
                    if 'data_points' not in scaler_info:
                        scaler_info['data_points'] = len(df_ticker)
                    
                    ticker_scalers[ticker_id] = scaler_info
                    print(f"   ‚úÖ {ticker} (ID: {ticker_id}): Using pre-trained scaler with {scaler_info['data_points']} data points")
                    continue
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è {ticker} (ID: {ticker_id}): Pre-trained scaler failed test ({e}), creating new one")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler ‡πÉ‡∏´‡∏°‡πà
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á scaler
            feature_data = df_ticker[feature_columns].copy()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô numeric ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            non_numeric_cols = []
            for col in feature_columns:
                if col in feature_data.columns:
                    if not pd.api.types.is_numeric_dtype(feature_data[col]):
                        non_numeric_cols.append(col)
            
            if non_numeric_cols:
                print(f"   ‚ö†Ô∏è {ticker}: Non-numeric columns found: {non_numeric_cols}")
                for col in non_numeric_cols:
                    feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce')
            
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
            feature_data = feature_data.fillna(feature_data.mean()).fillna(0)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ infinite values ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            feature_data = feature_data.replace([np.inf, -np.inf], 0)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á feature scaler
            feature_scaler = RobustScaler()
            feature_scaler.fit(feature_data)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á price scaler
            price_scaler = RobustScaler()
            price_data = df_ticker[['Close']].copy()
            price_data = price_data.fillna(price_data.mean()).fillna(0)
            price_data = price_data.replace([np.inf, -np.inf], price_data.mean())
            price_scaler.fit(price_data)
            
            # ‡πÄ‡∏Å‡πá‡∏ö scaler ‡πÅ‡∏•‡∏∞ metadata
            ticker_scalers[ticker_id] = {
                'ticker_symbol': ticker,
                'feature_scaler': feature_scaler,
                'price_scaler': price_scaler,
                'data_points': len(df_ticker)
            }
            
            print(f"   ‚úÖ {ticker} (ID: {ticker_id}): Created new scaler with {len(df_ticker)} data points")
            
        except Exception as e:
            print(f"   ‚ùå {ticker}: Error creating scalers - {e}")
            # ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• debug
            print(f"      Debug - Feature data shape: {df_ticker[feature_columns].shape}")
            print(f"      Debug - Feature data dtypes: {df_ticker[feature_columns].dtypes.to_dict()}")
            print(f"      Debug - Sample values: {df_ticker[feature_columns].iloc[0].to_dict()}")
            continue
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scalers ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß
    try:
        os.makedirs(os.path.dirname(scaler_file_path), exist_ok=True)
        joblib.dump(ticker_scalers, scaler_file_path)
        print(f"üíæ Saved updated scalers to {scaler_file_path}")
    except Exception as e:
        print(f"‚ùå Error saving scalers: {e}")
    
    print(f"‚úÖ Created/loaded scalers for {len(ticker_scalers)} tickers")
    return ticker_scalers

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# Setup logging
class EnhancedDynamicEnsembleMetaLearner:
    """
    üèÜ Enhanced Dynamic Weighted Ensemble Meta Learner
    
    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏ä‡∏±‡πà‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏∏‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞:
    - ‡πÉ‡∏ä‡πâ historical performance ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏∏‡πâ‡∏ô
    - ‡∏£‡∏ß‡∏° MAE, R¬≤ Score, ‡πÅ‡∏•‡∏∞ Direction Accuracy
    - ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏Å‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡∏´‡∏∏‡πâ‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡πÜ
    """
    
    def __init__(self, window_size=20):
        self.window_size = window_size
        self.performance_history = {}
        
        # üìä Historical Performance Data ‡∏à‡∏≤‡∏Å metrics ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤
        self.stock_performance = {
            # Stock: {metric: (LSTM_value, GRU_value)}
            'AAPL': {
                'MAE': (7.1761, 4.7221),
                'R2': (0.7203, 0.8783),
                'Direction_Acc': (0.6455, 0.6499)
            },
            'ADVANC': {
                'MAE': (6.5163, 5.6464),
                'R2': (0.8814, 0.9046),
                'Direction_Acc': (0.6888, 0.7056)
            },
            'AMD': {
                'MAE': (4.3655, 3.6774),
                'R2': (0.9354, 0.9613),
                'Direction_Acc': (0.7695, 0.7533)
            },
            'AMZN': {
                'MAE': (6.1828, 7.5087),
                'R2': (0.8641, 0.8300),
                'Direction_Acc': (0.6686, 0.6472)
            },
            'AVGO': {
                'MAE': (14.2846, 13.0835),
                'R2': (0.7351, 0.8006),
                'Direction_Acc': (0.6542, 0.6366)
            },
            'DIF': {
                'MAE': (0.2062, 0.1458),
                'R2': (0.6259, 0.7279),
                'Direction_Acc': (0.7147, 0.6844)
            },
            'DITTO': {
                'MAE': (1.3103, 0.6702),
                'R2': (0.7695, 0.9302),
                'Direction_Acc': (0.6945, 0.6844)
            },
            'GOOGL': {
                'MAE': (5.1876, 5.3210),
                'R2': (0.7843, 0.7746),
                'Direction_Acc': (0.7320, 0.7215)
            },
            'HUMAN': {
                'MAE': (0.2718, 0.1781),
                'R2': (0.9480, 0.9737),
                'Direction_Acc': (0.7630, 0.7686)
            },
            'INET': {
                'MAE': (0.1599, 0.1269),
                'R2': (0.8131, 0.8977),
                'Direction_Acc': (0.7378, 0.7162)
            },
            'INSET': {
                'MAE': (0.0824, 0.0539),
                'R2': (0.9629, 0.9832),
                'Direction_Acc': (0.7262, 0.7427)
            },
            'JAS': {
                'MAE': (0.0678, 0.0705),
                'R2': (0.9710, 0.9726),
                'Direction_Acc': (0.7378, 0.7507)
            },
            'JMART': {
                'MAE': (0.8154, 0.5712),
                'R2': (0.8850, 0.9508),
                'Direction_Acc': (0.7378, 0.7666)
            },
            'META': {
                'MAE': (22.3500, 28.5451),
                'R2': (0.8665, 0.7128),
                'Direction_Acc': (0.6340, 0.6658)
            },
            'MSFT': {
                'MAE': (16.2797, 8.9404),
                'R2': (0.6759, 0.9007),
                'Direction_Acc': (0.6282, 0.6101)
            },
            'NVDA': {
                'MAE': (12.2969, 9.9044),
                'R2': (0.1414, 0.4617),
                'Direction_Acc': (0.5303, 0.5199)
            },
            'TRUE': {
                'MAE': (0.3843, 0.2568),
                'R2': (0.7830, 0.8893),
                'Direction_Acc': (0.7176, 0.6844)
            },
            'TSLA': {
                'MAE': (19.3031, 7.7488),
                'R2': (0.8700, 0.9774),
                'Direction_Acc': (0.6916, 0.6790)
            },
            'TSM': {
                'MAE': (6.1881, 5.6068),
                'R2': (0.8837, 0.9083),
                'Direction_Acc': (0.7262, 0.6764)
            }
        }
        
        logger.info("üöÄ Enhanced Dynamic Weighted Ensemble Meta Learner initialized")
        logger.info(f"Window size: {window_size} days")
        logger.info(f"Loaded performance data for {len(self.stock_performance)} stocks")
    
    def calculate_stock_specific_weights(self, ticker):
        """
        üéØ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏∏‡πâ‡∏ô‡∏à‡∏≤‡∏Å historical performance
        ‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ MAE (‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ), R¬≤ Score (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ), Direction Accuracy (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)
        """
        
        if ticker not in self.stock_performance:
            logger.warning(f"No historical performance data for {ticker}, using equal weights")
            return 0.5, 0.5
        
        perf = self.stock_performance[ticker]
        
        # Extract metrics (LSTM, GRU)
        lstm_mae, gru_mae = perf['MAE']
        lstm_r2, gru_r2 = perf['R2']
        lstm_dir, gru_dir = perf['Direction_Acc']
        
        # 1. MAE Score (inverse - lower is better)
        mae_lstm_score = 1 / (lstm_mae + 1e-8)
        mae_gru_score = 1 / (gru_mae + 1e-8)
        mae_total = mae_lstm_score + mae_gru_score
        mae_lstm_weight = mae_lstm_score / mae_total
        mae_gru_weight = mae_gru_score / mae_total
        
        # 2. R¬≤ Score (direct - higher is better)
        r2_total = lstm_r2 + gru_r2
        r2_lstm_weight = lstm_r2 / r2_total if r2_total > 0 else 0.5
        r2_gru_weight = gru_r2 / r2_total if r2_total > 0 else 0.5
        
        # 3. Direction Accuracy (direct - higher is better)
        dir_total = lstm_dir + gru_dir
        dir_lstm_weight = lstm_dir / dir_total if dir_total > 0 else 0.5
        dir_gru_weight = gru_dir / dir_total if dir_total > 0 else 0.5
        
        # üèÜ Combined weighted score
        # MAE ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å 40%, R¬≤ ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å 40%, Direction Accuracy ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å 20%
        lstm_final_weight = (0.4 * mae_lstm_weight + 
                           0.4 * r2_lstm_weight + 
                           0.2 * dir_lstm_weight)
        
        gru_final_weight = (0.4 * mae_gru_weight + 
                          0.4 * r2_gru_weight + 
                          0.2 * dir_gru_weight)
        
        # Normalize to ensure sum = 1
        total_weight = lstm_final_weight + gru_final_weight
        if total_weight > 0:
            lstm_final_weight /= total_weight
            gru_final_weight /= total_weight
        else:
            lstm_final_weight = gru_final_weight = 0.5
        
        logger.debug(f"{ticker}: LSTM={lstm_final_weight:.3f}, GRU={gru_final_weight:.3f} "
                    f"[MAE: {mae_lstm_weight:.2f}/{mae_gru_weight:.2f}, "
                    f"R¬≤: {r2_lstm_weight:.2f}/{r2_gru_weight:.2f}, "
                    f"Dir: {dir_lstm_weight:.2f}/{dir_gru_weight:.2f}]")
        
        return lstm_final_weight, gru_final_weight
    
    def calculate_dynamic_weights(self, ticker, recent_performance):
        """
        üéØ Dynamic Weighting Algorithm - ‡∏ú‡∏™‡∏°‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á historical ‡πÅ‡∏•‡∏∞ recent performance
        """
        
        # Get stock-specific historical weights
        hist_lstm_weight, hist_gru_weight = self.calculate_stock_specific_weights(ticker)
        
        # If insufficient recent data, use historical weights
        if len(recent_performance) < 3:
            logger.debug(f"{ticker}: Using historical weights (insufficient recent data)")
            return hist_lstm_weight, hist_gru_weight
        
        try:
            # Calculate recent performance weights
            actual_prices = recent_performance['Actual_Price'].values
            lstm_predictions = recent_performance['Predicted_Price_LSTM'].values
            gru_predictions = recent_performance['Predicted_Price_GRU'].values
            
            # Recent MAE calculation
            lstm_recent_mae = mean_absolute_error(actual_prices, lstm_predictions)
            gru_recent_mae = mean_absolute_error(actual_prices, gru_predictions)
            
            # Recent weights (inverse MAE)
            lstm_recent_inv = 1 / (lstm_recent_mae + 1e-8)
            gru_recent_inv = 1 / (gru_recent_mae + 1e-8)
            total_recent_inv = lstm_recent_inv + gru_recent_inv
            
            recent_lstm_weight = lstm_recent_inv / total_recent_inv
            recent_gru_weight = gru_recent_inv / total_recent_inv
            
            # üèÜ Adaptive blending: ‡πÉ‡∏ä‡πâ historical 70% + recent 30%
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö established performance, historical data ‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤
            alpha = 0.3  # recent performance weight
            final_lstm_weight = (1 - alpha) * hist_lstm_weight + alpha * recent_lstm_weight
            final_gru_weight = (1 - alpha) * hist_gru_weight + alpha * recent_gru_weight
            
            logger.debug(f"{ticker}: Blended weights - LSTM={final_lstm_weight:.3f}, GRU={final_gru_weight:.3f} "
                        f"[Hist: {hist_lstm_weight:.2f}/{hist_gru_weight:.2f}, "
                        f"Recent: {recent_lstm_weight:.2f}/{recent_gru_weight:.2f}]")
            
            return final_lstm_weight, final_gru_weight
            
        except Exception as e:
            logger.warning(f"Error calculating dynamic weights for {ticker}: {e}")
            return hist_lstm_weight, hist_gru_weight
    
    def get_performance_summary(self, ticker):
        """üìä Get performance summary for a stock"""
        if ticker not in self.stock_performance:
            return "No historical data"
        
        perf = self.stock_performance[ticker]
        lstm_mae, gru_mae = perf['MAE']
        lstm_r2, gru_r2 = perf['R2']
        lstm_dir, gru_dir = perf['Direction_Acc']
        
        # Determine better model for each metric
        mae_winner = "LSTM" if lstm_mae < gru_mae else "GRU"
        r2_winner = "LSTM" if lstm_r2 > gru_r2 else "GRU"
        dir_winner = "LSTM" if lstm_dir > gru_dir else "GRU"
        
        return f"MAE:{mae_winner}, R¬≤:{r2_winner}, Dir:{dir_winner}"
    
    def prepare_data_for_model(self, df):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ensemble"""
        
        prepared_df = df.copy()
        
        # Map column names
        column_mapping = {
            'StockSymbol': 'Ticker',
            'Close': 'Current_Price',
            'PredictionClose_LSTM': 'Predicted_Price_LSTM',
            'PredictionClose_GRU': 'Predicted_Price_GRU',
            'PredictionTrend_LSTM': 'Predicted_Dir_LSTM',
            'PredictionTrend_GRU': 'Predicted_Dir_GRU'
        }
        
        for old_name, new_name in column_mapping.items():
            if old_name in prepared_df.columns:
                prepared_df[new_name] = prepared_df[old_name]
        
        # Add Date if missing
        if 'Date' not in prepared_df.columns:
            prepared_df['Date'] = datetime.now().strftime('%Y-%m-%d')
        prepared_df['Date'] = pd.to_datetime(prepared_df['Date'])
        
        # Filter valid data
        prediction_mask = (
            prepared_df['Predicted_Price_LSTM'].notna() & 
            prepared_df['Predicted_Price_GRU'].notna() &
            prepared_df['Predicted_Dir_LSTM'].notna() & 
            prepared_df['Predicted_Dir_GRU'].notna() &
            (prepared_df['Current_Price'] > 0)
        )
        
        prepared_df = prepared_df[prediction_mask].copy()
        
        if len(prepared_df) == 0:
            logger.error("No valid data for prediction")
            return None
        
        logger.info(f"‚úÖ Data prepared: {len(prepared_df)} rows")
        return prepared_df
    
    def predict_single_stock(self, stock_data, ticker):
        """üéØ Enhanced Stock-Specific Ensemble Prediction"""
        
        current_price = stock_data['Current_Price']
        lstm_price = stock_data['Predicted_Price_LSTM']
        gru_price = stock_data['Predicted_Price_GRU']
        lstm_dir = stock_data['Predicted_Dir_LSTM']
        gru_dir = stock_data['Predicted_Dir_GRU']
        
        # Get recent performance for dynamic weighting
        recent_performance = self.performance_history.get(ticker, pd.DataFrame())
        
        # Calculate stock-specific dynamic weights
        lstm_weight, gru_weight = self.calculate_dynamic_weights(ticker, recent_performance)
        
        # üèÜ Stock-Specific Weighted Prediction
        ensemble_price = lstm_weight * lstm_price + gru_weight * gru_price
        
        # Direction prediction (weighted voting)
        lstm_dir_weighted = lstm_weight * lstm_dir
        gru_dir_weighted = gru_weight * gru_dir
        ensemble_dir_prob = lstm_dir_weighted + gru_dir_weighted
        ensemble_direction = 1 if ensemble_dir_prob >= 0.5 else 0
        
        # Price change analysis
        price_change_pct = ((ensemble_price - current_price) / current_price) * 100
        price_implied_direction = 1 if price_change_pct > 0.5 else 0
        
        # Consistency check
        is_consistent = (ensemble_direction == price_implied_direction)
        
        # Enhanced confidence calculation based on historical performance
        direction_confidence = abs(ensemble_dir_prob - 0.5) * 2
        price_confidence = min(abs(price_change_pct) / 10, 1.0)
        
        # Add historical performance boost to confidence
        if ticker in self.stock_performance:
            perf = self.stock_performance[ticker]
            avg_r2 = (perf['R2'][0] + perf['R2'][1]) / 2
            avg_dir_acc = (perf['Direction_Acc'][0] + perf['Direction_Acc'][1]) / 2
            historical_confidence = (avg_r2 + avg_dir_acc) / 2
            
            # Blend current confidence with historical performance
            overall_confidence = 0.7 * ((direction_confidence + price_confidence) / 2) + 0.3 * historical_confidence
        else:
            overall_confidence = (direction_confidence + price_confidence) / 2
        
        # Consistency bonus
        if is_consistent:
            overall_confidence = min(overall_confidence * 1.15, 0.95)
        
        # Enhanced risk assessment
        perf_summary = self.get_performance_summary(ticker)
        
        if not is_consistent:
            risk_level = "üî¥ HIGH_RISK"
            warning = f"INCONSISTENT_PREDICTION - {perf_summary}"
            action = "EXERCISE_EXTREME_CAUTION"
        elif abs(price_change_pct) > 15:
            risk_level = "üü° MEDIUM_RISK"
            warning = f"HIGH_VOLATILITY_PREDICTION - {perf_summary}"
            action = "HIGH_RISK"
        elif overall_confidence >= 0.75:
            risk_level = "üü¢ LOW_RISK"
            warning = f"HIGH_CONFIDENCE - {perf_summary}"
            action = "CONSIDER"
        elif overall_confidence >= 0.5:
            risk_level = "üü° MEDIUM_RISK"
            warning = f"MODERATE_CONFIDENCE - {perf_summary}"
            action = "CAUTION"
        else:
            risk_level = "üî¥ HIGH_RISK"
            warning = f"LOW_CONFIDENCE - {perf_summary}"
            action = "AVOID"
        
        return {
            'predicted_price': ensemble_price,
            'predicted_direction': ensemble_direction,
            'direction_probability': ensemble_dir_prob,
            'confidence': overall_confidence,
            'price_change_pct': price_change_pct,
            'is_consistent': is_consistent,
            'lstm_weight': lstm_weight,
            'gru_weight': gru_weight,
            'risk_level': risk_level,
            'warning': warning,
            'action': action,
            'performance_summary': perf_summary
        }
    
    def predict_meta(self, df):
        """üèÜ Main Prediction Method using Enhanced Stock-Specific Weighted Ensemble"""
        
        logger.info("üöÄ Starting Enhanced Stock-Specific Weighted Ensemble Prediction...")
        
        # Prepare data
        prepared_df = self.prepare_data_for_model(df)
        if prepared_df is None:
            return df
        
        logger.info(f"Processing {len(prepared_df)} stocks with stock-specific weighting")
        
        # Process each stock
        predictions = []
        weight_summary = {}
        
        for _, stock_data in prepared_df.iterrows():
            ticker = stock_data['Ticker']
            prediction = self.predict_single_stock(stock_data, ticker)
            prediction['ticker'] = ticker
            predictions.append(prediction)
            
            # Collect weight summary
            weight_summary[ticker] = {
                'lstm_weight': prediction['lstm_weight'],
                'gru_weight': prediction['gru_weight'],
                'performance': prediction['performance_summary']
            }
        
        if len(predictions) == 0:
            logger.error("No predictions generated")
            return df
        
        # Enhanced statistics
        confidences = [p['confidence'] for p in predictions]
        consistency_rate = np.mean([p['is_consistent'] for p in predictions])
        price_changes = [p['price_change_pct'] for p in predictions]
        
        # Weight distribution analysis
        avg_lstm_weight = np.mean([p['lstm_weight'] for p in predictions])
        avg_gru_weight = np.mean([p['gru_weight'] for p in predictions])
        
        logger.info(f"üéØ Results: {len(predictions)} stocks processed")
        logger.info(f"üìä Avg Confidence: {np.mean(confidences):.3f}")
        logger.info(f"üìä Consistency Rate: {consistency_rate:.1%}")
        logger.info(f"‚öñÔ∏è Avg Weights - LSTM: {avg_lstm_weight:.3f}, GRU: {avg_gru_weight:.3f}")
        
        # Map results back to dataframe
        ticker_col = 'StockSymbol' if 'StockSymbol' in df.columns else 'Ticker'
        prediction_mask = df[ticker_col].isin(prepared_df['Ticker'])
        
        if prediction_mask.any():
            ticker_to_results = {p['ticker']: p for p in predictions}
            ensemble_method = f"Enhanced_Stock_Specific_v2.0_consistency_{consistency_rate:.1%}"
            
            for idx, row in df[prediction_mask].iterrows():
                ticker = row[ticker_col]
                if ticker in ticker_to_results:
                    result = ticker_to_results[ticker]
                    
                    # Core predictions
                    df.loc[idx, 'XGB_Predicted_Direction'] = result['predicted_direction']
                    df.loc[idx, 'XGB_Predicted_Price'] = result['predicted_price']
                    df.loc[idx, 'XGB_Confidence'] = result['confidence']
                    df.loc[idx, 'XGB_Predicted_Direction_Proba'] = result['direction_probability']
                    
                    # Enhanced info
                    df.loc[idx, 'Reliability_Score'] = result['confidence']
                    df.loc[idx, 'Reliability_Warning'] = result['warning']
                    df.loc[idx, 'Suggested_Action'] = result['action']
                    df.loc[idx, 'Risk_Level'] = result['risk_level']
                    df.loc[idx, 'Ensemble_Method'] = ensemble_method
                    df.loc[idx, 'Price_Change_Percent'] = result['price_change_pct']
                    
                    # Stock-specific ensemble info
                    df.loc[idx, 'LSTM_Weight'] = result['lstm_weight']
                    df.loc[idx, 'GRU_Weight'] = result['gru_weight']
                    df.loc[idx, 'Is_Consistent'] = result['is_consistent']
                    df.loc[idx, 'Historical_Performance'] = result['performance_summary']
        
        # Log detailed weight analysis
        logger.info("üîç Stock-Specific Weight Analysis:")
        for ticker, info in weight_summary.items():
            logger.info(f"  {ticker}: LSTM={info['lstm_weight']:.3f}, GRU={info['gru_weight']:.3f} [{info['performance']}]")
        
        # Log distribution
        risk_dist = pd.Series([p['risk_level'] for p in predictions]).value_counts()
        action_dist = pd.Series([p['action'] for p in predictions]).value_counts()
        
        logger.info("üìä Risk Distribution: " + ", ".join([f"{r}:{c}" for r, c in risk_dist.items()]))
        logger.info("üìä Action Distribution: " + ", ".join([f"{a}:{c}" for a, c in action_dist.items()]))
        
        return df
    
    def update_performance_history(self, ticker, actual_price, lstm_pred, gru_pred, date):
        """Update performance history for dynamic weighting"""
        
        if ticker not in self.performance_history:
            self.performance_history[ticker] = pd.DataFrame()
        
        new_record = pd.DataFrame({
            'Date': [date],
            'Actual_Price': [actual_price],
            'Predicted_Price_LSTM': [lstm_pred],
            'Predicted_Price_GRU': [gru_pred]
        })
        
        self.performance_history[ticker] = pd.concat([
            self.performance_history[ticker], 
            new_record
        ], ignore_index=True)
        
        # Keep only recent records
        max_history = self.window_size * 3
        if len(self.performance_history[ticker]) > max_history:
            self.performance_history[ticker] = self.performance_history[ticker].tail(max_history)
    
    def get_stock_recommendations(self):
        """üìà Get stock recommendations based on historical performance"""
        recommendations = {}
        
        for ticker, perf in self.stock_performance.items():
            lstm_mae, gru_mae = perf['MAE']
            lstm_r2, gru_r2 = perf['R2']
            lstm_dir, gru_dir = perf['Direction_Acc']
            
            # Calculate overall scores
            lstm_score = (1/lstm_mae) * lstm_r2 * lstm_dir
            gru_score = (1/gru_mae) * gru_r2 * gru_dir
            avg_score = (lstm_score + gru_score) / 2
            
            if avg_score > 0.8:
                recommendation = "üü¢ HIGH_CONFIDENCE"
            elif avg_score > 0.4:
                recommendation = "üü° MODERATE_CONFIDENCE"
            else:
                recommendation = "üî¥ LOW_CONFIDENCE"
            
            recommendations[ticker] = {
                'score': avg_score,
                'recommendation': recommendation
            }
        
        return recommendations
    
    # Compatibility methods
    def is_model_available(self):
        return True
    
    def get_model_status(self):
        return "ENHANCED_STOCK_SPECIFIC_ENSEMBLE_READY"
    
    def should_retrain_meta(self):
        return False  # Rule-based, no retraining needed
    
    def validate_predictions(self, df):
        return 'XGB_Predicted_Price' in df.columns and df['XGB_Predicted_Price'].notna().any()

# Backward compatibility
DynamicEnsembleMetaLearner = EnhancedDynamicEnsembleMetaLearner
XGBoostMetaLearner = EnhancedDynamicEnsembleMetaLearner
UpdatedXGBoostMetaLearner = EnhancedDynamicEnsembleMetaLearner

# ======================== ENHANCED PREDICTION SYSTEM ========================

# ‡πÇ‡∏´‡∏•‡∏î configuration ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö environment
print("üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î configuration...")
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.env')

if not os.path.exists(path):
    print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå config.env ‡∏ó‡∏µ‡πà {path}")
    print("üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á config.env...")
    
    try:
        with open(path, 'w') as f:
            f.write("# Database Configuration\n")
            f.write("DB_USER=your_username\n")
            f.write("DB_PASSWORD=your_password\n")
            f.write("DB_HOST=localhost\n")
            f.write("DB_NAME=your_database\n")
        
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á config.env ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏µ‡πà {path}")
        print("üìã ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì")
        exit()
        
    except Exception as e:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå config.env ‡πÑ‡∏î‡πâ: {e}")
        print("üìù ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå config.env ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
        print("   DB_USER=your_username")
        print("   DB_PASSWORD=your_password") 
        print("   DB_HOST=your_host")
        print("   DB_NAME=your_database")
        exit()

load_dotenv(path)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö environment variables
required_vars = ['DB_USER', 'DB_PASSWORD', 'DB_HOST', 'DB_NAME']
missing_vars = [var for var in required_vars if not os.getenv(var)]

if missing_vars:
    print(f"‚ùå ‡∏Ç‡∏≤‡∏î environment variables: {missing_vars}")
    exit()

try:
    DB_CONNECTION = f"mysql+mysqlconnector://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_NAME')}"
    print("‚úÖ Database connection string ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
except Exception as e:
    print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á database connection: {e}")
    exit()

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î    
MODEL_LSTM_PATH = "../LSTM_model/best_v6_plus_minimal_tuning_v2_final_model.keras"
MODEL_GRU_PATH = "../GRU_Model/best_v6_plus_minimal_tuning_v2_final_model.keras"
SEQ_LENGTH = 10
RETRAIN_FREQUENCY = 5

# Dynamic weight parameters
WEIGHT_DECAY = 0.95
MIN_WEIGHT = 0.1
MAX_WEIGHT = 0.9

# ‡∏™‡∏£‡πâ‡∏≤‡∏á XGBoost Meta-Learner
print("üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° XGBoost Meta-Learner...")
meta_learner = UpdatedXGBoostMetaLearner()

def fetch_latest_data():
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    try:
        engine = sqlalchemy.create_engine(DB_CONNECTION)

        query = f"""
            SELECT 
                StockDetail.Date, 
                StockDetail.StockSymbol, 
                Stock.Market,  
                StockDetail.OpenPrice AS Open, 
                StockDetail.HighPrice AS High, 
                StockDetail.LowPrice AS Low, 
                StockDetail.ClosePrice AS Close, 
                StockDetail.Volume, 
                StockDetail.P_BV_Ratio,
                StockDetail.Sentiment, 
                StockDetail.Changepercen AS Change_Percent, 
                StockDetail.TotalRevenue, 
                StockDetail.QoQGrowth, 
                StockDetail.EPS, 
                StockDetail.ROE, 
                StockDetail.NetProfitMargin, 
                StockDetail.DebtToEquity, 
                StockDetail.PERatio, 
                StockDetail.Dividend_Yield, 
                StockDetail.positive_news, 
                StockDetail.negative_news, 
                StockDetail.neutral_news,
                StockDetail.PredictionClose_GRU, 
                StockDetail.PredictionClose_LSTM, 
                StockDetail.PredictionTrend_GRU, 
                StockDetail.PredictionTrend_LSTM 
            FROM StockDetail
            LEFT JOIN Stock ON StockDetail.StockSymbol = Stock.StockSymbol
            WHERE Stock.Market in ('America','Thailand')
            AND StockDetail.Date >= CURDATE() - INTERVAL 350 DAY
            ORDER BY StockDetail.StockSymbol, StockDetail.Date ASC;
        """

        df = pd.read_sql(query, engine)
        engine.dispose()
        
        print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å DB:")
        print(f"   üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {df['Date'].min()} ‡∏ñ‡∏∂‡∏á {df['Date'].max()}")
        print(f"   üè∑Ô∏è TRUE data:")
        true_data = df[df['StockSymbol'] == 'TRUE']
        if not true_data.empty:
            print(f"      üìÖ TRUE ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {true_data['Date'].min()} ‡∏ñ‡∏∂‡∏á {true_data['Date'].max()}")
            print(f"      üìã ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(true_data)} ‡∏ß‡∏±‡∏ô")
            print(f"      üí∞ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {true_data.iloc[-1]['Date']} = {true_data.iloc[-1]['Close']}")
        

        if df.empty:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà")
            return df

        # Data processing
        df['Date'] = pd.to_datetime(df['Date'])
        
        # üîß Debug ‡∏´‡∏•‡∏±‡∏á convert datetime
        print(f"\nüìä ‡∏´‡∏•‡∏±‡∏á convert datetime:")
        true_data = df[df['StockSymbol'] == 'TRUE']
        if not true_data.empty:
            print(f"   üè∑Ô∏è TRUE data: {true_data['Date'].min()} ‡∏ñ‡∏∂‡∏á {true_data['Date'].max()} ({len(true_data)} ‡∏ß‡∏±‡∏ô)")
        
        # Fill missing dates for each stock
        grouped = df.groupby('StockSymbol')
        filled_dfs = []
        
        for name, group in grouped:
            if name == 'TRUE':  # üîß Debug ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ TRUE
                print(f"\nüîç Processing TRUE:")
                print(f"   üìÖ ‡∏Å‡πà‡∏≠‡∏ô fill: {group['Date'].min()} ‡∏ñ‡∏∂‡∏á {group['Date'].max()} ({len(group)} ‡∏ß‡∏±‡∏ô)")
                print(f"   üí∞ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á: {group['Close'].notna().sum()} ‡∏ß‡∏±‡∏ô")
                print(f"   üîç ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {group.iloc[-1]['Date']} (Close: {group.iloc[-1]['Close']})")
            
            # Create complete date range for this stock
            all_dates = pd.date_range(start=group['Date'].min(), end=group['Date'].max(), freq='D')
            temp_df = pd.DataFrame({'Date': all_dates})
            temp_df['StockSymbol'] = name
            # Merge with original data
            merged = pd.merge(temp_df, group, on=['StockSymbol', 'Date'], how='left')
            
            if name == 'TRUE':  # üîß Debug ‡∏´‡∏•‡∏±‡∏á merge
                print(f"   üìÖ ‡∏´‡∏•‡∏±‡∏á merge: {merged['Date'].min()} ‡∏ñ‡∏∂‡∏á {merged['Date'].max()} ({len(merged)} ‡∏ß‡∏±‡∏ô)")
                print(f"   üí∞ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏Å‡πà‡∏≠‡∏ô fill: {merged['Close'].notna().sum()} ‡∏ß‡∏±‡∏ô")
                print(f"   üîç ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {merged.iloc[-1]['Date']} (Close: {merged.iloc[-1]['Close']})")
            
            # Forward fill missing values
            financial_cols = [
                'TotalRevenue', 'QoQGrowth', 'EPS', 'ROE',
                'NetProfitMargin', 'DebtToEquity', 'PERatio', 'Dividend_Yield'
            ]
            merged[financial_cols] = merged[financial_cols].fillna(0)
            merged = merged.ffill()
            
            if name == 'TRUE':  # üîß Debug ‡∏´‡∏•‡∏±‡∏á ffill
                print(f"   üìÖ ‡∏´‡∏•‡∏±‡∏á ffill: {merged['Date'].min()} ‡∏ñ‡∏∂‡∏á {merged['Date'].max()} ({len(merged)} ‡∏ß‡∏±‡∏ô)")
                print(f"   üí∞ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {merged.iloc[-1]['Date']} = {merged.iloc[-1]['Close']}")
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö critical columns
                critical_check = merged.iloc[-1][['Open', 'High', 'Low', 'Close']].isna()
                print(f"   üîç Critical columns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {critical_check.to_dict()}")
            
            filled_dfs.append(merged)
        
        df = pd.concat(filled_dfs, ignore_index=True)
        
        # üîß Debug ‡∏´‡∏•‡∏±‡∏á concat
        print(f"\nüìä ‡∏´‡∏•‡∏±‡∏á fill missing dates:")
        true_data = df[df['StockSymbol'] == 'TRUE']
        if not true_data.empty:
            print(f"   üè∑Ô∏è TRUE data: {true_data['Date'].min()} ‡∏ñ‡∏∂‡∏á {true_data['Date'].max()} ({len(true_data)} ‡∏ß‡∏±‡∏ô)")
            print(f"   üí∞ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {true_data.iloc[-1]['Date']} = {true_data.iloc[-1]['Close']}")
        
        # Calculate technical indicators for each stock - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç DeprecationWarning
        def calculate_indicators(group):
            if len(group) < 14:
                return group
                
            try:
                # Calculate RSI
                group['RSI'] = ta.momentum.RSIIndicator(group['Close'], window=14).rsi()
                
                # Calculate EMAs
                group['EMA_12'] = group['Close'].ewm(span=12, adjust=False).mean()
                group['EMA_26'] = group['Close'].ewm(span=26, adjust=False).mean()
                group['EMA_10'] = group['Close'].ewm(span=10, adjust=False).mean()
                group['EMA_20'] = group['Close'].ewm(span=20, adjust=False).mean()
                
                # Calculate SMAs
                group['SMA_50'] = group['Close'].rolling(window=50).mean()
                group['SMA_200'] = group['Close'].rolling(window=200).mean()
                
                # Calculate MACD
                group['MACD'] = group['EMA_12'] - group['EMA_26']
                group['MACD_Signal'] = group['MACD'].rolling(window=9).mean()
                
                # Calculate ATR
                if len(group) >= 14:
                    atr = ta.volatility.AverageTrueRange(high=group['High'], low=group['Low'], close=group['Close'], window=14)
                    group['ATR'] = atr.average_true_range()
                
                # Calculate Bollinger Bands
                bollinger = ta.volatility.BollingerBands(group['Close'], window=20, window_dev=2)
                group['Bollinger_High'] = bollinger.bollinger_hband()
                group['Bollinger_Low'] = bollinger.bollinger_lband()
                
                # Convert Sentiment to numerical values
                group['Sentiment'] = group['Sentiment'].map({'Positive': 1, 'Negative': -1, 'Neutral': 0})
                
                # Calculate Keltner Channel
                keltner = ta.volatility.KeltnerChannel(high=group['High'], low=group['Low'], close=group['Close'], window=20, window_atr=10)
                group['Keltner_High'] = keltner.keltner_channel_hband()
                group['Keltner_Low'] = keltner.keltner_channel_lband()
                group['Keltner_Middle'] = keltner.keltner_channel_mband()
                
                # Calculate Chaikin Volatility
                window_cv = 10
                group['High_Low_Diff'] = group['High'] - group['Low']
                group['High_Low_EMA'] = group['High_Low_Diff'].ewm(span=window_cv, adjust=False).mean()
                group['Chaikin_Vol'] = group['High_Low_EMA'].pct_change(periods=window_cv) * 100
                
                # Calculate Donchian Channel
                window_dc = 20
                group['Donchian_High'] = group['High'].rolling(window=window_dc).max()
                group['Donchian_Low'] = group['Low'].rolling(window=window_dc).min()
                
                # Calculate PSAR
                psar = ta.trend.PSARIndicator(high=group['High'], low=group['Low'], close=group['Close'], step=0.02, max_step=0.2)
                group['PSAR'] = psar.psar()
                
                # Add date-related features
                group['DayOfWeek'] = group['Date'].dt.dayofweek
                group['Is_Day_0'] = (group['Date'].dt.dayofweek == 0).astype(int)  # Monday
                group['Is_Day_4'] = (group['Date'].dt.dayofweek == 4).astype(int)  # Friday
                group['DayOfMonth'] = group['Date'].dt.day
                group['IsFirstHalfOfMonth'] = (group['Date'].dt.day <= 15).astype(int)
                group['IsSecondHalfOfMonth'] = (group['Date'].dt.day > 15).astype(int)
                
            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indicators ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {group['StockSymbol'].iloc[0] if not group.empty else 'Unknown'}: {e}")
            
            return group
        
        # Apply indicators calculation to each stock group - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç DeprecationWarning
        df = df.groupby('StockSymbol', group_keys=False).apply(calculate_indicators)
        df = df.reset_index(drop=True)
        
        # üîß Debug ‡∏´‡∏•‡∏±‡∏á calculate indicators
        print(f"\nüìä ‡∏´‡∏•‡∏±‡∏á calculate indicators:")
        true_data = df[df['StockSymbol'] == 'TRUE']
        if not true_data.empty:
            print(f"   üè∑Ô∏è TRUE data: {true_data['Date'].min()} ‡∏ñ‡∏∂‡∏á {true_data['Date'].max()} ({len(true_data)} ‡∏ß‡∏±‡∏ô)")
            print(f"   üí∞ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {true_data.iloc[-1]['Date']} = {true_data.iloc[-1]['Close']}")
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö critical columns ‡∏´‡∏•‡∏±‡∏á indicators
            critical_check = true_data.iloc[-1][['Open', 'High', 'Low', 'Close']].isna()
            print(f"   üîç Critical columns: {critical_check.to_dict()}")
        
        # Handle missing values
        critical_columns = ['Open', 'High', 'Low', 'Close']
        before_drop = len(df)
        before_drop_true = len(df[df['StockSymbol'] == 'TRUE'])
        
        df = df.dropna(subset=critical_columns)
        
        after_drop = len(df)
        after_drop_true = len(df[df['StockSymbol'] == 'TRUE'])
        
        # üîß Debug ‡∏´‡∏•‡∏±‡∏á dropna
        print(f"\nüìä ‡∏´‡∏•‡∏±‡∏á dropna critical columns:")
        print(f"   üìã ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å {before_drop - after_drop} ‡πÅ‡∏ñ‡∏ß ({before_drop} ‚Üí {after_drop})")
        print(f"   üè∑Ô∏è TRUE: ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å {before_drop_true - after_drop_true} ‡πÅ‡∏ñ‡∏ß ({before_drop_true} ‚Üí {after_drop_true})")
        
        true_data = df[df['StockSymbol'] == 'TRUE']
        if not true_data.empty:
            print(f"   üè∑Ô∏è TRUE data: {true_data['Date'].min()} ‡∏ñ‡∏∂‡∏á {true_data['Date'].max()} ({len(true_data)} ‡∏ß‡∏±‡∏ô)")
            print(f"   üí∞ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {true_data.iloc[-1]['Date']} = {true_data.iloc[-1]['Close']}")
        else:
            print(f"   ‚ùå TRUE data ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏´‡∏°‡∏î!")
        
        # Fill NaN values
        df = df.ffill().bfill()
        
        # Fill remaining NaN with 0 for technical indicators
        technical_columns = ['RSI', 'MACD', 'MACD_Signal', 'ATR', 
                            'Bollinger_High', 'Bollinger_Low', 'SMA_50', 'SMA_200',
                            'EMA_10', 'EMA_20', 'Keltner_High', 'Keltner_Low', 'Keltner_Middle',
                            'Chaikin_Vol', 'Donchian_High', 'Donchian_Low', 'PSAR']
        
        for col in technical_columns:
            if col in df.columns:
                df[col] = df[col].fillna(0)
        
        # Fill remaining NaN with 0
        df = df.fillna(0)
        
        # üîß Final debug
        print(f"\nüìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢:")
        true_data = df[df['StockSymbol'] == 'TRUE']
        if not true_data.empty:
            print(f"   üè∑Ô∏è TRUE data: {true_data['Date'].min()} ‡∏ñ‡∏∂‡∏á {true_data['Date'].max()} ({len(true_data)} ‡∏ß‡∏±‡∏ô)")
            print(f"   üí∞ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {true_data.iloc[-1]['Date']} = {true_data.iloc[-1]['Close']}")
        
        print(f"\n‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: {len(df)} ‡πÅ‡∏ñ‡∏ß, {len(df['StockSymbol'].unique())} ‡∏´‡∏∏‡πâ‡∏ô")
        print(f"üìä Technical indicators ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ: {[col for col in technical_columns if col in df.columns]}")
        
        return df
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def calculate_realistic_features(df):
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì features ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ default values"""
    df_enhanced = df.copy()
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Price Agreement ‡∏à‡∏£‡∏¥‡∏á‡πÜ
    if 'PredictionClose_LSTM' in df.columns and 'PredictionClose_GRU' in df.columns:
        lstm_pred = df['PredictionClose_LSTM']
        gru_pred = df['PredictionClose_GRU']
        
        price_diff_pct = abs(lstm_pred - gru_pred) / df['Close'] * 100
        df_enhanced['Price_Agreement'] = np.exp(-price_diff_pct / 10)
        df_enhanced['LSTM_Error_Pct'] = np.minimum(price_diff_pct, 20.0)
        df_enhanced['GRU_Error_Pct'] = np.minimum(price_diff_pct, 20.0)
    else:
        df_enhanced['Price_Agreement'] = 0.7
        df_enhanced['LSTM_Error_Pct'] = 10.0
        df_enhanced['GRU_Error_Pct'] = 10.0
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Direction Confidence
    if 'PredictionTrend_LSTM' in df.columns and 'PredictionTrend_GRU' in df.columns:
        lstm_conf = abs(df['PredictionTrend_LSTM'] - 0.5) * 2
        gru_conf = abs(df['PredictionTrend_GRU'] - 0.5) * 2
        df_enhanced['Dir_Confidence'] = (lstm_conf + gru_conf) / 2
    else:
        df_enhanced['Dir_Confidence'] = 0.5
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Volume Normalized
    if 'Volume' in df.columns:
        volume_max = df['Volume'].rolling(window=30, min_periods=1).max()
        df_enhanced['Volume_Normalized'] = df['Volume'] / (volume_max + 1e-8)
    else:
        df_enhanced['Volume_Normalized'] = 0.5
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Price Volatility
    if 'ATR' in df.columns:
        df_enhanced['Price_Volatility'] = df['ATR'] / (df['Close'] + 1e-8)
    else:
        price_std = df['Close'].rolling(window=14, min_periods=1).std()
        df_enhanced['Price_Volatility'] = price_std / (df['Close'] + 1e-8)
    
    return df_enhanced

def calculate_dynamic_weights(df_ticker, price_weight_factor=0.6, direction_weight_factor=0.4):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì dynamic weight ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU ‡∏ï‡∏≤‡∏° performance ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
    """
    
    # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 15 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì weight
    recent_data = df_ticker.tail(15)
    
    if len(recent_data) < 5:
        # ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏ä‡πâ weight ‡πÄ‡∏ó‡πà‡∏≤‡πÜ ‡∏Å‡∏±‡∏ô
        return 0.5, 0.5
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ columns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    required_cols = ['PredictionClose_LSTM', 'PredictionClose_GRU', 
                     'PredictionTrend_LSTM', 'PredictionTrend_GRU']
    
    if not all(col in df_ticker.columns for col in required_cols):
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• predictions ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dynamic weighting")
        return 0.5, 0.5
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì price performance
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á predictions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö error calculation
        lstm_predictions = recent_data['PredictionClose_LSTM'].dropna()
        gru_predictions = recent_data['PredictionClose_GRU'].dropna()
        actual_prices = recent_data['Close'].dropna()
        
        if len(lstm_predictions) >= 3 and len(gru_predictions) >= 3 and len(actual_prices) >= 3:
            # ‡πÉ‡∏ä‡πâ length ‡∏ó‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á index mismatch
            min_len = min(len(lstm_predictions), len(gru_predictions), len(actual_prices))
            
            if min_len >= 2:
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì MAE ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö price predictions - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç index alignment
                lstm_pred_vals = lstm_predictions.iloc[:min_len-1].values
                gru_pred_vals = gru_predictions.iloc[:min_len-1].values
                actual_vals_next = actual_prices.iloc[1:min_len].values
                
                lstm_price_error = np.mean(np.abs(lstm_pred_vals - actual_vals_next))
                gru_price_error = np.mean(np.abs(gru_pred_vals - actual_vals_next))
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì direction accuracy - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç index alignment
                actual_vals_current = actual_prices.iloc[:min_len-1].values
                
                lstm_dir_pred = (lstm_pred_vals > actual_vals_current).astype(int)
                gru_dir_pred = (gru_pred_vals > actual_vals_current).astype(int)
                actual_dir = (actual_vals_next > actual_vals_current).astype(int)
                
                lstm_dir_acc = np.mean(lstm_dir_pred == actual_dir)
                gru_dir_acc = np.mean(gru_dir_pred == actual_dir)
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì weights
                total_price_error = lstm_price_error + gru_price_error
                if total_price_error > 0:
                    lstm_price_score = gru_price_error / total_price_error  # ‡∏Å‡∏•‡∏±‡∏ö‡∏Ñ‡πà‡∏≤
                    gru_price_score = lstm_price_error / total_price_error
                else:
                    lstm_price_score = 0.5
                    gru_price_score = 0.5
                
                total_dir_acc = lstm_dir_acc + gru_dir_acc
                if total_dir_acc > 0:
                    lstm_dir_score = lstm_dir_acc / total_dir_acc
                    gru_dir_score = gru_dir_acc / total_dir_acc
                else:
                    lstm_dir_score = 0.5
                    gru_dir_score = 0.5
                
                # ‡∏£‡∏ß‡∏° weights
                lstm_weight = (price_weight_factor * lstm_price_score + 
                              direction_weight_factor * lstm_dir_score)
                gru_weight = (price_weight_factor * gru_price_score + 
                             direction_weight_factor * gru_dir_score)
                
                # Normalize
                total_weight = lstm_weight + gru_weight
                if total_weight > 0:
                    lstm_weight = lstm_weight / total_weight
                    gru_weight = gru_weight / total_weight
                else:
                    lstm_weight = 0.5
                    gru_weight = 0.5
                
                # Apply constraints
                lstm_weight = max(MIN_WEIGHT, min(MAX_WEIGHT, lstm_weight))
                gru_weight = max(MIN_WEIGHT, min(MAX_WEIGHT, gru_weight))
                
                # Re-normalize
                total_weight = lstm_weight + gru_weight
                lstm_weight = lstm_weight / total_weight
                gru_weight = gru_weight / total_weight
                
                return lstm_weight, gru_weight
            
    except Exception as e:
        print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì dynamic weights: {e}")
    
    return 0.5, 0.5


def load_training_scalers(scaler_path="../LSTM_model/ticker_scalers.pkl"):
    """
    ‡πÇ‡∏´‡∏•‡∏î ticker_scalers ‡∏à‡∏≤‡∏Å‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô
    ‚úÖ ‡πÉ‡∏ä‡πâ scalers ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô
    ‚úÖ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU
    """
    print(f"üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î training scalers ‡∏à‡∏≤‡∏Å {scaler_path}...")
    
    if not os.path.exists(scaler_path):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {scaler_path}")
        return None, False
    
    try:
        ticker_scalers = joblib.load(scaler_path)
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î training scalers ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        print(f"   üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô tickers: {len(ticker_scalers)}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö structure
        sample_ticker_id = list(ticker_scalers.keys())[0]
        sample_scaler = ticker_scalers[sample_ticker_id]
        
        required_keys = ['feature_scaler', 'price_scaler']
        if all(key in sample_scaler for key in required_keys):
            print(f"   ‚úÖ Structure ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {list(sample_scaler.keys())}")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• scalers
            print(f"   üìã Ticker scalers:")
            for i, (ticker_id, scaler_info) in enumerate(list(ticker_scalers.items())[:5]):
                ticker_name = scaler_info.get('ticker', f'ID_{ticker_id}')
                print(f"      {ticker_name} (ID: {ticker_id})")
            
            if len(ticker_scalers) > 5:
                print(f"      ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(ticker_scalers) - 5} tickers")
            
            return ticker_scalers, True
        else:
            print(f"   ‚ùå Structure ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: ‡∏Ç‡∏≤‡∏î {[k for k in required_keys if k not in sample_scaler]}")
            return None, False
            
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î: {e}")
        return None, False

def validate_ticker_scalers(ticker_scalers, df, feature_columns):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ ticker_scalers ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
    """
    print(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏Ç‡∏≠‡∏á scalers...")
    
    valid_scalers = {}
    validation_results = []
    
    available_tickers = df['StockSymbol'].unique()
    
    for ticker in available_tickers:
        ticker_data = df[df['StockSymbol'] == ticker]
        if len(ticker_data) == 0:
            continue
            
        # ‡∏´‡∏≤ ticker_id
        ticker_id = ticker_data['Ticker_ID'].iloc[0]
        
        if ticker_id not in ticker_scalers:
            print(f"   ‚ö†Ô∏è {ticker}: ‡πÑ‡∏°‡πà‡∏û‡∏ö scaler ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ticker_ID {ticker_id}")
            validation_results.append({'ticker': ticker, 'status': 'missing_scaler'})
            continue
        
        try:
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö feature scaler
            test_features = ticker_data[feature_columns].iloc[:3].fillna(0)
            scaler_info = ticker_scalers[ticker_id]
            
            transformed_features = scaler_info['feature_scaler'].transform(test_features)
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö price scaler
            test_prices = ticker_data[['Close']].iloc[:3]
            transformed_prices = scaler_info['price_scaler'].transform(test_prices)
            
            valid_scalers[ticker_id] = scaler_info
            validation_results.append({'ticker': ticker, 'status': 'valid'})
            print(f"   ‚úÖ {ticker}: Scaler ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ")
            
        except Exception as e:
            print(f"   ‚ùå {ticker}: Scaler ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ - {e}")
            validation_results.append({'ticker': ticker, 'status': 'invalid', 'error': str(e)})
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    valid_count = len([r for r in validation_results if r['status'] == 'valid'])
    total_count = len(validation_results)
    
    print(f"\nüìä ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:")
    print(f"   ‚úÖ ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ: {valid_count}/{total_count} tickers")
    print(f"   ‚ùå ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {total_count - valid_count}/{total_count} tickers")
    
    return valid_scalers, validation_results


def predict_with_consistent_scalers(model_lstm, model_gru, df, feature_columns, 
                                  ticker_scalers, ticker_encoder, market_encoder, seq_length=10):
    """
    ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ scalers ‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô
    ‚úÖ LSTM ‡πÅ‡∏•‡∏∞ GRU ‡πÉ‡∏ä‡πâ scaler ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
    ‚úÖ ‡πÉ‡∏ä‡πâ per-ticker scalers ‡∏à‡∏≤‡∏Å‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô
    """
    print(f"\nüîÆ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Consistent Scalers...")
    print(f"   üìä ‡πÉ‡∏ä‡πâ per-ticker scalers ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô")
    print(f"   üîÑ LSTM ‡πÅ‡∏•‡∏∞ GRU ‡πÉ‡∏ä‡πâ scaler ‡∏ä‡∏∏‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô")
    
    future_predictions = []
    tickers = df['StockSymbol'].unique()
    
    for ticker in tickers:
        print(f"\nüìä ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ {ticker}...")
        
        df_ticker = df[df['StockSymbol'] == ticker].sort_values('Date').reset_index(drop=True)
        
        if len(df_ticker) < seq_length:
            print(f"   ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠: {len(df_ticker)} < {seq_length}")
            continue
        
        try:
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            latest_data = df_ticker.iloc[-seq_length:]
            ticker_id = latest_data['Ticker_ID'].iloc[-1]
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö scaler
            if ticker_id not in ticker_scalers:
                print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö scaler ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {ticker} (ID: {ticker_id})")
                continue
            
            # ‚úÖ ‡πÉ‡∏ä‡πâ scaler ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU
            scaler_info = ticker_scalers[ticker_id]
            feature_scaler = scaler_info['feature_scaler']
            price_scaler = scaler_info['price_scaler']
            
            print(f"   üîß ‡πÉ‡∏ä‡πâ scaler ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {ticker}:")
            print(f"      üìä Feature scaler: {type(feature_scaler).__name__}")
            print(f"      üí∞ Price scaler: {type(price_scaler).__name__}")
            
            # Scale features
            features_scaled = feature_scaler.transform(latest_data[feature_columns])
            ticker_ids = latest_data["Ticker_ID"].values
            market_ids = latest_data["Market_ID"].values
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° input tensors
            X_feat = features_scaled.reshape(1, seq_length, -1)
            X_ticker = ticker_ids.reshape(1, seq_length)
            X_market = market_ids.reshape(1, seq_length)
            
            # === LSTM PREDICTION ===
            print(f"   üî¥ LSTM Prediction...")
            pred_output_lstm = model_lstm.predict([X_feat, X_ticker, X_market], verbose=0)
            pred_price_lstm_scaled = np.squeeze(pred_output_lstm[0])
            pred_direction_lstm = np.squeeze(pred_output_lstm[1])
            
            # ‚úÖ ‡πÉ‡∏ä‡πâ price_scaler ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
            pred_price_lstm = price_scaler.inverse_transform(
                pred_price_lstm_scaled.reshape(-1, 1)
            ).flatten()[0]
            
            print(f"      üí∞ LSTM Price: {pred_price_lstm:.2f}")
            print(f"      üìà LSTM Direction: {pred_direction_lstm:.4f}")
            
            # === GRU PREDICTION ===
            print(f"   üîµ GRU Prediction...")
            pred_output_gru = model_gru.predict([X_feat, X_ticker, X_market], verbose=0)
            pred_price_gru_scaled = np.squeeze(pred_output_gru[0])
            pred_direction_gru = np.squeeze(pred_output_gru[1])
            
            # ‚úÖ ‡πÉ‡∏ä‡πâ price_scaler ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà global scaler)
            pred_price_gru = price_scaler.inverse_transform(
                pred_price_gru_scaled.reshape(-1, 1)
            ).flatten()[0]
            
            print(f"      üí∞ GRU Price: {pred_price_gru:.2f}")
            print(f"      üìà GRU Direction: {pred_direction_gru:.4f}")
            
            # === CONSISTENCY CHECK ===
            current_price = df_ticker.iloc[-1]['Close']
            price_diff = abs(pred_price_lstm - pred_price_gru)
            price_diff_pct = (price_diff / current_price) * 100
            
            direction_lstm = 1 if pred_direction_lstm > 0.5 else 0
            direction_gru = 1 if pred_direction_gru > 0.5 else 0
            direction_agreement = direction_lstm == direction_gru
            
            print(f"   ü§ù Consistency Check:")
            print(f"      üí∞ Price difference: {price_diff:.2f} ({price_diff_pct:.2f}%)")
            print(f"      üìä Direction agreement: {direction_agreement}")
            
            # === ENSEMBLE PREDICTION ===
            # Simple weighted average (‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö weights ‡πÑ‡∏î‡πâ)
            ensemble_price = (pred_price_lstm + pred_price_gru) / 2
            ensemble_direction_prob = (pred_direction_lstm + pred_direction_gru) / 2
            ensemble_direction = 1 if ensemble_direction_prob > 0.5 else 0
            
            # Calculate confidence
            confidence = min(
                1.0 - (price_diff_pct / 100),  # Price agreement
                abs(ensemble_direction_prob - 0.5) * 2  # Direction confidence
            )
            
            print(f"   üéØ Ensemble Result:")
            print(f"      üí∞ Price: {ensemble_price:.2f}")
            print(f"      üìä Direction: {ensemble_direction} (prob: {ensemble_direction_prob:.4f})")
            print(f"      üéØ Confidence: {confidence:.3f}")
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            last_date = df_ticker['Date'].max()
            next_date = last_date + pd.Timedelta(days=1)
            
            prediction_result = {
                'StockSymbol': ticker,
                'Date': next_date,
                'Current_Price': current_price,
                'LSTM_Price': pred_price_lstm,
                'GRU_Price': pred_price_gru,
                'Ensemble_Price': ensemble_price,
                'LSTM_Direction': direction_lstm,
                'GRU_Direction': direction_gru,
                'Ensemble_Direction': ensemble_direction,
                'LSTM_Direction_Prob': pred_direction_lstm,
                'GRU_Direction_Prob': pred_direction_gru,
                'Ensemble_Direction_Prob': ensemble_direction_prob,
                'Price_Difference_Pct': price_diff_pct,
                'Direction_Agreement': direction_agreement,
                'Confidence': confidence,
                'Scaler_Used': f"Ticker_{ticker_id}",
                'Consistent_Scaling': True
            }
            
            future_predictions.append(prediction_result)
            print(f"   ‚úÖ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ {ticker} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            
        except Exception as e:
            print(f"   ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô {ticker}: {e}")
            continue
    
    print(f"\nüéâ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {len(future_predictions)}/{len(tickers)} ‡∏´‡∏∏‡πâ‡∏ô")
    return pd.DataFrame(future_predictions)

def save_predictions_simple(predictions_df):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢
    ‡πÄ‡∏Å‡πá‡∏ö: ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà, ‡∏´‡∏∏‡πâ‡∏ô, ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (LSTM, GRU, Ensemble), ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (LSTM, GRU, Ensemble)
    """
    if predictions_df.empty:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
        return False

    try:
        engine = sqlalchemy.create_engine(DB_CONNECTION)
        
        with engine.connect() as connection:
            success_count = 0
            created_count = 0
            updated_count = 0
            
            for _, row in predictions_df.iterrows():
                try:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ record ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    check_query = sqlalchemy.text("""
                        SELECT COUNT(*) FROM StockDetail 
                        WHERE StockSymbol = :symbol AND Date = :date
                    """)
                    
                    result = connection.execute(check_query, {
                        'symbol': row['StockSymbol'],
                        'date': row['Date'].strftime('%Y-%m-%d')
                    })
                    exists = result.scalar()
                    
                    if exists > 0:
                        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï predictions ‡∏ó‡∏±‡πâ‡∏á LSTM, GRU, ‡πÅ‡∏•‡∏∞ Ensemble
                        update_query = sqlalchemy.text("""
                            UPDATE StockDetail
                            SET PredictionClose_LSTM = :lstm_price,
                                PredictionTrend_LSTM = :lstm_trend,
                                PredictionClose_GRU = :gru_price,
                                PredictionTrend_GRU = :gru_trend,
                                PredictionClose_Ensemble = :ensemble_price, 
                                PredictionTrend_Ensemble = :ensemble_trend
                            WHERE StockSymbol = :symbol AND Date = :date
                        """)
                        
                        connection.execute(update_query, {
                            'lstm_price': float(row.get('LSTM_Prediction', row['Predicted_Price'])),
                            'lstm_trend': int(row.get('LSTM_Direction', row['Predicted_Direction'])),
                            'gru_price': float(row.get('GRU_Prediction', row['Predicted_Price'])),
                            'gru_trend': int(row.get('GRU_Direction', row['Predicted_Direction'])),
                            'ensemble_price': float(row['Predicted_Price']),
                            'ensemble_trend': int(row['Predicted_Direction']),
                            'symbol': row['StockSymbol'],
                            'date': row['Date'].strftime('%Y-%m-%d')
                        })
                        print(f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï {row['StockSymbol']} (LSTM+GRU+Ensemble)")
                        updated_count += 1
                        
                    else:
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á record ‡πÉ‡∏´‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° predictions ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                        insert_query = sqlalchemy.text("""
                            INSERT INTO StockDetail 
                            (StockSymbol, Date, 
                             PredictionClose_LSTM, PredictionTrend_LSTM,
                             PredictionClose_GRU, PredictionTrend_GRU,
                             PredictionClose_Ensemble, PredictionTrend_Ensemble)
                            VALUES 
                            (:symbol, :date, 
                             :lstm_price, :lstm_trend,
                             :gru_price, :gru_trend,
                             :ensemble_price, :ensemble_trend)
                        """)
                        
                        connection.execute(insert_query, {
                            'symbol': row['StockSymbol'],
                            'date': row['Date'].strftime('%Y-%m-%d'),
                            'lstm_price': float(row.get('LSTM_Prediction', row['Predicted_Price'])),
                            'lstm_trend': int(row.get('LSTM_Direction', row['Predicted_Direction'])),
                            'gru_price': float(row.get('GRU_Prediction', row['Predicted_Price'])),
                            'gru_trend': int(row.get('GRU_Direction', row['Predicted_Direction'])),
                            'ensemble_price': float(row['Predicted_Price']),
                            'ensemble_trend': int(row['Predicted_Direction'])
                        })
                        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà {row['StockSymbol']} (LSTM+GRU+Ensemble)")
                        created_count += 1
                    
                    success_count += 1
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {row['StockSymbol']}: {e}")
                    continue
            
            # Commit ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
            connection.commit()
            
            print(f"\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
            print(f"   üìä ‡∏£‡∏ß‡∏°: {success_count}/{len(predictions_df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            if updated_count > 0:
                print(f"   üîÑ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï: {updated_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            if created_count > 0:
                print(f"   ‚ûï ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà: {created_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            print(f"   üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: LSTM + GRU + Ensemble predictions")
            
            return success_count > 0
            
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
        import traceback
        traceback.print_exc()
        return False

# ======================== MAIN EXECUTION ========================

# Add the fixed create_ticker_scalers function
def create_ticker_scalers_fixed(df, feature_columns, scaler_file_path="../LSTM_model/ticker_scalers.pkl"):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á ticker scalers ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô"""
    
    def clean_data_for_scalers(df, feature_columns):
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á scalers"""
        print("üßπ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scalers...")
        
        df_clean = df.copy()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        for col in feature_columns:
            if col in df_clean.columns:
                try:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
                    col_data = df_clean[col].astype(str)
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÄ‡∏ä‡πà‡∏ô 49.0349.03)
                    problematic_mask = col_data.str.contains(r'\d+\.\d+\d+\.\d+', regex=True, na=False)
                    
                    if problematic_mask.any():
                        print(f"   ‚ö†Ô∏è ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå {col}: {problematic_mask.sum()} ‡πÅ‡∏ñ‡∏ß")
                        
                        # ‡πÅ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô (‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
                        def extract_first_number(x):
                            try:
                                if pd.isna(x):
                                    return 0.0
                                x_str = str(x)
                                # ‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
                                import re
                                match = re.search(r'^\d+\.?\d*', x_str)
                                if match:
                                    return float(match.group())
                                else:
                                    return 0.0
                            except:
                                return 0.0
                        
                        df_clean[col] = col_data.apply(extract_first_number)
                    else:
                        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numeric ‡∏õ‡∏Å‡∏ï‡∏¥
                        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
                    
                    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡πÅ‡∏•‡∏∞ inf ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                    col_mean = df_clean[col].replace([np.inf, -np.inf], np.nan).mean()
                    if pd.isna(col_mean):
                        col_mean = 0.0
                    
                    df_clean[col] = df_clean[col].replace([np.inf, -np.inf, np.nan], col_mean)
                    
                    print(f"   ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î {col}: {df_clean[col].dtype}, range: {df_clean[col].min():.3f} - {df_clean[col].max():.3f}")
                    
                except Exception as e:
                    print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î {col}: {e}")
                    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                    df_clean[col] = 0.0
        
        return df_clean
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô
    df_clean = clean_data_for_scalers(df, feature_columns)
    
    ticker_scalers = {}
    tickers = df_clean['StockSymbol'].unique()
    
    print("üîß Creating/loading individual scalers for each ticker...")
    
    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î scalers ‡πÄ‡∏Å‡πà‡∏≤
    pre_trained_scalers = {}
    try:
        if os.path.exists(scaler_file_path):
            import joblib
            pre_trained_scalers = joblib.load(scaler_file_path)
            print(f"‚úÖ Loaded pre-trained scalers from {scaler_file_path} for {len(pre_trained_scalers)} tickers")
        else:
            print(f"‚ö†Ô∏è No pre-trained scalers found at {scaler_file_path}, creating new scalers")
    except Exception as e:
        print(f"‚ùå Error loading pre-trained scalers: {e}, creating new scalers")
    
    for ticker in tickers:
        df_ticker = df_clean[df_clean['StockSymbol'] == ticker].copy()
        
        if len(df_ticker) < 30:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 30 ‡∏ß‡∏±‡∏ô
            print(f"   ‚ö†Ô∏è {ticker}: Not enough data ({len(df_ticker)} days), skipping...")
            continue
        
        ticker_id = df_ticker['Ticker_ID'].iloc[0]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö pre-trained scaler
        if ticker_id in pre_trained_scalers:
            scaler_info = pre_trained_scalers[ticker_id]
            if all(key in scaler_info for key in ['ticker_symbol', 'feature_scaler', 'price_scaler']):
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ scaler ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                try:
                    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö transform ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                    test_data = df_ticker[feature_columns].iloc[:5].fillna(df_ticker[feature_columns].mean())
                    _ = scaler_info['feature_scaler'].transform(test_data)
                    _ = scaler_info['price_scaler'].transform(df_ticker[['Close']].iloc[:5])
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° data_points ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏≤‡∏î
                    if 'data_points' not in scaler_info:
                        scaler_info['data_points'] = len(df_ticker)
                    
                    ticker_scalers[ticker_id] = scaler_info
                    print(f"   ‚úÖ {ticker} (ID: {ticker_id}): Using pre-trained scaler with {scaler_info['data_points']} data points")
                    continue
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è {ticker} (ID: {ticker_id}): Pre-trained scaler failed test ({e}), creating new one")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler ‡πÉ‡∏´‡∏°‡πà
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á scaler
            feature_data = df_ticker[feature_columns].copy()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô numeric ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            non_numeric_cols = []
            for col in feature_columns:
                if col in feature_data.columns:
                    if not pd.api.types.is_numeric_dtype(feature_data[col]):
                        non_numeric_cols.append(col)
            
            if non_numeric_cols:
                print(f"   ‚ö†Ô∏è {ticker}: Non-numeric columns found: {non_numeric_cols}")
                for col in non_numeric_cols:
                    feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce')
            
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
            feature_data = feature_data.fillna(feature_data.mean()).fillna(0)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ infinite values ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            feature_data = feature_data.replace([np.inf, -np.inf], 0)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á feature scaler
            feature_scaler = RobustScaler()
            feature_scaler.fit(feature_data)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á price scaler
            price_scaler = RobustScaler()
            price_data = df_ticker[['Close']].copy()
            price_data = price_data.fillna(price_data.mean()).fillna(0)
            price_data = price_data.replace([np.inf, -np.inf], price_data.mean())
            price_scaler.fit(price_data)
            
            # ‡πÄ‡∏Å‡πá‡∏ö scaler ‡πÅ‡∏•‡∏∞ metadata
            ticker_scalers[ticker_id] = {
                'ticker_symbol': ticker,
                'feature_scaler': feature_scaler,
                'price_scaler': price_scaler,
                'data_points': len(df_ticker)
            }
            
            print(f"   ‚úÖ {ticker} (ID: {ticker_id}): Created new scaler with {len(df_ticker)} data points")
            
        except Exception as e:
            print(f"   ‚ùå {ticker}: Error creating scalers - {e}")
            # ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• debug
            print(f"      Debug - Feature data shape: {df_ticker[feature_columns].shape}")
            print(f"      Debug - Feature data dtypes: {df_ticker[feature_columns].dtypes.to_dict()}")
            if len(df_ticker) > 0:
                print(f"      Debug - Sample values: {df_ticker[feature_columns].iloc[0].to_dict()}")
            continue
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scalers ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß
    try:
        import joblib
        os.makedirs(os.path.dirname(scaler_file_path), exist_ok=True)
        joblib.dump(ticker_scalers, scaler_file_path)
        print(f"üíæ Saved updated scalers to {scaler_file_path}")
    except Exception as e:
        print(f"‚ùå Error saving scalers: {e}")
    
    print(f"‚úÖ Created/loaded scalers for {len(ticker_scalers)} tickers")
    return ticker_scalers

if __name__ == "__main__":
    print("\nüöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏ö‡∏ö Enhanced 3-Layer Ensemble (Automated Mode)")
    print("üîß Using Consistent Scalers System")
    print("‚ö° ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ scalers ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô")

    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM ‡πÅ‡∏•‡∏∞ GRU
    print("\nü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM ‡πÅ‡∏•‡∏∞ GRU...")

    MODEL_LSTM_PATH = "../LSTM_model/best_v6_plus_minimal_tuning_v2_final_model.keras"
    MODEL_GRU_PATH = "../GRU_Model/best_v6_plus_minimal_tuning_v2_final_model.keras"

    if not os.path.exists(MODEL_LSTM_PATH):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM ‡∏ó‡∏µ‡πà {MODEL_LSTM_PATH}")
        sys.exit()

    if not os.path.exists(MODEL_GRU_PATH):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• GRU ‡∏ó‡∏µ‡πà {MODEL_GRU_PATH}")
        sys.exit()

    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á instance ‡∏Ç‡∏≠‡∏á WalkForwardMiniRetrainManager
        manager = WalkForwardMiniRetrainManager(
            lstm_model_path=MODEL_LSTM_PATH,
            gru_model_path=MODEL_GRU_PATH
        )
        
        # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á compile ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
        print(f"\n‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢...")
        model_lstm = manager.load_models_for_prediction(model_path=MODEL_LSTM_PATH, compile_model=False)
        model_gru = manager.load_models_for_prediction(model_path=MODEL_GRU_PATH, compile_model=False)
        
        if model_lstm is None or model_gru is None:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ")
            sys.exit()
            
        print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM ‡πÅ‡∏•‡∏∞ GRU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•
        print(f"üìä LSTM model: {len(model_lstm.layers)} layers")
        print(f"üìä GRU model: {len(model_gru.layers)} layers")
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
        sys.exit()

    # ‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    print("\nüì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    raw_df = fetch_latest_data()  # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

    if raw_df.empty:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
        sys.exit()

    print(f"üìä ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö: {len(raw_df)} ‡πÅ‡∏ñ‡∏ß ‡∏à‡∏≤‡∏Å {len(raw_df['StockSymbol'].unique())} ‡∏´‡∏∏‡πâ‡∏ô")

    # ======== FEATURE PREPARATION ========
    base_feature_columns = [
        'Open', 'High', 'Low', 'Close', 'Volume', 'Change_Percent', 'Sentiment',
        'positive_news', 'negative_news', 'neutral_news',
        'TotalRevenue', 'QoQGrowth', 'EPS', 'ROE', 'NetProfitMargin', 
        'DebtToEquity', 'PERatio', 'Dividend_Yield', 'P_BV_Ratio'
    ]

    technical_feature_columns = [
        'ATR', 'Keltner_High', 'Keltner_Low', 'Keltner_Middle', 'Chaikin_Vol',
        'Donchian_High', 'Donchian_Low', 'PSAR',
        'RSI', 'EMA_10', 'EMA_20', 'MACD', 'MACD_Signal', 
        'Bollinger_High', 'Bollinger_Low', 'SMA_50', 'SMA_200'
    ]

    # ‡∏£‡∏ß‡∏° feature columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
    available_base = [col for col in base_feature_columns if col in raw_df.columns]
    available_technical = [col for col in technical_feature_columns if col in raw_df.columns]
    feature_columns = available_base + available_technical

    print(f"üìã Available feature columns ({len(feature_columns)}): {feature_columns}")

    if len(feature_columns) < 10:
        print("‚ùå ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• features ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 10 columns")
        sys.exit()

    # ======== CONSISTENT SCALER SYSTEM ========
    print("\nüîß ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Consistent Scalers...")
    
    # 1. ‡πÇ‡∏´‡∏•‡∏î training scalers
    ticker_scalers, scalers_loaded = load_training_scalers("../LSTM_model/ticker_scalers.pkl")
    
    if not scalers_loaded:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î training scalers ‡πÑ‡∏î‡πâ")
        print("üí° ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á scalers ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô...")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á scalers ‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
        try:
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Ticker_ID ‡πÅ‡∏•‡∏∞ Market_ID
            ticker_encoder_temp = LabelEncoder()
            raw_df["Ticker_ID"] = ticker_encoder_temp.fit_transform(raw_df["StockSymbol"])
            
            us_stock = ['AAPL', 'NVDA', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'AVGO', 'TSM', 'AMD']
            thai_stock = ['ADVANC', 'TRUE', 'DITTO', 'DIF', 'INSET', 'JMART', 'INET', 'JAS', 'HUMAN']
            raw_df['Market_ID'] = raw_df['StockSymbol'].apply(
                lambda x: 0 if x in us_stock else 1 if x in thai_stock else 2
            )
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á scalers ‡πÉ‡∏´‡∏°‡πà
            ticker_scalers = create_ticker_scalers_fixed(raw_df, feature_columns)
            
            if len(ticker_scalers) == 0:
                print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á scalers ‡πÑ‡∏î‡πâ")
                sys.exit()
                
            scalers_loaded = True
            print("‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á scalers ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á scalers: {e}")
            sys.exit()
    
    # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prediction
    prediction_df = raw_df.copy()
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Ticker_ID ‡πÅ‡∏•‡∏∞ Market_ID ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    if 'Ticker_ID' not in prediction_df.columns:
        ticker_encoder = LabelEncoder()
        prediction_df["Ticker_ID"] = ticker_encoder.fit_transform(prediction_df["StockSymbol"])
    else:
        ticker_encoder = LabelEncoder()
        ticker_encoder.fit(prediction_df["StockSymbol"])
    
    if 'Market_ID' not in prediction_df.columns:
        us_stock = ['AAPL', 'NVDA', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'AVGO', 'TSM', 'AMD']
        thai_stock = ['ADVANC', 'TRUE', 'DITTO', 'DIF', 'INSET', 'JMART', 'INET', 'JAS', 'HUMAN']
        prediction_df['Market_ID'] = prediction_df['StockSymbol'].apply(
            lambda x: 0 if x in us_stock else 1 if x in thai_stock else 2
        )
        market_encoder = LabelEncoder()
        market_encoder.fit(['US', 'TH', 'OTHER'])
    else:
        market_encoder = LabelEncoder()
        market_encoder.fit(prediction_df['Market_ID'].astype(str).unique())
    
    # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏Ç‡∏≠‡∏á scalers
    valid_scalers, validation_results = validate_ticker_scalers(
        ticker_scalers, prediction_df, feature_columns
    )
    
    if len(valid_scalers) == 0:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ scaler ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ")
        sys.exit()
    
    print(f"‚úÖ Scalers ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: {len(valid_scalers)} tickers")

    # ======== PREDICTION WITH CONSISTENT SCALERS ========
    print(f"\nüîÆ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Consistent Scalers...")
    
    # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    future_predictions_df = predict_with_consistent_scalers(
        model_lstm=model_lstm,
        model_gru=model_gru,
        df=prediction_df,
        feature_columns=feature_columns,
        ticker_scalers=valid_scalers,  # ‚úÖ ‡πÉ‡∏ä‡πâ valid_scalers (dict)
        ticker_encoder=ticker_encoder,  # ‚úÖ ‡πÉ‡∏ä‡πâ ticker_encoder
        market_encoder=market_encoder,  # ‚úÖ ‡πÉ‡∏ä‡πâ market_encoder
        seq_length=10  # ‚úÖ ‡πÉ‡∏ä‡πâ seq_length
    )
    
    if not future_predictions_df.empty:
        print(f"\nüéØ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Consistent Scalers:")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        display_cols = ['StockSymbol', 'Date', 'Current_Price', 'Ensemble_Price', 
                       'Ensemble_Direction', 'Confidence', 'Direction_Agreement', 'Consistent_Scaling']
        
        available_display_cols = [col for col in display_cols if col in future_predictions_df.columns]
        
        if len(available_display_cols) > 0:
            print(future_predictions_df[available_display_cols])
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            print(f"\nüìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:")
            print(f"   üìà ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {len(future_predictions_df)}")
            
            if 'Ensemble_Direction' in future_predictions_df.columns:
                buy_signals = len(future_predictions_df[future_predictions_df['Ensemble_Direction'] == 1])
                sell_signals = len(future_predictions_df[future_predictions_df['Ensemble_Direction'] == 0])
                print(f"   üìà ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì BUY: {buy_signals}")
                print(f"   üìâ ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì SELL: {sell_signals}")
            
            if 'Confidence' in future_predictions_df.columns:
                avg_confidence = future_predictions_df['Confidence'].mean()
                print(f"   üéØ Average Confidence: {avg_confidence:.3f}")
                
                # ‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ confidence ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                high_confidence = future_predictions_df.nlargest(3, 'Confidence')
                print(f"\nüèÜ ‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ Confidence ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î:")
                for _, row in high_confidence.iterrows():
                    direction_text = "üìà BUY" if row.get('Ensemble_Direction', 0) == 1 else "üìâ SELL"
                    current_price = row.get('Current_Price', 0)
                    ensemble_price = row.get('Ensemble_Price', 0)
                    price_change_pct = ((ensemble_price - current_price) / current_price * 100) if current_price > 0 else 0
                    
                    print(f"   {row['StockSymbol']}: {direction_text} "
                          f"(Confidence: {row['Confidence']:.3f}, "
                          f"Expected: {price_change_pct:+.2f}%)")
            
            # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
            if 'Direction_Agreement' in future_predictions_df.columns:
                agreement_rate = future_predictions_df['Direction_Agreement'].mean()
                print(f"   ü§ù Model Agreement Rate: {agreement_rate:.1%}")
            
            if 'Consistent_Scaling' in future_predictions_df.columns:
                consistent_scaling_rate = future_predictions_df['Consistent_Scaling'].mean()
                print(f"   ‚úÖ Consistent Scaling Rate: {consistent_scaling_rate:.1%}")
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV
            csv_filename = f"consistent_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            future_predictions_df.to_csv(csv_filename, index=False)
            print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå: {csv_filename}")
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
            print(f"\nüíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö database schema
            db_predictions = future_predictions_df.copy()
            
            # Map columns ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö save_predictions_simple
            if 'Ensemble_Price' in db_predictions.columns:
                db_predictions['Predicted_Price'] = db_predictions['Ensemble_Price']
            if 'Ensemble_Direction' in db_predictions.columns:
                db_predictions['Predicted_Direction'] = db_predictions['Ensemble_Direction']
            if 'LSTM_Price' in db_predictions.columns:
                db_predictions['LSTM_Prediction'] = db_predictions['LSTM_Price']
            if 'LSTM_Direction' in db_predictions.columns:
                db_predictions['LSTM_Direction'] = db_predictions['LSTM_Direction']
            if 'GRU_Price' in db_predictions.columns:
                db_predictions['GRU_Prediction'] = db_predictions['GRU_Price']
            if 'GRU_Direction' in db_predictions.columns:
                db_predictions['GRU_Direction'] = db_predictions['GRU_Direction']
            
            try:
                db_save_success = save_predictions_simple(db_predictions)
                
                if db_save_success:
                    print("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                    print("üîÑ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß")
                    print("üì± ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß")
                else:
                    print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
                print("üíæ ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏°‡∏µ‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
            
        else:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î columns")
            print(f"Available columns: {list(future_predictions_df.columns)}")
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
            csv_filename = f"predictions_all_columns_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            future_predictions_df.to_csv(csv_filename, index=False)
            print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå: {csv_filename}")
    
    else:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á")
        print("üîç ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:")
        print("   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ticker_scalers ‡∏°‡∏µ‡∏Ñ‡∏£‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
        print("   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sequence length ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
        print("   - ‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM ‡πÅ‡∏•‡∏∞ GRU ‡πÇ‡∏´‡∏•‡∏î‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
    print(f"\nüéâ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ Consistent Scalers ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    print(f"üìã ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:")
    print(f"   üîß Scalers ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {'Training scalers' if scalers_loaded else 'New scalers'}")
    print(f"   üîÆ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {'‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à' if not future_predictions_df.empty else '‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à'}")
    print(f"   üíæ ‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {'‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß' if not future_predictions_df.empty else '‡πÑ‡∏°‡πà‡∏°‡∏µ'}")
    print(f"   ‚úÖ LSTM ‡πÅ‡∏•‡∏∞ GRU ‡πÉ‡∏ä‡πâ scaler ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô: {'‡πÉ‡∏ä‡πà' if not future_predictions_df.empty else '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'}")
    
    print("\nüîö ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏´‡∏∏‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Consistent Scalers")
    print("‚ú® ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ scalers ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î")