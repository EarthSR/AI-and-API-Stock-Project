import sys
import numpy as np
import pandas as pd
import sqlalchemy
import os
import tensorflow as tf
import ta
from tensorflow.keras.models import load_model
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, RobustScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score
import joblib
import warnings
from datetime import datetime, timedelta
import mysql.connector
from dotenv import load_dotenv
from tensorflow.keras.optimizers import Adam
from sqlalchemy import text
from sklearn.utils.class_weight import compute_class_weight
# XGBoost imports
import xgboost as xgb
from sklearn.impute import SimpleImputer
from ta.momentum import RSIIndicator
from ta.trend import SMAIndicator, MACD
from ta.volatility import BollingerBands, AverageTrueRange
import pickle
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

with open('../LSTM_model/class_weights.pkl', 'rb') as f:
    class_weights_dict = pickle.load(f)

@tf.keras.utils.register_keras_serializable()
def quantile_loss(y_true, y_pred, quantile=0.5):
    error = y_true - y_pred
    return tf.keras.backend.mean(tf.keras.backend.maximum(quantile * error, (quantile - 1) * error))

def focal_weighted_binary_crossentropy(class_weights, gamma=2.0, alpha_pos=0.7):
    def loss(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        epsilon = 1e-7
        y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)
        
        weights = tf.where(y_true == 1, class_weights[1], class_weights[0])
        alpha = tf.where(y_true == 1, alpha_pos, 1 - alpha_pos)
        pt = tf.where(y_true == 1, y_pred, 1 - y_pred)
        focal_factor = tf.pow(1 - pt, gamma)
        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
        weighted_bce = bce * weights * alpha * focal_factor
        return tf.reduce_mean(weighted_bce)
    return loss

# ======================== RETRAIN TRACKING SYSTEM ========================
def save_retrain_log(model_name, chunk_idx, retrain_count, performance_metrics=None):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ retrain (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)"""
    log_file = f"retrain_log_{model_name}.csv"
    
    log_data = {
        'Timestamp': datetime.now(),
        'Model': model_name,
        'Chunk_Index': chunk_idx,
        'Retrain_Count': retrain_count,
        'Date': datetime.now().strftime('%Y-%m-%d'),
        'Time': datetime.now().strftime('%H:%M:%S')
    }
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° performance metrics ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if performance_metrics:
        log_data.update(performance_metrics)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
    try:
        if os.path.exists(log_file):
            existing_df = pd.read_csv(log_file)
            new_df = pd.concat([existing_df, pd.DataFrame([log_data])], ignore_index=True)
        else:
            new_df = pd.DataFrame([log_data])
        
        new_df.to_csv(log_file, index=False)
        print(f"            üìù Retrain log saved to {log_file}")
        
    except Exception as e:
        print(f"            ‚ö†Ô∏è Error saving retrain log: {e}")

def should_retrain_model(model_name, retrain_frequency_days=5):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£ retrain ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤"""
    last_trained_file = f"last_retrain_{model_name}.txt"
    
    if not os.path.exists(last_trained_file):
        return True, "No previous retrain record"
    
    try:
        with open(last_trained_file, "r") as f:
            last_trained_str = f.read().strip()
        
        last_trained_date = datetime.strptime(last_trained_str, "%Y-%m-%d")
        days_since_last_retrain = (datetime.now() - last_trained_date).days
        
        if days_since_last_retrain >= retrain_frequency_days:
            return True, f"Last retrain: {days_since_last_retrain} days ago"
        else:
            return False, f"Last retrain: {days_since_last_retrain} days ago (too recent)"
            
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading retrain date: {e}")
        return True, "Error reading retrain record"

def update_retrain_date(model_name):
    """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà retrain ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
    last_trained_file = f"last_retrain_{model_name}.txt"
    
    try:
        with open(last_trained_file, "w") as f:
            f.write(datetime.now().strftime("%Y-%m-%d"))
        print(f"            üìÖ Updated retrain date for {model_name}")
        
    except Exception as e:
        print(f"            ‚ö†Ô∏è Error updating retrain date: {e}")

def get_retrain_stats(model_name):
    """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ retrain (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)"""
    log_file = f"retrain_log_{model_name}.csv"
    
    if not os.path.exists(log_file):
        return "No retrain history found"
    
    try:
        df = pd.read_csv(log_file)
        
        stats = {
            'Total_Retrains': len(df),
            'First_Retrain': df['Date'].min(),
            'Last_Retrain': df['Date'].max(),
            'Total_Chunks_Processed': df['Chunk_Index'].nunique() if 'Chunk_Index' in df.columns else 0,
            'Avg_Retrains_Per_Day': len(df) / max(1, (pd.to_datetime(df['Date'].max()) - pd.to_datetime(df['Date'].min())).days + 1)
        }
        
        return stats
        
    except Exception as e:
        return f"Error reading retrain stats: {e}"
# ======================== WALK-FORWARD VALIDATION FUNCTION ========================

def walk_forward_validation_multi_task_batch(
    model,
    df,
    feature_columns,
    ticker_scalers,   # Dict ‡∏Ç‡∏≠‡∏á Scaler per Ticker
    ticker_encoder,
    market_encoder,
    seq_length=10,
    retrain_frequency=5,
    chunk_size = 200
):
    """
    ‡∏ó‡∏≥ Walk-Forward Validation ‡πÅ‡∏ö‡∏ö Multi-Task (Price + Direction)
    ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô chunks ‡∏•‡∏∞ chunk_size ‡∏ß‡∏±‡∏ô ‡∏û‡∏£‡πâ‡∏≠‡∏° Online Learning
    
    - Mini-retrain: ‡∏ó‡∏∏‡∏Å retrain_frequency ‡∏ß‡∏±‡∏ô (Online Learning ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)
    - Chunk-based: ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """

    all_predictions = []
    chunk_metrics = []
    tickers = df['StockSymbol'].unique()

    for ticker in tickers:
        print(f"\nProcessing Ticker: {ticker}")
        df_ticker = df[df['StockSymbol'] == ticker].sort_values('Date').reset_index(drop=True)
        
        total_days = len(df_ticker)
        print(f"   üìä Total data available: {total_days} days")
        
        if total_days < chunk_size + seq_length:
            print(f"   ‚ö†Ô∏è Not enough data (need at least {chunk_size + seq_length} days), skipping...")
            continue
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ
        num_chunks = total_days // chunk_size
        remaining_days = total_days % chunk_size
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° partial chunk ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
        if remaining_days > seq_length:
            num_chunks += 1
            
        print(f"   üì¶ Number of chunks: {num_chunks} (chunk_size={chunk_size})")
        
        ticker_predictions = []
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * chunk_size
            end_idx = min(start_idx + chunk_size, total_days)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î chunk
            if (end_idx - start_idx) < seq_length + 1:
                print(f"      ‚ö†Ô∏è Chunk {chunk_idx + 1} too small ({end_idx - start_idx} days), skipping...")
                continue
                
            current_chunk = df_ticker.iloc[start_idx:end_idx].reset_index(drop=True)
            
            print(f"\n      üì¶ Processing Chunk {chunk_idx + 1}/{num_chunks}")
            print(f"         üìÖ Date range: {current_chunk['Date'].min()} to {current_chunk['Date'].max()}")
            print(f"         üìà Days: {len(current_chunk)} ({start_idx}-{end_idx})")
            
            # === Walk-Forward Validation ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Chunk ===
            chunk_predictions = []
            batch_features = []
            batch_tickers = []
            batch_market = []
            batch_price = []
            batch_dir = []
            predictions_count = 0

            for i in range(len(current_chunk) - seq_length):
                historical_data = current_chunk.iloc[i : i + seq_length]
                target_data = current_chunk.iloc[i + seq_length]

                t_id = historical_data['Ticker_ID'].iloc[-1]
                if t_id not in ticker_scalers:
                    print(f"         ‚ö†Ô∏è Ticker_ID {t_id} not found in scalers, skipping...")
                    continue

                scaler_f = ticker_scalers[t_id]['feature_scaler']
                scaler_p = ticker_scalers[t_id]['price_scaler']

                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° input features
                features = historical_data[feature_columns].values
                ticker_ids = historical_data['Ticker_ID'].values
                market_ids = historical_data['Market_ID'].values

                try:
                    features_scaled = scaler_f.transform(features)
                except Exception as e:
                    print(f"         ‚ö†Ô∏è Feature scaling error: {e}")
                    continue

                X_features = features_scaled.reshape(1, seq_length, len(feature_columns))
                X_ticker = ticker_ids.reshape(1, seq_length)
                X_market = market_ids.reshape(1, seq_length)

                # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
                try:
                    pred_output = model.predict([X_features, X_ticker, X_market], verbose=0)
                    pred_price_scaled = pred_output[0]
                    pred_dir_prob = pred_output[1]

                    predicted_price = scaler_p.inverse_transform(pred_price_scaled)[0][0]
                    predicted_dir = 1 if pred_dir_prob[0][0] >= 0.5 else 0

                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                    actual_price = target_data['Close']
                    future_date = target_data['Date']
                    last_close = historical_data.iloc[-1]['Close']
                    actual_dir = 1 if (target_data['Close'] > last_close) else 0

                    chunk_predictions.append({
                        'Ticker': ticker,
                        'Date': future_date,
                        'Chunk_Index': chunk_idx + 1,
                        'Position_in_Chunk': i + 1,
                        'Predicted_Price': predicted_price,
                        'Actual_Price': actual_price,
                        'Predicted_Dir': predicted_dir,
                        'Actual_Dir': actual_dir,
                        'Last_Close': last_close,
                        'Price_Change_Actual': actual_price - last_close,
                        'Price_Change_Predicted': predicted_price - last_close
                    })

                    predictions_count += 1

                    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mini-retrain
                    batch_features.append(X_features)
                    batch_tickers.append(X_ticker)
                    batch_market.append(X_market)

                    y_price_true_scaled = scaler_p.transform(np.array([[actual_price]], dtype=float))
                    batch_price.append(y_price_true_scaled)

                    y_dir_true = np.array([actual_dir], dtype=float)
                    batch_dir.append(y_dir_true)

                    # üîÑ Mini-retrain (Online Learning ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô chunk)
                    if (i+1) % retrain_frequency == 0 or (i == (len(current_chunk) - seq_length - 1)):
                        if len(batch_features) > 0:
                            try:
                                bf = np.concatenate(batch_features, axis=0)
                                bt = np.concatenate(batch_tickers, axis=0)
                                bm = np.concatenate(batch_market, axis=0)
                                bp = np.concatenate(batch_price, axis=0)
                                bd = np.concatenate(batch_dir, axis=0)

                                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡πà‡∏≠‡∏ô retrain
                                pre_retrain_loss = model.evaluate(
                                    [bf, bt, bm],
                                    {
                                        'price_output': bp,
                                        'direction_output': bd
                                    },
                                    verbose=0
                                )

                                # ‡∏ó‡∏≥ mini-retrain
                                history = model.fit(
                                    [bf, bt, bm],
                                    {
                                        'price_output': bp,
                                        'direction_output': bd
                                    },
                                    epochs=1,
                                    batch_size=len(bf),
                                    verbose=0,
                                    shuffle=False
                                )
                                
                                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏á retrain
                                post_retrain_loss = model.evaluate(
                                    [bf, bt, bm],
                                    {
                                        'price_output': bp,
                                        'direction_output': bd
                                    },
                                    verbose=0
                                )
                                
                                # ‡∏™‡∏£‡πâ‡∏≤‡∏á performance metrics
                                performance_metrics = {
                                    'Pre_Retrain_Loss': pre_retrain_loss[0] if isinstance(pre_retrain_loss, list) else pre_retrain_loss,
                                    'Post_Retrain_Loss': post_retrain_loss[0] if isinstance(post_retrain_loss, list) else post_retrain_loss,
                                    'Loss_Improvement': (pre_retrain_loss[0] - post_retrain_loss[0]) if isinstance(pre_retrain_loss, list) else (pre_retrain_loss - post_retrain_loss),
                                    'Batch_Size': len(bf),
                                    'Position_in_Chunk': i+1
                                }
                                
                                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å retrain log
                                model_name = model.name if hasattr(model, 'name') else 'Unknown_Model'
                                save_retrain_log(model_name, chunk_idx + 1, (i+1)//retrain_frequency + 1, performance_metrics)
                                
                                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà retrain
                                update_retrain_date(model_name)
                                
                                print(f"            üîÑ Mini-retrain at position {i+1} (batch size: {len(bf)})")
                                print(f"            üìä Loss: {pre_retrain_loss[0]:.4f} ‚Üí {post_retrain_loss[0]:.4f}" if isinstance(pre_retrain_loss, list) else f"            üìä Loss: {pre_retrain_loss:.4f} ‚Üí {post_retrain_loss:.4f}")
                                
                            except Exception as e:
                                print(f"            ‚ö†Ô∏è Mini-retrain error: {e}")

                            # ‡∏•‡πâ‡∏≤‡∏á batch
                            batch_features = []
                            batch_tickers = []
                            batch_market = []
                            batch_price = []
                            batch_dir = []
                            
                except Exception as e:
                    print(f"         ‚ö†Ô∏è Prediction error at position {i}: {e}")
                    continue

            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö chunk ‡∏ô‡∏µ‡πâ
            if chunk_predictions:
                chunk_df = pd.DataFrame(chunk_predictions)
                
                actual_prices = chunk_df['Actual_Price'].values
                pred_prices = chunk_df['Predicted_Price'].values
                actual_dirs = chunk_df['Actual_Dir'].values
                pred_dirs = chunk_df['Predicted_Dir'].values
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
                mae_val = mean_absolute_error(actual_prices, pred_prices)
                mse_val = mean_squared_error(actual_prices, pred_prices)
                rmse_val = np.sqrt(mse_val)
                r2_val = r2_score(actual_prices, pred_prices)
                dir_acc = accuracy_score(actual_dirs, pred_dirs)
                dir_f1 = f1_score(actual_dirs, pred_dirs, zero_division=0)
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì MAPE ‡πÅ‡∏•‡∏∞ SMAPE (safe calculation)
                try:
                    mape_val = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100
                except:
                    mape_val = 0
                    
                try:
                    smape_val = 100/len(actual_prices) * np.sum(2 * np.abs(pred_prices - actual_prices) / (np.abs(actual_prices) + np.abs(pred_prices)))
                except:
                    smape_val = 0

                chunk_metric = {
                    'Ticker': ticker,
                    'Chunk_Index': chunk_idx + 1,
                    'Chunk_Start_Date': current_chunk['Date'].min(),
                    'Chunk_End_Date': current_chunk['Date'].max(),
                    'Chunk_Days': len(current_chunk),
                    'Predictions_Count': predictions_count,
                    'MAE': mae_val,
                    'MSE': mse_val,
                    'RMSE': rmse_val,
                    'MAPE': mape_val,
                    'SMAPE': smape_val,
                    'R2_Score': r2_val,
                    'Direction_Accuracy': dir_acc,
                    'Direction_F1': dir_f1
                }
                
                chunk_metrics.append(chunk_metric)
                ticker_predictions.extend(chunk_predictions)
                
                print(f"         üìä Chunk results: {predictions_count} predictions")
                print(f"         üìà Direction accuracy: {dir_acc:.3f}")
                print(f"         üìà Price MAE: {mae_val:.3f}")
            
            # ‚úÖ ‡πÅ‡∏Ñ‡πà Mini-retrain (Online Learning) ‡∏Å‡πá‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡πÅ‡∏•‡πâ‡∏ß
            print(f"         ‚úÖ Chunk {chunk_idx + 1} completed with continuous online learning")
        
        all_predictions.extend(ticker_predictions)
        print(f"   ‚úÖ Completed {ticker}: {len(ticker_predictions)} total predictions from {num_chunks} chunks")

    # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    print(f"\nüìä Processing complete!")
    print(f"   Total predictions: {len(all_predictions)}")
    print(f"   Total chunks processed: {len(chunk_metrics)}")
    
    if len(all_predictions) == 0:
        print("‚ùå No predictions generated!")
        return pd.DataFrame(), {}

    predictions_df = pd.DataFrame(all_predictions)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å predictions
    predictions_df.to_csv('predictions_chunk_walkforward.csv', index=False)
    print("üíæ Saved predictions to 'predictions_chunk_walkforward.csv'")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å chunk metrics
    if chunk_metrics:
        chunk_metrics_df = pd.DataFrame(chunk_metrics)
        chunk_metrics_df.to_csv('chunk_metrics.csv', index=False)
        print("üíæ Saved chunk metrics to 'chunk_metrics.csv'")

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Overall Metrics ‡∏ï‡πà‡∏≠ Ticker
    print("\nüìä Calculating overall metrics...")
    overall_metrics = {}
    
    for ticker, group in predictions_df.groupby('Ticker'):
        actual_prices = group['Actual_Price'].values
        pred_prices = group['Predicted_Price'].values
        actual_dirs = group['Actual_Dir'].values
        pred_dirs = group['Predicted_Dir'].values

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
        mae_val = mean_absolute_error(actual_prices, pred_prices)
        mse_val = mean_squared_error(actual_prices, pred_prices)
        rmse_val = np.sqrt(mse_val)
        r2_val = r2_score(actual_prices, pred_prices)

        dir_acc = accuracy_score(actual_dirs, pred_dirs)
        dir_f1 = f1_score(actual_dirs, pred_dirs, zero_division=0)
        dir_precision = precision_score(actual_dirs, pred_dirs, zero_division=0)
        dir_recall = recall_score(actual_dirs, pred_dirs, zero_division=0)

        # Safe MAPE ‡πÅ‡∏•‡∏∞ SMAPE calculation
        try:
            mape_val = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100
        except:
            mape_val = 0
            
        try:
            smape_val = 100/len(actual_prices) * np.sum(2 * np.abs(pred_prices - actual_prices) / (np.abs(actual_prices) + np.abs(pred_prices)))
        except:
            smape_val = 0

        overall_metrics[ticker] = {
            'Total_Predictions': len(group),
            'Number_of_Chunks': len(group['Chunk_Index'].unique()),
            'MAE': mae_val,
            'MSE': mse_val,
            'RMSE': rmse_val,
            'MAPE': mape_val,
            'SMAPE': smape_val,
            'R2_Score': r2_val,
            'Direction_Accuracy': dir_acc,
            'Direction_F1_Score': dir_f1,
            'Direction_Precision': dir_precision,
            'Direction_Recall': dir_recall
        }

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å overall metrics
    overall_metrics_df = pd.DataFrame.from_dict(overall_metrics, orient='index')
    overall_metrics_df.to_csv('overall_metrics_per_ticker.csv')
    print("üíæ Saved overall metrics to 'overall_metrics_per_ticker.csv'")

    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    print(f"\nüéØ Summary:")
    print(f"   üìà Tickers processed: {len(predictions_df['Ticker'].unique())}")
    print(f"   üìà Average predictions per ticker: {len(predictions_df)/len(predictions_df['Ticker'].unique()):.1f}")
    print(f"   üìà Average chunks per ticker: {len(chunk_metrics)/len(predictions_df['Ticker'].unique()):.1f}")
    
    if chunk_metrics:
        avg_chunk_acc = np.mean([c['Direction_Accuracy'] for c in chunk_metrics])
        avg_chunk_mae = np.mean([c['MAE'] for c in chunk_metrics])
        print(f"   üìà Average chunk direction accuracy: {avg_chunk_acc:.3f}")
        print(f"   üìà Average chunk MAE: {avg_chunk_mae:.3f}")

    print(f"\nüìÅ Files generated:")
    print(f"   üìÑ predictions_chunk_walkforward.csv - All predictions with chunk info")
    print(f"   üìÑ chunk_metrics.csv - Performance metrics per chunk")  
    print(f"   üìÑ overall_metrics_per_ticker.csv - Overall performance per ticker")

    return predictions_df, overall_metrics

def create_unified_ticker_scalers(df, feature_columns, scaler_file_path="../LSTM_model/ticker_scalers.pkl"):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á ticker scalers ‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô + ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    """
    print("üîß Creating unified per-ticker scalers...")
    
    # ======== STEP 1: Data Cleaning (‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô) ========
    df_clean = clean_data_for_unified_scaling(df, feature_columns)
    
    # ======== STEP 2: Create Per-Ticker Scalers (‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô) ========
    ticker_scalers = {}
    unique_tickers = df_clean['StockSymbol'].unique()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Ticker_ID ‡∏Å‡∏±‡∏ö StockSymbol
    ticker_id_to_name = {}
    name_to_ticker_id = {}
    
    print("üìã Creating ticker mappings...")
    for ticker_name in unique_tickers:
        ticker_rows = df_clean[df_clean['StockSymbol'] == ticker_name]
        if len(ticker_rows) > 0:
            ticker_id = ticker_rows['Ticker_ID'].iloc[0]
            ticker_id_to_name[ticker_id] = ticker_name
            name_to_ticker_id[ticker_name] = ticker_id
            print(f"   Mapping: Ticker_ID {ticker_id} = {ticker_name}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î pre-trained scalers
    pre_trained_scalers = {}
    try:
        if os.path.exists(scaler_file_path):
            pre_trained_scalers = joblib.load(scaler_file_path)
            print(f"‚úÖ Loaded pre-trained scalers for {len(pre_trained_scalers)} tickers")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not load pre-trained scalers: {e}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á scalers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ ticker
    for ticker_name in unique_tickers:
        ticker_data = df_clean[df_clean['StockSymbol'] == ticker_name].copy()
        
        if len(ticker_data) < 30:
            print(f"   ‚ö†Ô∏è {ticker_name}: Not enough data ({len(ticker_data)} days), skipping...")
            continue
        
        ticker_id = name_to_ticker_id[ticker_name]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö pre-trained scaler
        if ticker_id in pre_trained_scalers:
            scaler_info = pre_trained_scalers[ticker_id]
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö structure ‡∏Ç‡∏≠‡∏á scaler
            required_keys = ['feature_scaler', 'price_scaler']
            if all(key in scaler_info for key in required_keys):
                try:
                    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö scaler
                    test_features = ticker_data[feature_columns].iloc[:5]
                    test_price = ticker_data[['Close']].iloc[:5]
                    
                    _ = scaler_info['feature_scaler'].transform(test_features.fillna(0))
                    _ = scaler_info['price_scaler'].transform(test_price)
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                    scaler_info.update({
                        'ticker_symbol': ticker_name,
                        'ticker': ticker_name,  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö compatibility
                        'data_points': len(ticker_data)
                    })
                    
                    ticker_scalers[ticker_id] = scaler_info
                    print(f"   ‚úÖ {ticker_name} (ID: {ticker_id}): Using pre-trained scaler")
                    continue
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è {ticker_name}: Pre-trained scaler failed ({e}), creating new one")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler ‡πÉ‡∏´‡∏°‡πà
        try:
            print(f"   üîß {ticker_name}: Creating new scaler...")
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° feature data
            features = ticker_data[feature_columns].copy()
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ inf ‡πÅ‡∏•‡∏∞ NaN ‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô
            features = handle_infinite_values(features)
            features = features.fillna(features.mean()).fillna(0)
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° price data
            price_data = ticker_data[['Close']].copy()
            price_data = handle_infinite_values(price_data)
            price_data = price_data.fillna(price_data.mean())
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á scalers
            feature_scaler = RobustScaler()
            price_scaler = RobustScaler()
            
            feature_scaler.fit(features)
            price_scaler.fit(price_data)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scaler ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata (‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô)
            ticker_scalers[ticker_id] = {
                'feature_scaler': feature_scaler,
                'price_scaler': price_scaler,
                'ticker': ticker_name,  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö compatibility ‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô
                'ticker_symbol': ticker_name,  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                'data_points': len(ticker_data)
            }
            
            print(f"   ‚úÖ {ticker_name} (ID: {ticker_id}): Created new scaler with {len(ticker_data)} data points")
            
        except Exception as e:
            print(f"   ‚ùå {ticker_name}: Error creating scaler - {e}")
            continue
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scalers
    try:
        os.makedirs(os.path.dirname(scaler_file_path), exist_ok=True)
        joblib.dump(ticker_scalers, scaler_file_path)
        print(f"üíæ Saved unified scalers to {scaler_file_path}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ
        print(f"\nüìä Unified Ticker Scalers Summary:")
        for t_id, scaler_info in ticker_scalers.items():
            ticker_name = scaler_info.get('ticker', 'Unknown')
            data_points = scaler_info.get('data_points', 'Unknown')
            print(f"   Ticker_ID {t_id}: {ticker_name} ({data_points} data points)")
            
    except Exception as e:
        print(f"‚ùå Error saving scalers: {e}")
    
    print(f"‚úÖ Created unified scalers for {len(ticker_scalers)} tickers")
    return ticker_scalers

def clean_data_for_unified_scaling(df, feature_columns):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á unified scalers"""
    print("üßπ Cleaning data for unified scaling...")
    
    df_clean = df.copy()
    
    # Map column names ‡∏à‡∏≤‡∏Å database format ‡πÄ‡∏õ‡πá‡∏ô training format
    column_mapping = {
        'Change_Percent': 'Change (%)',
        'P_BV_Ratio': 'P/BV Ratio',
        'TotalRevenue': 'Total Revenue',
        'QoQGrowth': 'QoQ Growth (%)',
        'EPS': 'Earnings Per Share (EPS)',
        'ROE': 'ROE (%)',
        'NetProfitMargin': 'Net Profit Margin (%)',
        'DebtToEquity': 'Debt to Equity',
        'PERatio': 'P/E Ratio',
        'Dividend_Yield': 'Dividend Yield (%)',
    }
    
    # Rename columns ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    for old_name, new_name in column_mapping.items():
        if old_name in df_clean.columns and new_name not in df_clean.columns:
            df_clean[new_name] = df_clean[old_name]
            print(f"   üîÑ Mapped {old_name} ‚Üí {new_name}")
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° feature columns
    for col in feature_columns:
        if col in df_clean.columns:
            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numeric
                if not pd.api.types.is_numeric_dtype(df_clean[col]):
                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
                
                # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ infinite values (‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô)
                df_clean[col] = handle_infinite_values_column(df_clean[col])
                
                print(f"   ‚úÖ Cleaned {col}: range {df_clean[col].min():.3f} - {df_clean[col].max():.3f}")
                
            except Exception as e:
                print(f"   ‚ùå Error cleaning {col}: {e}")
                df_clean[col] = 0.0
    
    return df_clean

def handle_infinite_values(data):
    """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ infinite values ‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô"""
    data_clean = data.copy()
    
    for col in data_clean.columns:
        col_data = data_clean[col]
        
        # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà +inf ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà inf
        pos_inf_mask = col_data == np.inf
        if pos_inf_mask.any():
            max_val = col_data[col_data != np.inf].max()
            if pd.notna(max_val):
                data_clean.loc[pos_inf_mask, col] = max_val
            else:
                data_clean.loc[pos_inf_mask, col] = 0
        
        # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà -inf ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà -inf
        neg_inf_mask = col_data == -np.inf
        if neg_inf_mask.any():
            min_val = col_data[col_data != -np.inf].min()
            if pd.notna(min_val):
                data_clean.loc[neg_inf_mask, col] = min_val
            else:
                data_clean.loc[neg_inf_mask, col] = 0
    
    return data_clean

def handle_infinite_values_column(series):
    """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ infinite values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö column ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"""
    series_clean = series.copy()
    
    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà +inf
    pos_inf_mask = series_clean == np.inf
    if pos_inf_mask.any():
        max_val = series_clean[series_clean != np.inf].max()
        if pd.notna(max_val):
            series_clean[pos_inf_mask] = max_val
        else:
            series_clean[pos_inf_mask] = 0
    
    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà -inf
    neg_inf_mask = series_clean == -np.inf
    if neg_inf_mask.any():
        min_val = series_clean[series_clean != -np.inf].min()
        if pd.notna(min_val):
            series_clean[neg_inf_mask] = min_val
        else:
            series_clean[neg_inf_mask] = 0
    
    return series_clean

def prepare_data_for_walk_forward(df, feature_columns):
    """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Walk-Forward Validation ‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô"""
    print("üìä Preparing data for Walk-Forward Validation...")
    
    df_prepared = df.copy()
    
    # ======== STEP 1: Calculate Technical Indicators (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ) ========
    df_prepared = ensure_technical_indicators(df_prepared)
    
    # ======== STEP 2: Handle Sentiment Mapping ========
    if 'Sentiment' in df_prepared.columns:
        # ‡πÅ‡∏õ‡∏•‡∏á text sentiment ‡πÄ‡∏õ‡πá‡∏ô numeric (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
        if df_prepared['Sentiment'].dtype == 'object':
            df_prepared['Sentiment'] = df_prepared['Sentiment'].map({
                'Positive': 1, 'Negative': -1, 'Neutral': 0
            })
            print("   üîÑ Mapped sentiment values to numeric")
    
    # ======== STEP 3: Create Target Variables ========
    print("   üéØ Creating target variables...")
    df_prepared = df_prepared.sort_values(['StockSymbol', 'Date']).reset_index(drop=True)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Direction ‡πÅ‡∏•‡∏∞ TargetPrice per ticker
    df_prepared['Direction'] = 0
    df_prepared['TargetPrice'] = np.nan
    
    for ticker in df_prepared['StockSymbol'].unique():
        ticker_mask = df_prepared['StockSymbol'] == ticker
        ticker_data = df_prepared[ticker_mask].copy()
        
        if len(ticker_data) > 1:
            # Direction: 1 ‡∏ñ‡πâ‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ
            direction = (ticker_data['Close'].shift(-1) > ticker_data['Close']).astype(int)
            target_price = ticker_data['Close'].shift(-1)
            
            df_prepared.loc[ticker_mask, 'Direction'] = direction
            df_prepared.loc[ticker_mask, 'TargetPrice'] = target_price
    
    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ target
    df_prepared = df_prepared.dropna(subset=['Direction', 'TargetPrice'])
    
    # ======== STEP 4: Ensure Encoders ========
    if 'Ticker_ID' not in df_prepared.columns:
        ticker_encoder = LabelEncoder()
        df_prepared['Ticker_ID'] = ticker_encoder.fit_transform(df_prepared['StockSymbol'])
        print("   üîÑ Created Ticker_ID encoding")
    
    if 'Market_ID' not in df_prepared.columns or df_prepared['Market_ID'].dtype == 'object':
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Market_ID
        us_stock = ['AAPL', 'NVDA', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'AVGO', 'TSM', 'AMD']
        thai_stock = ['ADVANC', 'INTUCH', 'TRUE', 'DITTO', 'DIF', 
                     'INSET', 'JMART', 'INET', 'JAS', 'HUMAN']
        
        df_prepared['Market'] = df_prepared['StockSymbol'].apply(
            lambda x: "US" if x in us_stock else "TH" if x in thai_stock else "OTHER"
        )
        
        market_encoder = LabelEncoder()
        df_prepared['Market_ID'] = market_encoder.fit_transform(df_prepared['Market'])
        print("   üîÑ Created Market_ID encoding")
    
    # ======== STEP 5: Handle Missing Technical Indicators ========
    stock_columns = [
        'RSI', 'EMA_12', 'EMA_26', 'MACD', 'MACD_Signal', 'Bollinger_High',
        'Bollinger_Low', 'ATR', 'Keltner_High', 'Keltner_Low', 'Keltner_Middle',
        'Chaikin_Vol', 'Donchian_High', 'Donchian_Low', 'PSAR', 'SMA_50', 'SMA_200'
    ]
    
    available_stock_cols = [col for col in stock_columns if col in df_prepared.columns]
    if available_stock_cols:
        print(f"   üîß Forward filling {len(available_stock_cols)} technical indicators...")
        # Forward fill per ticker
        for ticker in df_prepared['StockSymbol'].unique():
            ticker_mask = df_prepared['StockSymbol'] == ticker
            df_prepared.loc[ticker_mask, available_stock_cols] = \
                df_prepared.loc[ticker_mask, available_stock_cols].fillna(method='ffill')
    
    # Fill remaining NaN with 0
    df_prepared = df_prepared.fillna(0)
    
    print(f"   ‚úÖ Prepared data: {len(df_prepared)} rows, {len(df_prepared['StockSymbol'].unique())} tickers")
    return df_prepared

def ensure_technical_indicators(df):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° technical indicators ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
    df_with_indicators = df.copy()
    
    required_indicators = ['RSI', 'MACD', 'MACD_Signal', 'ATR', 'Bollinger_High', 'Bollinger_Low']
    missing_indicators = [ind for ind in required_indicators if ind not in df_with_indicators.columns]
    
    if missing_indicators:
        print(f"   üîß Adding missing technical indicators: {missing_indicators}")
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indicators ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ per ticker
        for ticker in df_with_indicators['StockSymbol'].unique():
            ticker_mask = df_with_indicators['StockSymbol'] == ticker
            ticker_data = df_with_indicators[ticker_mask].copy()
            
            if len(ticker_data) < 20:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 20 ‡∏ß‡∏±‡∏ô
                continue
            
            try:
                if 'RSI' in missing_indicators:
                    rsi = ta.momentum.RSIIndicator(ticker_data['Close'], window=14).rsi()
                    df_with_indicators.loc[ticker_mask, 'RSI'] = rsi
                
                if any(ind in missing_indicators for ind in ['MACD', 'MACD_Signal']):
                    ema_12 = ticker_data['Close'].ewm(span=12).mean()
                    ema_26 = ticker_data['Close'].ewm(span=26).mean()
                    macd = ema_12 - ema_26
                    macd_signal = macd.rolling(window=9).mean()
                    
                    if 'MACD' in missing_indicators:
                        df_with_indicators.loc[ticker_mask, 'MACD'] = macd
                    if 'MACD_Signal' in missing_indicators:
                        df_with_indicators.loc[ticker_mask, 'MACD_Signal'] = macd_signal
                
                if 'ATR' in missing_indicators and all(col in ticker_data.columns for col in ['High', 'Low']):
                    atr = ta.volatility.AverageTrueRange(
                        high=ticker_data['High'], 
                        low=ticker_data['Low'], 
                        close=ticker_data['Close'], 
                        window=14
                    ).average_true_range()
                    df_with_indicators.loc[ticker_mask, 'ATR'] = atr
                
                if any(ind in missing_indicators for ind in ['Bollinger_High', 'Bollinger_Low']):
                    bollinger = ta.volatility.BollingerBands(ticker_data['Close'], window=20, window_dev=2)
                    if 'Bollinger_High' in missing_indicators:
                        df_with_indicators.loc[ticker_mask, 'Bollinger_High'] = bollinger.bollinger_hband()
                    if 'Bollinger_Low' in missing_indicators:
                        df_with_indicators.loc[ticker_mask, 'Bollinger_Low'] = bollinger.bollinger_lband()
                        
            except Exception as e:
                print(f"      ‚ö†Ô∏è Error calculating indicators for {ticker}: {e}")
                continue
    
    return df_with_indicators

# ======================== INTEGRATION WITH MAIN SYSTEM ========================

def create_walk_forward_compatible_scalers(df, feature_columns):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á scalers ‡∏ó‡∏µ‡πà compatible ‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Walk-Forward Validation ‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô"""
    
    print("üîÑ Creating Walk-Forward compatible scalers...")
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    df_prepared = prepare_data_for_walk_forward(df, feature_columns)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á unified scalers
    ticker_scalers = create_unified_ticker_scalers(df_prepared, feature_columns)
    
    return ticker_scalers, df_prepared
# ======================== WalkForwardMiniRetrainManager Class ========================
class WalkForwardMiniRetrainManager:
    """
    Walk-Forward Validation + Retrain System ‡∏û‡∏£‡πâ‡∏≠‡∏° XGBoost Ensemble
    - ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô chunks
    - retrain ‡∏ó‡∏∏‡∏Å N ‡∏ß‡∏±‡∏ô
    - Continuous learning ‡πÅ‡∏ö‡∏ö incremental
    - ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
    """
    def __init__(self, 
                 lstm_model_path="../LSTM_model/best_v6_plus_minimal_tuning_v2_final_model.keras",
                 gru_model_path="../GRU_Model/best_v6_plus_minimal_tuning_v2_final_model.keras",
                 retrain_frequency=5,  
                 chunk_size=200,       
                 seq_length=10):
        
        self.lstm_model_path = lstm_model_path
        self.gru_model_path = gru_model_path
        self.retrain_frequency = retrain_frequency
        self.chunk_size = chunk_size
        self.seq_length = seq_length
        
        # ‡πÇ‡∏°‡πÄ‡∏î‡∏•
        self.lstm_model = None
        self.gru_model = None
        
        # Performance tracking
        self.all_predictions = []
        self.chunk_metrics = []
        
    def load_models_for_prediction(self, model_path=None, compile_model=False):
        """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU"""
        custom_objects = {
            "quantile_loss": quantile_loss,
            "focal_weighted_binary_crossentropy": focal_weighted_binary_crossentropy
        }
        try:
            if model_path:  # ‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
                print(f"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å {model_path}...")
                model = tf.keras.models.load_model(
                    model_path,
                    custom_objects=custom_objects,
                    safe_mode=False,
                    compile=compile_model  # ‡πÉ‡∏ä‡πâ compile_model ‡πÅ‡∏ó‡∏ô compile
                )
                print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                return model
            else:  # ‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU
                print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢...")
                self.lstm_model = tf.keras.models.load_model(
                    self.lstm_model_path,
                    custom_objects=custom_objects,
                    safe_mode=False,
                    compile=compile_model
                )
                self.gru_model = tf.keras.models.load_model(
                    self.gru_model_path,
                    custom_objects=custom_objects,
                    safe_mode=False,
                    compile=compile_model
                )
                print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                return True
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
            return None if model_path else False

# Fix 2: Enhanced data cleaning function
def clean_data_for_scalers(df, feature_columns):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á scalers"""
    print("üßπ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scalers...")
    
    df_clean = df.copy()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    for col in feature_columns:
        if col in df_clean.columns:
            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
                col_data = df_clean[col].astype(str)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÄ‡∏ä‡πà‡∏ô 49.0349.03)
                problematic_mask = col_data.str.contains(r'\d+\.\d+\d+\.\d+', regex=True, na=False)
                
                if problematic_mask.any():
                    print(f"   ‚ö†Ô∏è ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå {col}: {problematic_mask.sum()} ‡πÅ‡∏ñ‡∏ß")
                    
                    # ‡πÅ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô (‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
                    def extract_first_number(x):
                        try:
                            if pd.isna(x):
                                return 0.0
                            x_str = str(x)
                            # ‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
                            import re
                            match = re.search(r'^\d+\.?\d*', x_str)
                            if match:
                                return float(match.group())
                            else:
                                return 0.0
                        except:
                            return 0.0
                    
                    df_clean[col] = col_data.apply(extract_first_number)
                else:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numeric ‡∏õ‡∏Å‡∏ï‡∏¥
                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
                
                # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡πÅ‡∏•‡∏∞ inf ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                col_mean = df_clean[col].replace([np.inf, -np.inf], np.nan).mean()
                if pd.isna(col_mean):
                    col_mean = 0.0
                
                df_clean[col] = df_clean[col].replace([np.inf, -np.inf, np.nan], col_mean)
                
                print(f"   ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î {col}: {df_clean[col].dtype}, range: {df_clean[col].min():.3f} - {df_clean[col].max():.3f}")
                
            except Exception as e:
                print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î {col}: {e}")
                # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                df_clean[col] = 0.0
    
    return df_clean

# Fix 3: Enhanced create_ticker_scalers function
def create_ticker_scalers_fixed(df, feature_columns, scaler_file_path="../LSTM_model/ticker_scalers.pkl"):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á ticker scalers ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô"""
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô
    df_clean = clean_data_for_scalers(df, feature_columns)
    
    ticker_scalers = {}
    tickers = df_clean['StockSymbol'].unique()
    
    print("üîß Creating/loading individual scalers for each ticker...")
    
    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î scalers ‡πÄ‡∏Å‡πà‡∏≤
    pre_trained_scalers = {}
    try:
        if os.path.exists(scaler_file_path):
            pre_trained_scalers = joblib.load(scaler_file_path)
            print(f"‚úÖ Loaded pre-trained scalers from {scaler_file_path} for {len(pre_trained_scalers)} tickers")
        else:
            print(f"‚ö†Ô∏è No pre-trained scalers found at {scaler_file_path}, creating new scalers")
    except Exception as e:
        print(f"‚ùå Error loading pre-trained scalers: {e}, creating new scalers")
    
    for ticker in tickers:
        df_ticker = df_clean[df_clean['StockSymbol'] == ticker].copy()
        
        if len(df_ticker) < 30:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 30 ‡∏ß‡∏±‡∏ô
            print(f"   ‚ö†Ô∏è {ticker}: Not enough data ({len(df_ticker)} days), skipping...")
            continue
        
        ticker_id = df_ticker['Ticker_ID'].iloc[0]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö pre-trained scaler
        if ticker_id in pre_trained_scalers:
            scaler_info = pre_trained_scalers[ticker_id]
            if all(key in scaler_info for key in ['ticker_symbol', 'feature_scaler', 'price_scaler']):
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ scaler ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                try:
                    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö transform ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                    test_data = df_ticker[feature_columns].iloc[:5].fillna(df_ticker[feature_columns].mean())
                    _ = scaler_info['feature_scaler'].transform(test_data)
                    _ = scaler_info['price_scaler'].transform(df_ticker[['Close']].iloc[:5])
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° data_points ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏≤‡∏î
                    if 'data_points' not in scaler_info:
                        scaler_info['data_points'] = len(df_ticker)
                    
                    ticker_scalers[ticker_id] = scaler_info
                    print(f"   ‚úÖ {ticker} (ID: {ticker_id}): Using pre-trained scaler with {scaler_info['data_points']} data points")
                    continue
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è {ticker} (ID: {ticker_id}): Pre-trained scaler failed test ({e}), creating new one")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler ‡πÉ‡∏´‡∏°‡πà
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á scaler
            feature_data = df_ticker[feature_columns].copy()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô numeric ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            non_numeric_cols = []
            for col in feature_columns:
                if col in feature_data.columns:
                    if not pd.api.types.is_numeric_dtype(feature_data[col]):
                        non_numeric_cols.append(col)
            
            if non_numeric_cols:
                print(f"   ‚ö†Ô∏è {ticker}: Non-numeric columns found: {non_numeric_cols}")
                for col in non_numeric_cols:
                    feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce')
            
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
            feature_data = feature_data.fillna(feature_data.mean()).fillna(0)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ infinite values ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            feature_data = feature_data.replace([np.inf, -np.inf], 0)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á feature scaler
            feature_scaler = RobustScaler()
            feature_scaler.fit(feature_data)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á price scaler
            price_scaler = RobustScaler()
            price_data = df_ticker[['Close']].copy()
            price_data = price_data.fillna(price_data.mean()).fillna(0)
            price_data = price_data.replace([np.inf, -np.inf], price_data.mean())
            price_scaler.fit(price_data)
            
            # ‡πÄ‡∏Å‡πá‡∏ö scaler ‡πÅ‡∏•‡∏∞ metadata
            ticker_scalers[ticker_id] = {
                'ticker_symbol': ticker,
                'feature_scaler': feature_scaler,
                'price_scaler': price_scaler,
                'data_points': len(df_ticker)
            }
            
            print(f"   ‚úÖ {ticker} (ID: {ticker_id}): Created new scaler with {len(df_ticker)} data points")
            
        except Exception as e:
            print(f"   ‚ùå {ticker}: Error creating scalers - {e}")
            # ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• debug
            print(f"      Debug - Feature data shape: {df_ticker[feature_columns].shape}")
            print(f"      Debug - Feature data dtypes: {df_ticker[feature_columns].dtypes.to_dict()}")
            print(f"      Debug - Sample values: {df_ticker[feature_columns].iloc[0].to_dict()}")
            continue
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scalers ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß
    try:
        os.makedirs(os.path.dirname(scaler_file_path), exist_ok=True)
        joblib.dump(ticker_scalers, scaler_file_path)
        print(f"üíæ Saved updated scalers to {scaler_file_path}")
    except Exception as e:
        print(f"‚ùå Error saving scalers: {e}")
    
    print(f"‚úÖ Created/loaded scalers for {len(ticker_scalers)} tickers")
    return ticker_scalers

# ======================== ENHANCED XGBoost Meta-Learner Class ========================
class XGBoostMetaLearner:
    """
    XGBoost Meta-Learner ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏ß‡∏° predictions ‡∏à‡∏≤‡∏Å LSTM ‡πÅ‡∏•‡∏∞ GRU
    ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ technical indicators ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
    """
    
    def __init__(self, 
                 clf_model_path='../Ensemble_Model/xgb_classifier_model.pkl', 
                 reg_model_path='../Ensemble_Model/xgb_regressor_model.pkl',
                 scaler_dir_path='../Ensemble_Model/scaler_dir.pkl', 
                 scaler_price_path='../Ensemble_Model/scaler_price.pkl',
                 retrain_frequency=5):
        
        self.clf_model_path = clf_model_path
        self.reg_model_path = reg_model_path
        self.scaler_dir_path = scaler_dir_path
        self.scaler_price_path = scaler_price_path
        self.retrain_frequency = retrain_frequency
        
        self.xgb_clf = None
        self.xgb_reg = None
        self.scaler_dir = None
        self.scaler_price = None
        
        # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
        self.load_models()
    
    def load_models(self):
        """‡πÇ‡∏´‡∏•‡∏î XGBoost models ‡πÅ‡∏•‡∏∞ scalers"""
        try:
            if os.path.exists(self.clf_model_path):
                self.xgb_clf = joblib.load(self.clf_model_path)
                print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î XGBoost Classifier ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            else:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå XGBoost Classifier")
            
            if os.path.exists(self.reg_model_path):
                self.xgb_reg = joblib.load(self.reg_model_path)
                print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î XGBoost Regressor ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            else:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå XGBoost Regressor")
            
            if os.path.exists(self.scaler_dir_path):
                self.scaler_dir = joblib.load(self.scaler_dir_path)
                print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î Direction Scaler ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            else:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Direction Scaler")
                
            if os.path.exists(self.scaler_price_path):
                self.scaler_price = joblib.load(self.scaler_price_path)
                print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î Price Scaler ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            else:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Price Scaler")
                
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
    
    def calculate_technical_indicators(self, df):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì technical indicators ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XGBoost"""
        
        def calculate_for_ticker(group):
            if len(group) < 26:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 26 ‡∏ß‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MACD
                return group
            
            try:
                # RSI
                group['RSI'] = RSIIndicator(close=group['Close'], window=14).rsi()
                
                # SMA
                group['SMA_20'] = SMAIndicator(close=group['Close'], window=20).sma_indicator()
                
                # MACD
                macd = MACD(close=group['Close'])
                group['MACD'] = macd.macd()
                
                # Bollinger Bands
                bb = BollingerBands(close=group['Close'], window=20)
                group['BB_High'] = bb.bollinger_hband()
                group['BB_Low'] = bb.bollinger_lband()
                
                # ATR (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ High, Low columns)
                if 'High' in group.columns and 'Low' in group.columns:
                    atr = AverageTrueRange(high=group['High'], low=group['Low'], 
                                         close=group['Close'], window=14)
                    group['ATR'] = atr.average_true_range()
                else:
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á High, Low ‡∏à‡∏≤‡∏Å Close ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
                    group['High'] = group['Close'] * 1.01
                    group['Low'] = group['Close'] * 0.99
                    atr = AverageTrueRange(high=group['High'], low=group['Low'], 
                                         close=group['Close'], window=14)
                    group['ATR'] = atr.average_true_range()
            
            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì technical indicators: {e}")
            
            return group
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indicators ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° ticker - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç DeprecationWarning
        df_with_indicators = df.groupby('StockSymbol', group_keys=False).apply(calculate_for_ticker)
        df_with_indicators = df_with_indicators.reset_index(drop=True)
        
        # Fill NaN values
        indicator_cols = ['RSI', 'SMA_20', 'MACD', 'BB_High', 'BB_Low', 'ATR']
        for col in indicator_cols:
            if col in df_with_indicators.columns:
                df_with_indicators[col] = df_with_indicators.groupby('StockSymbol')[col].ffill().bfill()
                df_with_indicators[col] = df_with_indicators[col].fillna(df_with_indicators[col].mean())
        
        return df_with_indicators
    
    def prepare_features(self, df):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XGBoost"""
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì technical indicators
        df = self.calculate_technical_indicators(df)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á meta features
        df['Price_Diff'] = df['PredictionClose_LSTM'] - df['PredictionClose_GRU']
        df['Dir_Agreement'] = (df['PredictionTrend_LSTM'] == df['PredictionTrend_GRU']).astype(int)
        
        # Normalize actual price ‡∏ï‡∏≤‡∏° ticker
        df['Actual_Price_Normalized'] = df.groupby('StockSymbol')['Close'].transform(
            lambda x: (x - x.mean()) / x.std() if x.std() != 0 else 0
        )
        
        # Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö direction prediction
        direction_features = [
            'PredictionTrend_LSTM', 'PredictionTrend_GRU', 'Dir_Agreement', 
            'RSI', 'SMA_20', 'MACD', 'BB_High', 'BB_Low', 'ATR'
        ]
        
        # Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö price prediction
        price_features = [
            'PredictionClose_LSTM', 'PredictionClose_GRU', 'Price_Diff',
            'RSI', 'SMA_20', 'MACD', 'BB_High', 'BB_Low', 'ATR',
            'Actual_Price_Normalized'
        ]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ features ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
        available_dir_features = [f for f in direction_features if f in df.columns]
        available_price_features = [f for f in price_features if f in df.columns]
        
        return df, available_dir_features, available_price_features
    
    def predict_meta(self, df):
        """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ XGBoost Meta-Learner"""
        
        if self.xgb_clf is None or self.xgb_reg is None:
            print("‚ùå XGBoost models ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÇ‡∏´‡∏•‡∏î ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ")
            return df
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° features
        df_prepared, dir_features, price_features = self.prepare_features(df)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ predictions ‡∏à‡∏≤‡∏Å LSTM ‡πÅ‡∏•‡∏∞ GRU
        prediction_mask = (
            df_prepared['PredictionClose_LSTM'].notna() & 
            df_prepared['PredictionClose_GRU'].notna() &
            df_prepared['PredictionTrend_LSTM'].notna() & 
            df_prepared['PredictionTrend_GRU'].notna()
        )
        
        if not prediction_mask.any():
            print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• predictions ‡∏à‡∏≤‡∏Å LSTM/GRU")
            return df
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ predictions
        df_to_predict = df_prepared[prediction_mask].copy()
        
        if len(df_to_predict) == 0:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
            return df
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ missing values
        imputer = SimpleImputer(strategy='mean')
        
        try:
            # Direction prediction
            X_dir = df_to_predict[dir_features]
            X_dir_filled = imputer.fit_transform(X_dir)
            
            if self.scaler_dir is not None:
                X_dir_scaled = self.scaler_dir.transform(X_dir_filled)
            else:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ Direction Scaler, ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö")
                X_dir_scaled = X_dir_filled
            
            # Price prediction
            X_price = df_to_predict[price_features]
            X_price_filled = imputer.fit_transform(X_price)
            
            if self.scaler_price is not None:
                X_price_scaled = self.scaler_price.transform(X_price_filled)
            else:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ Price Scaler, ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö")
                X_price_scaled = X_price_filled
            
            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            # Direction prediction
            xgb_pred_dir = self.xgb_clf.predict(X_dir_scaled)
            xgb_pred_dir_proba = self.xgb_clf.predict_proba(X_dir_scaled)[:, 1]
            
            # Price prediction
            xgb_pred_price = self.xgb_reg.predict(X_price_scaled)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô DataFrame
            df_prepared.loc[prediction_mask, 'XGB_Predicted_Direction_Raw'] = xgb_pred_dir
            df_prepared.loc[prediction_mask, 'XGB_Predicted_Direction_Proba'] = xgb_pred_dir_proba
            df_prepared.loc[prediction_mask, 'XGB_Predicted_Price_Raw'] = xgb_pred_price
            
            # ‡πÉ‡∏ä‡πâ Direction ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô
            df_prepared.loc[prediction_mask, 'XGB_Predicted_Direction'] = xgb_pred_dir
            
            # ‡∏õ‡∏£‡∏±‡∏ö Price ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Direction ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ
            current_prices = df_to_predict['Close'].values
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì price adjustment ‡∏ï‡∏≤‡∏° direction
            price_adjustments = []
            for i, (current_price, pred_dir, raw_price) in enumerate(zip(current_prices, xgb_pred_dir, xgb_pred_price)):
                raw_change_pct = (raw_price - current_price) / current_price
                
                if pred_dir == 1:  # ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
                    if raw_price <= current_price:  # ‡πÅ‡∏ï‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏•‡∏á
                        # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (0.5-2%)
                        adjusted_change = max(0.005, abs(raw_change_pct) * 0.5)
                        adjusted_price = current_price * (1 + adjusted_change)
                    else:
                        adjusted_price = raw_price  # ‡πÉ‡∏ä‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏î‡∏¥‡∏°
                else:  # ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏•‡∏á
                    if raw_price >= current_price:  # ‡πÅ‡∏ï‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
                        # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (0.5-2%)
                        adjusted_change = max(0.005, abs(raw_change_pct) * 0.5)
                        adjusted_price = current_price * (1 - adjusted_change)
                    else:
                        adjusted_price = raw_price  # ‡πÉ‡∏ä‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏î‡∏¥‡∏°
                
                price_adjustments.append(adjusted_price)
            
            df_prepared.loc[prediction_mask, 'XGB_Predicted_Price'] = price_adjustments
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì confidence score
            df_prepared.loc[prediction_mask, 'XGB_Confidence'] = np.abs(xgb_pred_dir_proba - 0.5) * 2
            
            print(f"‚úÖ XGBoost Meta-Learner ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {prediction_mask.sum()} ‡πÅ‡∏ñ‡∏ß (Direction-focused)")
            
            print(f"‚úÖ XGBoost Meta-Learner ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {prediction_mask.sum()} ‡πÅ‡∏ñ‡∏ß")
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ XGBoost: {e}")
            import traceback
            traceback.print_exc()
            
        return df_prepared
    
    def should_retrain_meta(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£ retrain XGBoost ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        meta_last_trained_path = "meta_last_trained.txt"
        
        if not os.path.exists(meta_last_trained_path):
            return True
        
        try:
            with open(meta_last_trained_path, "r") as f:
                last_trained_str = f.read().strip()
            last_trained_date = datetime.strptime(last_trained_str, "%Y-%m-%d")
            
            days_since_last_train = (datetime.now() - last_trained_date).days
            return days_since_last_train >= self.retrain_frequency
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà retrain: {e}")
            return True

# ======================== ENHANCED PREDICTION SYSTEM ========================

# ‡πÇ‡∏´‡∏•‡∏î configuration ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö environment
print("üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î configuration...")
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.env')

if not os.path.exists(path):
    print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå config.env ‡∏ó‡∏µ‡πà {path}")
    print("üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á config.env...")
    
    try:
        with open(path, 'w') as f:
            f.write("# Database Configuration\n")
            f.write("DB_USER=your_username\n")
            f.write("DB_PASSWORD=your_password\n")
            f.write("DB_HOST=localhost\n")
            f.write("DB_NAME=your_database\n")
        
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á config.env ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏µ‡πà {path}")
        print("üìã ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì")
        exit()
        
    except Exception as e:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå config.env ‡πÑ‡∏î‡πâ: {e}")
        print("üìù ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå config.env ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
        print("   DB_USER=your_username")
        print("   DB_PASSWORD=your_password") 
        print("   DB_HOST=your_host")
        print("   DB_NAME=your_database")
        exit()

load_dotenv(path)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö environment variables
required_vars = ['DB_USER', 'DB_PASSWORD', 'DB_HOST', 'DB_NAME']
missing_vars = [var for var in required_vars if not os.getenv(var)]

if missing_vars:
    print(f"‚ùå ‡∏Ç‡∏≤‡∏î environment variables: {missing_vars}")
    exit()

try:
    DB_CONNECTION = f"mysql+mysqlconnector://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_NAME')}"
    print("‚úÖ Database connection string ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
except Exception as e:
    print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á database connection: {e}")
    exit()

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î
current_hour = datetime.now().hour
if 8 <= current_hour < 18:
    print("üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢ (SET)...")
    market_filter = "Thailand"
elif 19 <= current_hour or current_hour < 5:
    print("üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏∏‡πâ‡∏ô‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏≤ (NYSE & NASDAQ)...")
    market_filter = "America"
else:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏≤")
    exit()
MODEL_LSTM_PATH = "../LSTM_model/best_v6_plus_minimal_tuning_v2_final_model.keras"
MODEL_GRU_PATH = "../GRU_Model/best_v6_plus_minimal_tuning_v2_final_model.keras"
SEQ_LENGTH = 10
RETRAIN_FREQUENCY = 5

# Dynamic weight parameters
WEIGHT_DECAY = 0.95
MIN_WEIGHT = 0.1
MAX_WEIGHT = 0.9

# ‡∏™‡∏£‡πâ‡∏≤‡∏á XGBoost Meta-Learner
print("üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° XGBoost Meta-Learner...")
meta_learner = XGBoostMetaLearner()

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ XGBoost models
xgb_available = (meta_learner.xgb_clf is not None and 
                meta_learner.xgb_reg is not None and
                meta_learner.scaler_dir is not None and 
                meta_learner.scaler_price is not None)

if xgb_available:
    print("‚úÖ XGBoost Meta-Learner ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
else:
    print("‚ö†Ô∏è XGBoost Meta-Learner ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° - ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Dynamic Weight ‡πÅ‡∏ó‡∏ô")
    missing_files = []
    if meta_learner.xgb_clf is None:
        missing_files.append("XGBoost Classifier")
    if meta_learner.xgb_reg is None:
        missing_files.append("XGBoost Regressor") 
    if meta_learner.scaler_dir is None:
        missing_files.append("Direction Scaler")
    if meta_learner.scaler_price is None:
        missing_files.append("Price Scaler")
    print(f"   ‡∏Ç‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå: {missing_files}")
    print("   üí° ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ XGBoost Meta-Learner:")
    print("      1. ‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô XGBoost ‡∏Å‡πà‡∏≠‡∏ô")
    print("      2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå .pkl ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô directory ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô")

def fetch_latest_data():
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    try:
        engine = sqlalchemy.create_engine(DB_CONNECTION)

        query = f"""
            SELECT 
                StockDetail.Date, 
                StockDetail.StockSymbol, 
                Stock.Market,  
                StockDetail.OpenPrice AS Open, 
                StockDetail.HighPrice AS High, 
                StockDetail.LowPrice AS Low, 
                StockDetail.ClosePrice AS Close, 
                StockDetail.Volume, 
                StockDetail.P_BV_Ratio,
                StockDetail.Sentiment, 
                StockDetail.Changepercen AS Change_Percent, 
                StockDetail.TotalRevenue, 
                StockDetail.QoQGrowth, 
                StockDetail.EPS, 
                StockDetail.ROE, 
                StockDetail.NetProfitMargin, 
                StockDetail.DebtToEquity, 
                StockDetail.PERatio, 
                StockDetail.Dividend_Yield, 
                StockDetail.positive_news, 
                StockDetail.negative_news, 
                StockDetail.neutral_news,
                StockDetail.PredictionClose_GRU, 
                StockDetail.PredictionClose_LSTM, 
                StockDetail.PredictionTrend_GRU, 
                StockDetail.PredictionTrend_LSTM 
            FROM StockDetail
            LEFT JOIN Stock ON StockDetail.StockSymbol = Stock.StockSymbol
            WHERE Stock.Market = '{market_filter}'  
            AND StockDetail.Date >= CURDATE() - INTERVAL 350 DAY
            ORDER BY StockDetail.StockSymbol, StockDetail.Date ASC;
        """

        df = pd.read_sql(query, engine)
        engine.dispose()

        if df.empty:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà")
            return df

        # Data processing
        df['Date'] = pd.to_datetime(df['Date'])
        
        # Fill missing dates for each stock
        grouped = df.groupby('StockSymbol')
        filled_dfs = []
        
        for name, group in grouped:
            # Create complete date range for this stock
            all_dates = pd.date_range(start=group['Date'].min(), end=group['Date'].max(), freq='D')
            temp_df = pd.DataFrame({'Date': all_dates})
            temp_df['StockSymbol'] = name
            # Merge with original data
            merged = pd.merge(temp_df, group, on=['StockSymbol', 'Date'], how='left')
            # Forward fill missing values
            financial_cols = [
                'TotalRevenue', 'QoQGrowth', 'EPS', 'ROE',
                'NetProfitMargin', 'DebtToEquity', 'PERatio', 'Dividend_Yield'
            ]
            merged[financial_cols] = merged[financial_cols].fillna(0)
            merged = merged.ffill()
            filled_dfs.append(merged)
        
        df = pd.concat(filled_dfs, ignore_index=True)
        
        # Calculate technical indicators for each stock - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç DeprecationWarning
        def calculate_indicators(group):
            if len(group) < 14:
                return group
                
            try:
                # Calculate RSI
                group['RSI'] = ta.momentum.RSIIndicator(group['Close'], window=14).rsi()
                
                # Calculate EMAs
                group['EMA_12'] = group['Close'].ewm(span=12, adjust=False).mean()
                group['EMA_26'] = group['Close'].ewm(span=26, adjust=False).mean()
                group['EMA_10'] = group['Close'].ewm(span=10, adjust=False).mean()
                group['EMA_20'] = group['Close'].ewm(span=20, adjust=False).mean()
                
                # Calculate SMAs
                group['SMA_50'] = group['Close'].rolling(window=50).mean()
                group['SMA_200'] = group['Close'].rolling(window=200).mean()
                
                # Calculate MACD
                group['MACD'] = group['EMA_12'] - group['EMA_26']
                group['MACD_Signal'] = group['MACD'].rolling(window=9).mean()
                
                # Calculate ATR
                if len(group) >= 14:
                    atr = ta.volatility.AverageTrueRange(high=group['High'], low=group['Low'], close=group['Close'], window=14)
                    group['ATR'] = atr.average_true_range()
                
                # Calculate Bollinger Bands
                bollinger = ta.volatility.BollingerBands(group['Close'], window=20, window_dev=2)
                group['Bollinger_High'] = bollinger.bollinger_hband()
                group['Bollinger_Low'] = bollinger.bollinger_lband()
                
                # Convert Sentiment to numerical values
                group['Sentiment'] = group['Sentiment'].map({'Positive': 1, 'Negative': -1, 'Neutral': 0})
                
                # Calculate Keltner Channel
                keltner = ta.volatility.KeltnerChannel(high=group['High'], low=group['Low'], close=group['Close'], window=20, window_atr=10)
                group['Keltner_High'] = keltner.keltner_channel_hband()
                group['Keltner_Low'] = keltner.keltner_channel_lband()
                group['Keltner_Middle'] = keltner.keltner_channel_mband()
                
                # Calculate Chaikin Volatility
                window_cv = 10
                group['High_Low_Diff'] = group['High'] - group['Low']
                group['High_Low_EMA'] = group['High_Low_Diff'].ewm(span=window_cv, adjust=False).mean()
                group['Chaikin_Vol'] = group['High_Low_EMA'].pct_change(periods=window_cv) * 100
                
                # Calculate Donchian Channel
                window_dc = 20
                group['Donchian_High'] = group['High'].rolling(window=window_dc).max()
                group['Donchian_Low'] = group['Low'].rolling(window=window_dc).min()
                
                # Calculate PSAR
                psar = ta.trend.PSARIndicator(high=group['High'], low=group['Low'], close=group['Close'], step=0.02, max_step=0.2)
                group['PSAR'] = psar.psar()
                
                # Add date-related features
                group['DayOfWeek'] = group['Date'].dt.dayofweek
                group['Is_Day_0'] = (group['Date'].dt.dayofweek == 0).astype(int)  # Monday
                group['Is_Day_4'] = (group['Date'].dt.dayofweek == 4).astype(int)  # Friday
                group['DayOfMonth'] = group['Date'].dt.day
                group['IsFirstHalfOfMonth'] = (group['Date'].dt.day <= 15).astype(int)
                group['IsSecondHalfOfMonth'] = (group['Date'].dt.day > 15).astype(int)
                
            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indicators ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {group['StockSymbol'].iloc[0] if not group.empty else 'Unknown'}: {e}")
            
            return group
        
        # Apply indicators calculation to each stock group - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç DeprecationWarning
        df = df.groupby('StockSymbol', group_keys=False).apply(calculate_indicators)
        df = df.reset_index(drop=True)
        
        # Handle missing values
        critical_columns = ['Open', 'High', 'Low', 'Close']
        df = df.dropna(subset=critical_columns)
        
        # Fill NaN values
        df = df.ffill().bfill()
        
        # Fill remaining NaN with 0 for technical indicators
        technical_columns = ['RSI', 'MACD', 'MACD_Signal', 'ATR', 
                            'Bollinger_High', 'Bollinger_Low', 'SMA_50', 'SMA_200',
                            'EMA_10', 'EMA_20', 'Keltner_High', 'Keltner_Low', 'Keltner_Middle',
                            'Chaikin_Vol', 'Donchian_High', 'Donchian_Low', 'PSAR']
        
        for col in technical_columns:
            if col in df.columns:
                df[col] = df[col].fillna(0)
        
        # Fill remaining NaN with 0
        df = df.fillna(0)
        
        print(f"‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: {len(df)} ‡πÅ‡∏ñ‡∏ß, {len(df['StockSymbol'].unique())} ‡∏´‡∏∏‡πâ‡∏ô")
        print(f"üìä Technical indicators ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ: {[col for col in technical_columns if col in df.columns]}")
        
        return df
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def calculate_dynamic_weights(df_ticker, price_weight_factor=0.6, direction_weight_factor=0.4):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì dynamic weight ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU ‡∏ï‡∏≤‡∏° performance ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
    """
    
    # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 15 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì weight
    recent_data = df_ticker.tail(15)
    
    if len(recent_data) < 5:
        # ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏ä‡πâ weight ‡πÄ‡∏ó‡πà‡∏≤‡πÜ ‡∏Å‡∏±‡∏ô
        return 0.5, 0.5
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ columns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    required_cols = ['PredictionClose_LSTM', 'PredictionClose_GRU', 
                     'PredictionTrend_LSTM', 'PredictionTrend_GRU']
    
    if not all(col in df_ticker.columns for col in required_cols):
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• predictions ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dynamic weighting")
        return 0.5, 0.5
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì price performance
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á predictions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö error calculation
        lstm_predictions = recent_data['PredictionClose_LSTM'].dropna()
        gru_predictions = recent_data['PredictionClose_GRU'].dropna()
        actual_prices = recent_data['Close'].dropna()
        
        if len(lstm_predictions) >= 3 and len(gru_predictions) >= 3 and len(actual_prices) >= 3:
            # ‡πÉ‡∏ä‡πâ length ‡∏ó‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á index mismatch
            min_len = min(len(lstm_predictions), len(gru_predictions), len(actual_prices))
            
            if min_len >= 2:
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì MAE ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö price predictions - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç index alignment
                lstm_pred_vals = lstm_predictions.iloc[:min_len-1].values
                gru_pred_vals = gru_predictions.iloc[:min_len-1].values
                actual_vals_next = actual_prices.iloc[1:min_len].values
                
                lstm_price_error = np.mean(np.abs(lstm_pred_vals - actual_vals_next))
                gru_price_error = np.mean(np.abs(gru_pred_vals - actual_vals_next))
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì direction accuracy - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç index alignment
                actual_vals_current = actual_prices.iloc[:min_len-1].values
                
                lstm_dir_pred = (lstm_pred_vals > actual_vals_current).astype(int)
                gru_dir_pred = (gru_pred_vals > actual_vals_current).astype(int)
                actual_dir = (actual_vals_next > actual_vals_current).astype(int)
                
                lstm_dir_acc = np.mean(lstm_dir_pred == actual_dir)
                gru_dir_acc = np.mean(gru_dir_pred == actual_dir)
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì weights
                total_price_error = lstm_price_error + gru_price_error
                if total_price_error > 0:
                    lstm_price_score = gru_price_error / total_price_error  # ‡∏Å‡∏•‡∏±‡∏ö‡∏Ñ‡πà‡∏≤
                    gru_price_score = lstm_price_error / total_price_error
                else:
                    lstm_price_score = 0.5
                    gru_price_score = 0.5
                
                total_dir_acc = lstm_dir_acc + gru_dir_acc
                if total_dir_acc > 0:
                    lstm_dir_score = lstm_dir_acc / total_dir_acc
                    gru_dir_score = gru_dir_acc / total_dir_acc
                else:
                    lstm_dir_score = 0.5
                    gru_dir_score = 0.5
                
                # ‡∏£‡∏ß‡∏° weights
                lstm_weight = (price_weight_factor * lstm_price_score + 
                              direction_weight_factor * lstm_dir_score)
                gru_weight = (price_weight_factor * gru_price_score + 
                             direction_weight_factor * gru_dir_score)
                
                # Normalize
                total_weight = lstm_weight + gru_weight
                if total_weight > 0:
                    lstm_weight = lstm_weight / total_weight
                    gru_weight = gru_weight / total_weight
                else:
                    lstm_weight = 0.5
                    gru_weight = 0.5
                
                # Apply constraints
                lstm_weight = max(MIN_WEIGHT, min(MAX_WEIGHT, lstm_weight))
                gru_weight = max(MIN_WEIGHT, min(MAX_WEIGHT, gru_weight))
                
                # Re-normalize
                total_weight = lstm_weight + gru_weight
                lstm_weight = lstm_weight / total_weight
                gru_weight = gru_weight / total_weight
                
                return lstm_weight, gru_weight
            
    except Exception as e:
        print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì dynamic weights: {e}")
    
    return 0.5, 0.5

def predict_future_day_with_meta(model_lstm, model_gru, df, feature_columns, 
                                scaler_features, scaler_target, ticker_encoder, seq_length):
    """
    ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ LSTM/GRU + XGBoost Meta-Learner
    """
    
    future_predictions = []
    tickers = df['StockSymbol'].unique()
    
    print("\nüîÆ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ 3-Layer Ensemble (LSTM + GRU + XGBoost)...")

    for ticker in tickers:
        print(f"\nüìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô: {ticker}")
        df_ticker = df[df['StockSymbol'] == ticker].sort_values('Date').reset_index(drop=True)

        if len(df_ticker) < seq_length:
            print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {ticker}, ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ...")
            continue

        try:
            # 1. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ LSTM ‡πÅ‡∏•‡∏∞ GRU
            latest_data = df_ticker.iloc[-seq_length:]
            features_scaled = scaler_features.transform(latest_data[feature_columns])
            ticker_ids = latest_data["Ticker_ID"].values
            market_ids = latest_data["Market_ID"].values

            X_feat = features_scaled.reshape(1, seq_length, -1)
            X_ticker = ticker_ids.reshape(1, seq_length)
            X_market = market_ids.reshape(1, seq_length)

            # LSTM predictions
            pred_output_lstm = model_lstm.predict([X_feat, X_ticker, X_market], verbose=0)
            pred_price_lstm_scaled = np.squeeze(pred_output_lstm[0])
            pred_direction_lstm = np.squeeze(pred_output_lstm[1])
            pred_price_lstm = scaler_target.inverse_transform(pred_price_lstm_scaled.reshape(-1, 1)).flatten()[0]

            # GRU predictions
            pred_output_gru = model_gru.predict([X_feat, X_ticker, X_market], verbose=0)
            pred_price_gru_scaled = np.squeeze(pred_output_gru[0])
            pred_direction_gru = np.squeeze(pred_output_gru[1])
            pred_price_gru = scaler_target.inverse_transform(pred_price_gru_scaled.reshape(-1, 1)).flatten()[0]

            # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XGBoost Meta-Learner
            meta_input = pd.DataFrame({
                'StockSymbol': [ticker],
                'Date': [df_ticker['Date'].max()],
                'Close': [df_ticker.iloc[-1]['Close']],
                'High': [df_ticker.iloc[-1]['High']] if 'High' in df_ticker.columns else [df_ticker.iloc[-1]['Close'] * 1.01],
                'Low': [df_ticker.iloc[-1]['Low']] if 'Low' in df_ticker.columns else [df_ticker.iloc[-1]['Close'] * 0.99],
                'PredictionClose_LSTM': [pred_price_lstm],
                'PredictionClose_GRU': [pred_price_gru],
                'PredictionTrend_LSTM': [1 if pred_direction_lstm > 0.5 else 0],
                'PredictionTrend_GRU': [1 if pred_direction_gru > 0.5 else 0]
            })
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö technical indicators
            historical_data = df_ticker.tail(30).copy()  # ‡πÉ‡∏ä‡πâ 30 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            historical_data = pd.concat([historical_data, meta_input], ignore_index=True)
            
            # 3. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ XGBoost Meta-Learner
            meta_predictions = meta_learner.predict_meta(historical_data)
            
            if 'XGB_Predicted_Price' in meta_predictions.columns:
                # ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å XGBoost
                final_predicted_price = meta_predictions['XGB_Predicted_Price'].iloc[-1]
                final_predicted_direction = meta_predictions['XGB_Predicted_Direction'].iloc[-1]
                final_direction_prob = meta_predictions['XGB_Predicted_Direction_Proba'].iloc[-1]
                xgb_confidence = meta_predictions['XGB_Confidence'].iloc[-1]
                ensemble_method = "XGBoost Meta-Learner"
            else:
                # Fallback: ‡πÉ‡∏ä‡πâ Dynamic Weight ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU
                lstm_weight, gru_weight = calculate_dynamic_weights(df_ticker)
                final_predicted_price = lstm_weight * pred_price_lstm + gru_weight * pred_price_gru
                final_direction_prob = lstm_weight * pred_direction_lstm + gru_weight * pred_direction_gru
                final_predicted_direction = 1 if final_direction_prob > 0.5 else 0
                xgb_confidence = abs(final_direction_prob - 0.5) * 2
                ensemble_method = "Dynamic Weight Fallback"

            # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            last_date = df_ticker['Date'].max()
            next_day = last_date + pd.Timedelta(days=1)
            current_close = df_ticker.iloc[-1]['Close']
            
            # Model agreement
            lstm_dir = 1 if pred_direction_lstm > 0.5 else 0
            gru_dir = 1 if pred_direction_gru > 0.5 else 0
            model_agreement = 1 if lstm_dir == gru_dir else 0
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            prediction_result = {
                'StockSymbol': ticker,
                'Date': next_day,
                'Predicted_Price': final_predicted_price,
                'Predicted_Direction': final_predicted_direction,
                'Direction_Probability': final_direction_prob,
                'XGB_Confidence': xgb_confidence,
                'Ensemble_Method': ensemble_method,
                'LSTM_Direction': lstm_dir,
                'GRU_Direction': gru_dir,
                'LSTM_Prediction': pred_price_lstm,
                'GRU_Prediction': pred_price_gru,
                'Last_Close': current_close,
                'Price_Change': final_predicted_price - current_close,
                'Price_Change_Percent': (final_predicted_price - current_close) / current_close * 100,
                'Model_Agreement': model_agreement
            }
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• debug ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XGBoost
            if 'XGB_Predicted_Price' in meta_predictions.columns:
                price_change_pct = prediction_result['Price_Change_Percent']
                direction_consistent = ((price_change_pct > 0 and final_predicted_direction == 1) or 
                                      (price_change_pct <= 0 and final_predicted_direction == 0))
                consistency_status = "‚úÖ" if direction_consistent else "‚ùå"
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤
                raw_price = meta_predictions['XGB_Predicted_Price_Raw'].iloc[-1] if 'XGB_Predicted_Price_Raw' in meta_predictions.columns else final_predicted_price
                raw_change = (raw_price - current_close) / current_close * 100
                
                print(f"    üéØ Direction: {int(final_predicted_direction)} (Confidence: {xgb_confidence:.3f})")
                print(f"    üìä Price: {raw_change:+.2f}% ‚Üí {price_change_pct:+.2f}% {consistency_status}")
            
            future_predictions.append(prediction_result)
            
            print(f"‚úÖ {ticker}: {ensemble_method} - "
                  f"Price: {final_predicted_price:.2f} "
                  f"({prediction_result['Price_Change_Percent']:.2f}%) "
                  f"Confidence: {xgb_confidence:.3f}")
                  
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ {ticker}: {e}")
            continue

    return pd.DataFrame(future_predictions)

def save_predictions_simple(predictions_df):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢
    ‡πÄ‡∏Å‡πá‡∏ö: ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà, ‡∏´‡∏∏‡πâ‡∏ô, ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (LSTM, GRU, Ensemble), ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (LSTM, GRU, Ensemble)
    """
    if predictions_df.empty:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
        return False

    try:
        engine = sqlalchemy.create_engine(DB_CONNECTION)
        
        with engine.connect() as connection:
            success_count = 0
            created_count = 0
            updated_count = 0
            
            for _, row in predictions_df.iterrows():
                try:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ record ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    check_query = sqlalchemy.text("""
                        SELECT COUNT(*) FROM StockDetail 
                        WHERE StockSymbol = :symbol AND Date = :date
                    """)
                    
                    result = connection.execute(check_query, {
                        'symbol': row['StockSymbol'],
                        'date': row['Date'].strftime('%Y-%m-%d')
                    })
                    exists = result.scalar()
                    
                    if exists > 0:
                        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï predictions ‡∏ó‡∏±‡πâ‡∏á LSTM, GRU, ‡πÅ‡∏•‡∏∞ Ensemble
                        update_query = sqlalchemy.text("""
                            UPDATE StockDetail
                            SET PredictionClose_LSTM = :lstm_price,
                                PredictionTrend_LSTM = :lstm_trend,
                                PredictionClose_GRU = :gru_price,
                                PredictionTrend_GRU = :gru_trend,
                                PredictionClose_Ensemble = :ensemble_price, 
                                PredictionTrend_Ensemble = :ensemble_trend
                            WHERE StockSymbol = :symbol AND Date = :date
                        """)
                        
                        connection.execute(update_query, {
                            'lstm_price': float(row.get('LSTM_Prediction', row['Predicted_Price'])),
                            'lstm_trend': int(row.get('LSTM_Direction', row['Predicted_Direction'])),
                            'gru_price': float(row.get('GRU_Prediction', row['Predicted_Price'])),
                            'gru_trend': int(row.get('GRU_Direction', row['Predicted_Direction'])),
                            'ensemble_price': float(row['Predicted_Price']),
                            'ensemble_trend': int(row['Predicted_Direction']),
                            'symbol': row['StockSymbol'],
                            'date': row['Date'].strftime('%Y-%m-%d')
                        })
                        print(f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï {row['StockSymbol']} (LSTM+GRU+Ensemble)")
                        updated_count += 1
                        
                    else:
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á record ‡πÉ‡∏´‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° predictions ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                        insert_query = sqlalchemy.text("""
                            INSERT INTO StockDetail 
                            (StockSymbol, Date, 
                             PredictionClose_LSTM, PredictionTrend_LSTM,
                             PredictionClose_GRU, PredictionTrend_GRU,
                             PredictionClose_Ensemble, PredictionTrend_Ensemble)
                            VALUES 
                            (:symbol, :date, 
                             :lstm_price, :lstm_trend,
                             :gru_price, :gru_trend,
                             :ensemble_price, :ensemble_trend)
                        """)
                        
                        connection.execute(insert_query, {
                            'symbol': row['StockSymbol'],
                            'date': row['Date'].strftime('%Y-%m-%d'),
                            'lstm_price': float(row.get('LSTM_Prediction', row['Predicted_Price'])),
                            'lstm_trend': int(row.get('LSTM_Direction', row['Predicted_Direction'])),
                            'gru_price': float(row.get('GRU_Prediction', row['Predicted_Price'])),
                            'gru_trend': int(row.get('GRU_Direction', row['Predicted_Direction'])),
                            'ensemble_price': float(row['Predicted_Price']),
                            'ensemble_trend': int(row['Predicted_Direction'])
                        })
                        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà {row['StockSymbol']} (LSTM+GRU+Ensemble)")
                        created_count += 1
                    
                    success_count += 1
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {row['StockSymbol']}: {e}")
                    continue
            
            # Commit ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
            connection.commit()
            
            print(f"\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
            print(f"   üìä ‡∏£‡∏ß‡∏°: {success_count}/{len(predictions_df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            if updated_count > 0:
                print(f"   üîÑ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï: {updated_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            if created_count > 0:
                print(f"   ‚ûï ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà: {created_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            print(f"   üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: LSTM + GRU + Ensemble predictions")
            
            return success_count > 0
            
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
        import traceback
        traceback.print_exc()
        return False

# ======================== MAIN EXECUTION ========================

# Add the fixed create_ticker_scalers function
def create_ticker_scalers_fixed(df, feature_columns, scaler_file_path="../LSTM_model/ticker_scalers.pkl"):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á ticker scalers ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô"""
    
    def clean_data_for_scalers(df, feature_columns):
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á scalers"""
        print("üßπ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scalers...")
        
        df_clean = df.copy()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        for col in feature_columns:
            if col in df_clean.columns:
                try:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
                    col_data = df_clean[col].astype(str)
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÄ‡∏ä‡πà‡∏ô 49.0349.03)
                    problematic_mask = col_data.str.contains(r'\d+\.\d+\d+\.\d+', regex=True, na=False)
                    
                    if problematic_mask.any():
                        print(f"   ‚ö†Ô∏è ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå {col}: {problematic_mask.sum()} ‡πÅ‡∏ñ‡∏ß")
                        
                        # ‡πÅ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô (‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
                        def extract_first_number(x):
                            try:
                                if pd.isna(x):
                                    return 0.0
                                x_str = str(x)
                                # ‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
                                import re
                                match = re.search(r'^\d+\.?\d*', x_str)
                                if match:
                                    return float(match.group())
                                else:
                                    return 0.0
                            except:
                                return 0.0
                        
                        df_clean[col] = col_data.apply(extract_first_number)
                    else:
                        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numeric ‡∏õ‡∏Å‡∏ï‡∏¥
                        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
                    
                    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡πÅ‡∏•‡∏∞ inf ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                    col_mean = df_clean[col].replace([np.inf, -np.inf], np.nan).mean()
                    if pd.isna(col_mean):
                        col_mean = 0.0
                    
                    df_clean[col] = df_clean[col].replace([np.inf, -np.inf, np.nan], col_mean)
                    
                    print(f"   ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î {col}: {df_clean[col].dtype}, range: {df_clean[col].min():.3f} - {df_clean[col].max():.3f}")
                    
                except Exception as e:
                    print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î {col}: {e}")
                    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                    df_clean[col] = 0.0
        
        return df_clean
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô
    df_clean = clean_data_for_scalers(df, feature_columns)
    
    ticker_scalers = {}
    tickers = df_clean['StockSymbol'].unique()
    
    print("üîß Creating/loading individual scalers for each ticker...")
    
    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î scalers ‡πÄ‡∏Å‡πà‡∏≤
    pre_trained_scalers = {}
    try:
        if os.path.exists(scaler_file_path):
            import joblib
            pre_trained_scalers = joblib.load(scaler_file_path)
            print(f"‚úÖ Loaded pre-trained scalers from {scaler_file_path} for {len(pre_trained_scalers)} tickers")
        else:
            print(f"‚ö†Ô∏è No pre-trained scalers found at {scaler_file_path}, creating new scalers")
    except Exception as e:
        print(f"‚ùå Error loading pre-trained scalers: {e}, creating new scalers")
    
    for ticker in tickers:
        df_ticker = df_clean[df_clean['StockSymbol'] == ticker].copy()
        
        if len(df_ticker) < 30:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 30 ‡∏ß‡∏±‡∏ô
            print(f"   ‚ö†Ô∏è {ticker}: Not enough data ({len(df_ticker)} days), skipping...")
            continue
        
        ticker_id = df_ticker['Ticker_ID'].iloc[0]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö pre-trained scaler
        if ticker_id in pre_trained_scalers:
            scaler_info = pre_trained_scalers[ticker_id]
            if all(key in scaler_info for key in ['ticker_symbol', 'feature_scaler', 'price_scaler']):
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ scaler ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                try:
                    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö transform ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                    test_data = df_ticker[feature_columns].iloc[:5].fillna(df_ticker[feature_columns].mean())
                    _ = scaler_info['feature_scaler'].transform(test_data)
                    _ = scaler_info['price_scaler'].transform(df_ticker[['Close']].iloc[:5])
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° data_points ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏≤‡∏î
                    if 'data_points' not in scaler_info:
                        scaler_info['data_points'] = len(df_ticker)
                    
                    ticker_scalers[ticker_id] = scaler_info
                    print(f"   ‚úÖ {ticker} (ID: {ticker_id}): Using pre-trained scaler with {scaler_info['data_points']} data points")
                    continue
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è {ticker} (ID: {ticker_id}): Pre-trained scaler failed test ({e}), creating new one")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler ‡πÉ‡∏´‡∏°‡πà
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á scaler
            feature_data = df_ticker[feature_columns].copy()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô numeric ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            non_numeric_cols = []
            for col in feature_columns:
                if col in feature_data.columns:
                    if not pd.api.types.is_numeric_dtype(feature_data[col]):
                        non_numeric_cols.append(col)
            
            if non_numeric_cols:
                print(f"   ‚ö†Ô∏è {ticker}: Non-numeric columns found: {non_numeric_cols}")
                for col in non_numeric_cols:
                    feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce')
            
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
            feature_data = feature_data.fillna(feature_data.mean()).fillna(0)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ infinite values ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            feature_data = feature_data.replace([np.inf, -np.inf], 0)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á feature scaler
            feature_scaler = RobustScaler()
            feature_scaler.fit(feature_data)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á price scaler
            price_scaler = RobustScaler()
            price_data = df_ticker[['Close']].copy()
            price_data = price_data.fillna(price_data.mean()).fillna(0)
            price_data = price_data.replace([np.inf, -np.inf], price_data.mean())
            price_scaler.fit(price_data)
            
            # ‡πÄ‡∏Å‡πá‡∏ö scaler ‡πÅ‡∏•‡∏∞ metadata
            ticker_scalers[ticker_id] = {
                'ticker_symbol': ticker,
                'feature_scaler': feature_scaler,
                'price_scaler': price_scaler,
                'data_points': len(df_ticker)
            }
            
            print(f"   ‚úÖ {ticker} (ID: {ticker_id}): Created new scaler with {len(df_ticker)} data points")
            
        except Exception as e:
            print(f"   ‚ùå {ticker}: Error creating scalers - {e}")
            # ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• debug
            print(f"      Debug - Feature data shape: {df_ticker[feature_columns].shape}")
            print(f"      Debug - Feature data dtypes: {df_ticker[feature_columns].dtypes.to_dict()}")
            if len(df_ticker) > 0:
                print(f"      Debug - Sample values: {df_ticker[feature_columns].iloc[0].to_dict()}")
            continue
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scalers ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß
    try:
        import joblib
        os.makedirs(os.path.dirname(scaler_file_path), exist_ok=True)
        joblib.dump(ticker_scalers, scaler_file_path)
        print(f"üíæ Saved updated scalers to {scaler_file_path}")
    except Exception as e:
        print(f"‚ùå Error saving scalers: {e}")
    
    print(f"‚úÖ Created/loaded scalers for {len(ticker_scalers)} tickers")
    return ticker_scalers

# ======================== CORRECTED MAIN EXECUTION ========================

if __name__ == "__main__":
    print("\nüöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏ö‡∏ö Enhanced 3-Layer Ensemble (Automated Mode)")
    print("üîß Using Unified Data Preparation System (Training + Online Learning Compatible)")
    print("‚ö° ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏±‡∏ô")

    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM ‡πÅ‡∏•‡∏∞ GRU
    print("\nü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM ‡πÅ‡∏•‡∏∞ GRU...")

    MODEL_LSTM_PATH = "../LSTM_model/best_v6_plus_minimal_tuning_v2_final_model.keras"
    MODEL_GRU_PATH = "../GRU_Model/best_v6_plus_minimal_tuning_v2_final_model.keras"

    if not os.path.exists(MODEL_LSTM_PATH):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM ‡∏ó‡∏µ‡πà {MODEL_LSTM_PATH}")
        sys.exit()

    if not os.path.exists(MODEL_GRU_PATH):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• GRU ‡∏ó‡∏µ‡πà {MODEL_GRU_PATH}")
        sys.exit()

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    print("\nüîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô...")
    should_retrain_lstm, lstm_reason = should_retrain_model("LSTM", retrain_frequency_days=5)
    should_retrain_gru, gru_reason = should_retrain_model("GRU", retrain_frequency_days=5)
    
    need_retrain = should_retrain_lstm or should_retrain_gru
    
    print(f"üìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô:")
    print(f"   üî¥ LSTM: {'‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô' if should_retrain_lstm else '‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô'} ({lstm_reason})")
    print(f"   üîµ GRU:  {'‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô' if should_retrain_gru else '‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô'} ({gru_reason})")

    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á instance ‡∏Ç‡∏≠‡∏á WalkForwardMiniRetrainManager
        manager = WalkForwardMiniRetrainManager(
            lstm_model_path=MODEL_LSTM_PATH,
            gru_model_path=MODEL_GRU_PATH
        )
        
        if need_retrain:
            print(f"\nüîÑ ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• - ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° compile...")
            model_lstm = manager.load_models_for_prediction(model_path=MODEL_LSTM_PATH, compile_model=True)
            model_gru = manager.load_models_for_prediction(model_path=MODEL_GRU_PATH, compile_model=True)
        else:
            print(f"\n‚úÖ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô - ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢...")
            model_lstm = manager.load_models_for_prediction(model_path=MODEL_LSTM_PATH, compile_model=False)
            model_gru = manager.load_models_for_prediction(model_path=MODEL_GRU_PATH, compile_model=False)
        
        if model_lstm is None or model_gru is None:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ")
            sys.exit()
            
        print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM ‡πÅ‡∏•‡∏∞ GRU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•
        print(f"üìä LSTM model: {len(model_lstm.layers)} layers")
        print(f"üìä GRU model: {len(model_gru.layers)} layers")
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
        sys.exit()

    # ‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    print("\nüì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    raw_df = fetch_latest_data()  # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

    if raw_df.empty:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
        sys.exit()

    print(f"üìä ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö: {len(raw_df)} ‡πÅ‡∏ñ‡∏ß ‡∏à‡∏≤‡∏Å {len(raw_df['StockSymbol'].unique())} ‡∏´‡∏∏‡πâ‡∏ô")

    # ======== UNIFIED FEATURE PREPARATION ========
    # ‡πÉ‡∏ä‡πâ feature columns ‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
    base_feature_columns = [
        'Open', 'High', 'Low', 'Close', 'Volume', 'Change_Percent', 'Sentiment',
        'positive_news', 'negative_news', 'neutral_news',
        'TotalRevenue', 'QoQGrowth', 'EPS', 'ROE', 'NetProfitMargin', 
        'DebtToEquity', 'PERatio', 'Dividend_Yield', 'P_BV_Ratio'
    ]

    technical_feature_columns = [
        'ATR', 'Keltner_High', 'Keltner_Low', 'Keltner_Middle', 'Chaikin_Vol',
        'Donchian_High', 'Donchian_Low', 'PSAR',
        'RSI', 'EMA_10', 'EMA_20', 'MACD', 'MACD_Signal', 
        'Bollinger_High', 'Bollinger_Low', 'SMA_50', 'SMA_200'
    ]

    # ‡∏£‡∏ß‡∏° feature columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
    available_base = [col for col in base_feature_columns if col in raw_df.columns]
    available_technical = [col for col in technical_feature_columns if col in raw_df.columns]
    feature_columns = available_base + available_technical

    print(f"üìã Available feature columns ({len(feature_columns)}): {feature_columns}")

    if len(feature_columns) < 10:
        print("‚ùå ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• features ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 10 columns")
        sys.exit()

    # ======== UNIFIED DATA PREPARATION ========
    print("\nüîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ Unified Data Preparation System...")
    
    try:
        # ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏° (‡∏£‡∏ß‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ó‡∏£‡∏ô + ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)
        ticker_scalers, prepared_df = create_walk_forward_compatible_scalers(raw_df, feature_columns)
        
        if len(ticker_scalers) == 0:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á ticker scalers ‡πÑ‡∏î‡πâ")
            sys.exit()
        
        print(f"‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:")
        print(f"   üìä Prepared data: {len(prepared_df)} ‡πÅ‡∏ñ‡∏ß")
        print(f"   üè∑Ô∏è Ticker scalers: {len(ticker_scalers)} ‡∏ï‡∏±‡∏ß")
        print(f"   üìà Features: {len(feature_columns)} columns")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ target variables ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if 'Direction' in prepared_df.columns and 'TargetPrice' in prepared_df.columns:
            print(f"   üéØ Target variables created successfully")
        else:
            print(f"   ‚ö†Ô∏è Target variables may not be available for some operations")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ticker scalers
        print(f"\nüìã Ticker Scalers Summary:")
        for t_id, scaler_info in list(ticker_scalers.items())[:3]:  # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 3 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
            ticker_name = scaler_info.get('ticker', 'Unknown')
            data_points = scaler_info.get('data_points', 'Unknown') 
            print(f"   ‚Ä¢ {ticker_name} (ID: {t_id}): {data_points} data points")
        if len(ticker_scalers) > 3:
            print(f"   ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(ticker_scalers) - 3} tickers")
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
        import traceback
        traceback.print_exc()
        sys.exit()

    # ======== ENCODER PREPARATION ========
    print("\nüîß ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° encoders...")
    try:
        # ‡πÉ‡∏ä‡πâ encoders ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô prepared_df ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
        if 'Ticker_ID' not in prepared_df.columns:
            ticker_encoder = LabelEncoder()
            prepared_df["Ticker_ID"] = ticker_encoder.fit_transform(prepared_df["StockSymbol"])
        else:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á encoder ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
            ticker_encoder = LabelEncoder()
            ticker_encoder.fit(prepared_df["StockSymbol"])
        
        if 'Market_ID' not in prepared_df.columns:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Market_ID
            us_stock = ['AAPL', 'NVDA', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'AVGO', 'TSM', 'AMD']
            thai_stock = ['ADVANC', 'INTUCH', 'TRUE', 'DITTO', 'DIF', 
                         'INSET', 'JMART', 'INET', 'JAS', 'HUMAN']
            prepared_df['Market_ID'] = prepared_df['StockSymbol'].apply(
                lambda x: "US" if x in us_stock else "TH" if x in thai_stock else "OTHER"
            )
            
            market_encoder = LabelEncoder()
            prepared_df['Market_ID'] = market_encoder.fit_transform(prepared_df['Market_ID'])
        else:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á encoder ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
            market_encoder = LabelEncoder()
            market_encoder.fit(prepared_df['Market_ID'].astype(str))
        
        print("‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° encoders ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° encoders: {e}")
        sys.exit()

    # ======== AUTOMATED WORKFLOW ========
    
    if need_retrain:
        # ======================== RETRAIN MODE ========================
        print(f"\nüîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥...")
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå
        chunk_size = 200
        retrain_freq = 5
        SEQ_LENGTH = 10
        
        print(f"\nüéØ ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô:")
        print(f"   üì¶ Chunk size: {chunk_size} ‡∏ß‡∏±‡∏ô")
        print(f"   üîÑ Retrain frequency: {retrain_freq} ‡∏ß‡∏±‡∏ô")
        print(f"   üìà Sequence length: {SEQ_LENGTH} ‡∏ß‡∏±‡∏ô")
        print(f"   ü§ñ Models: LSTM + GRU (‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•)")
        
        retrain_success = False
        
        try:
            if should_retrain_lstm:
                print(f"\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô LSTM...")
                predictions_lstm, metrics_lstm = walk_forward_validation_multi_task_batch(
                    model=model_lstm,
                    df=prepared_df,
                    feature_columns=feature_columns,
                    ticker_scalers=ticker_scalers,
                    ticker_encoder=ticker_encoder,
                    market_encoder=market_encoder,
                    seq_length=SEQ_LENGTH,
                    retrain_frequency=retrain_freq,
                    chunk_size=chunk_size
                )
                
                if not predictions_lstm.empty:
                    predictions_lstm.to_csv('retrain_lstm_results.csv', index=False)
                    update_retrain_date("LSTM")
                    print("‚úÖ ‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô LSTM ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                    retrain_success = True
                
            if should_retrain_gru:
                print(f"\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô GRU...")
                predictions_gru, metrics_gru = walk_forward_validation_multi_task_batch(
                    model=model_gru,
                    df=prepared_df,
                    feature_columns=feature_columns,
                    ticker_scalers=ticker_scalers,
                    ticker_encoder=ticker_encoder,
                    market_encoder=market_encoder,
                    seq_length=SEQ_LENGTH,
                    retrain_frequency=retrain_freq,
                    chunk_size=chunk_size
                )
                
                if not predictions_gru.empty:
                    predictions_gru.to_csv('retrain_gru_results.csv', index=False)
                    update_retrain_date("GRU")
                    print("‚úÖ ‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô GRU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                    retrain_success = True
            
            if retrain_success:
                print(f"\nüéâ ‡∏Å‡∏≤‡∏£‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢...")
                print(f"üíæ ‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô: retrain_lstm_results.csv, retrain_gru_results.csv")
            else:
                print(f"\n‚ö†Ô∏è ‡∏Å‡∏≤‡∏£‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡πÑ‡∏õ...")
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô: {e}")
            print(f"‚ö†Ô∏è ‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏¥‡∏°...")
            import traceback
            traceback.print_exc()
    
    else:
        print(f"\n‚úÖ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô - ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô")

    # ======================== PREDICTION MODE ========================
    
    print(f"\nüîÆ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥...")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á main scalers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prediction
    print("üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á main scalers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prediction...")
    scaler_main_features = RobustScaler()
    scaler_main_target = RobustScaler()
    
    try:
        # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡πâ‡∏ß
        feature_data = prepared_df[feature_columns]
        if feature_data.isnull().any().any():
            print("‚ö†Ô∏è ‡∏û‡∏ö NaN ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• features, ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£...")
            feature_data = feature_data.fillna(feature_data.mean())
            prepared_df[feature_columns] = feature_data
        
        scaler_main_features.fit(prepared_df[feature_columns])
        scaler_main_target.fit(prepared_df[["Close"]])
        print("‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á main scalers ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á main scalers: {e}")
        sys.exit()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• predictions ‡∏à‡∏≤‡∏Å LSTM/GRU
    prediction_cols = ['PredictionClose_LSTM', 'PredictionClose_GRU', 
                      'PredictionTrend_LSTM', 'PredictionTrend_GRU']
    available_predictions = [col for col in prediction_cols if col in prepared_df.columns]
    print(f"üîÆ Available prediction columns: {available_predictions}")
    
    if len(available_predictions) < 4:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• predictions ‡∏à‡∏≤‡∏Å LSTM/GRU ‡∏Ñ‡∏£‡∏ö, XGBoost Meta-Learner ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ")
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î SEQ_LENGTH ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prediction
    SEQ_LENGTH = 10
    
    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Enhanced 3-Layer Ensemble
    future_predictions_df = predict_future_day_with_meta(
        model_lstm, model_gru, prepared_df, feature_columns, 
        scaler_main_features, scaler_main_target, ticker_encoder, SEQ_LENGTH
    )
    
    if not future_predictions_df.empty:
        print(f"\nüéØ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (Enhanced 3-Layer Ensemble):")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f'automated_predictions_{timestamp}.csv'
        future_predictions_df.to_csv(output_path, index=False)
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô {output_path}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        display_cols = ['StockSymbol', 'Date', 'Last_Close', 'Predicted_Price', 
                       'Price_Change_Percent', 'Predicted_Direction', 'XGB_Confidence',
                       'Ensemble_Method', 'Model_Agreement']
        
        if all(col in future_predictions_df.columns for col in display_cols):
            print(future_predictions_df[display_cols])
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            print(f"\nüìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:")
            print(f"   üìà ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {len(future_predictions_df)}")
            print(f"   üìà ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì BUY: {len(future_predictions_df[future_predictions_df['Predicted_Direction'] == 1])}")
            print(f"   üìà ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì SELL: {len(future_predictions_df[future_predictions_df['Predicted_Direction'] == 0])}")
            
            avg_confidence = future_predictions_df['XGB_Confidence'].mean()
            print(f"   üìà Average Confidence: {avg_confidence:.3f}")
            
            # ‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ confidence ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            high_confidence = future_predictions_df.nlargest(3, 'XGB_Confidence')
            print(f"\nüèÜ ‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ Confidence ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î:")
            for _, row in high_confidence.iterrows():
                direction_text = "üìà BUY" if row['Predicted_Direction'] == 1 else "üìâ SELL"
                print(f"   {row['StockSymbol']}: {direction_text} "
                      f"(Confidence: {row['XGB_Confidence']:.3f}, "
                      f"Expected: {row['Price_Change_Percent']:.2f}%)")
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
            print(f"\nüíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥...")
            db_save_success = save_predictions_simple(future_predictions_df)
            
            if db_save_success:
                print("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                print("üîÑ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß")
                print("üì± ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß")
            else:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
        else:
            missing_cols = [col for col in display_cols if col not in future_predictions_df.columns]
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î columns: {missing_cols}")
            print("‡πÅ‡∏ï‡πà‡πÑ‡∏ü‡∏•‡πå CSV ‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß")
    
    else:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á")
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
    print(f"\nüéâ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    print(f"üìã ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:")
    print(f"   üîÑ ‡∏Å‡∏≤‡∏£‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô: {'‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡πâ‡∏ß' if need_retrain else '‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô'}")
    print(f"   üîÆ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {'‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à' if not future_predictions_df.empty else '‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à'}")
    print(f"   üíæ ‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {output_path if not future_predictions_df.empty else '‡πÑ‡∏°‡πà‡∏°‡∏µ'}")
    print(f"   üóìÔ∏è ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ñ‡∏±‡∏î‡πÑ‡∏õ: {(datetime.now() + timedelta(days=5)).strftime('%Y-%m-%d')}")
    
    print("\nüîö ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏´‡∏∏‡πâ‡∏ô Enhanced 3-Layer Ensemble (Automated Mode)")
    print("‚ú® ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô")