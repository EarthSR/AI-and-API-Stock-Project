import numpy as np
import pandas as pd
import sqlalchemy
import os
import ta
from tensorflow.keras.models import load_model
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
import joblib
import warnings
from datetime import datetime
import mysql.connector
from dotenv import load_dotenv

warnings.filterwarnings("ignore", category=UserWarning)

# ------------------------- 1) CONFIG -------------------------
# Load environment variables first
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.env')
load_dotenv(path)

# Get database connection string from environment variables
DB_CONNECTION = f"mysql+mysqlconnector://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_NAME')}"

# Determine market filter based on current time
current_hour = datetime.now().hour

# if 8 <= current_hour < 18:
#     print("üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢ (SET)...")
#     market_filter = "Thailand"
# elif 19 <= current_hour or current_hour < 5:
#     print("üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏∏‡πâ‡∏ô‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏≤ (NYSE & NASDAQ)...")
#     market_filter = "America"
# else:
#     print("‚ùå ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏≤")
#     exit()
market_filter = "America"
MODEL_LSTM_PATH = "../LSTM_model/best_multi_task_model.keras"
MODEL_GRU_PATH = "../GRU_Model/best_multi_task_model.keras"
SEQ_LENGTH = 10
RETRAIN_FREQUENCY = 5
w_lstm, w_gru = 0.5, 0.5  # ‚úÖ ‡πÉ‡∏ä‡πâ Weighted Stacking ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU

# ------------------------- 2) ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Database -------------------------
def fetch_latest_data():
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MySQL
    """
    engine = sqlalchemy.create_engine(DB_CONNECTION)

    query = f"""
        SELECT 
            StockDetail.Date, 
            StockDetail.StockSymbol, 
            Stock.Market,  
            StockDetail.OpenPrice AS Open, 
            StockDetail.HighPrice AS High, 
            StockDetail.LowPrice AS Low, 
            StockDetail.ClosePrice AS Close, 
            StockDetail.Volume, 
            StockDetail.P_BV_Ratio,
            StockDetail.Sentiment, 
            StockDetail.Changepercen AS Change_Percent, 
            StockDetail.TotalRevenue, 
            StockDetail.QoQGrowth, 
            StockDetail.EPS, 
            StockDetail.ROE, 
            StockDetail.NetProfitMargin, 
            StockDetail.DebtToEquity, 
            StockDetail.PERatio, 
            StockDetail.Dividend_Yield, 
            StockDetail.positive_news, 
            StockDetail.negative_news, 
            StockDetail.neutral_news 
        FROM StockDetail
        LEFT JOIN Stock ON StockDetail.StockSymbol = Stock.StockSymbol
        WHERE Stock.Market = '{market_filter}'  
        AND StockDetail.Date >= CURDATE() - INTERVAL 365 DAY
        ORDER BY StockDetail.StockSymbol, StockDetail.Date ASC;
    """

    df = pd.read_sql(query, engine)
    engine.dispose()

    if df.empty:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà")
        return df

    # Convert date column to datetime
    df['Date'] = pd.to_datetime(df['Date'])
    
    # Fill missing dates for each stock
    grouped = df.groupby('StockSymbol')
    filled_dfs = []
    
    for name, group in grouped:
        # Create complete date range for this stock
        all_dates = pd.date_range(start=group['Date'].min(), end=group['Date'].max(), freq='D')
        temp_df = pd.DataFrame({'Date': all_dates})
        temp_df['StockSymbol'] = name
        # Merge with original data
        merged = pd.merge(temp_df, group, on=['StockSymbol', 'Date'], how='left')
        # Forward fill missing values
        merged.fillna(method='ffill', inplace=True)
        filled_dfs.append(merged)
    
    df = pd.concat(filled_dfs)
    
    # Calculate technical indicators for each stock
    def calculate_indicators(group):
        if len(group) < 14:
            return group
            
        # Calculate RSI
        group['RSI'] = ta.momentum.RSIIndicator(group['Close'], window=14).rsi()
        
        # Calculate EMAs
        group['EMA_12'] = group['Close'].ewm(span=12, adjust=False).mean()
        group['EMA_26'] = group['Close'].ewm(span=26, adjust=False).mean()
        group['EMA_10'] = group['Close'].ewm(span=10, adjust=False).mean()
        group['EMA_20'] = group['Close'].ewm(span=20, adjust=False).mean()
        
        # Calculate SMAs
        group['SMA_50'] = group['Close'].rolling(window=50).mean()
        group['SMA_200'] = group['Close'].rolling(window=200).mean()
        
        # Calculate MACD
        group['MACD'] = group['EMA_12'] - group['EMA_26']
        group['MACD_Signal'] = group['MACD'].rolling(window=9).mean()
        
        # Calculate ATR
        if len(group) >= 14:
            atr = ta.volatility.AverageTrueRange(high=group['High'], low=group['Low'], close=group['Close'], window=14)
            group['ATR'] = atr.average_true_range()
        
        # Calculate Bollinger Bands
        bollinger = ta.volatility.BollingerBands(group['Close'], window=20, window_dev=2)
        group['Bollinger_High'] = bollinger.bollinger_hband()
        group['Bollinger_Low'] = bollinger.bollinger_lband()
        
        # Convert Sentiment to numerical values
        group['Sentiment'] = group['Sentiment'].map({'Positive': 1, 'Negative': -1, 'Neutral': 0})
        
        # Calculate Keltner Channel
        keltner = ta.volatility.KeltnerChannel(high=group['High'], low=group['Low'], close=group['Close'], window=20, window_atr=10)
        group['Keltner_High'] = keltner.keltner_channel_hband()
        group['Keltner_Low'] = keltner.keltner_channel_lband()
        group['Keltner_Middle'] = keltner.keltner_channel_mband()
        
        # Calculate Chaikin Volatility
        window_cv = 10
        group['High_Low_Diff'] = group['High'] - group['Low']
        group['High_Low_EMA'] = group['High_Low_Diff'].ewm(span=window_cv, adjust=False).mean()
        group['Chaikin_Vol'] = group['High_Low_EMA'].pct_change(periods=window_cv) * 100
        
        # Calculate Donchian Channel
        window_dc = 20
        group['Donchian_High'] = group['High'].rolling(window=window_dc).max()
        group['Donchian_Low'] = group['Low'].rolling(window=window_dc).min()
        
        # Calculate PSAR
        psar = ta.trend.PSARIndicator(high=group['High'], low=group['Low'], close=group['Close'], step=0.02, max_step=0.2)
        group['PSAR'] = psar.psar()
        
        return group
    
    # Apply indicators calculation to each stock group
    df = df.groupby('StockSymbol').apply(calculate_indicators)
    
    # Drop rows with NaN values
    df.dropna(inplace=True)
    
    return df

# ------------------------- 3) ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• -------------------------
try:
    model_lstm = load_model(MODEL_LSTM_PATH, compile=False)
    model_gru = load_model(MODEL_GRU_PATH, compile=False)
    print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
except Exception as e:
    print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
    exit()

# ------------------------- 4) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• -------------------------
feature_columns = [
    'Open', 'High', 'Low', 'Close', 'Volume', 'Change_Percent', 'Sentiment','positive_news','negative_news','neutral_news',
    'TotalRevenue', 'QoQGrowth', 'EPS', 'ROE', 'NetProfitMargin', 
    'DebtToEquity', 'PERatio', 'Dividend_Yield', 'P_BV_Ratio',
    'ATR', 'Keltner_High', 'Keltner_Low', 'Keltner_Middle', 'Chaikin_Vol',
    'Donchian_High', 'Donchian_Low', 'PSAR',
    'RSI', 'EMA_10', 'EMA_20', 'MACD', 'MACD_Signal', 
    'Bollinger_High', 'Bollinger_Low', 'SMA_50', 'SMA_200'
]

# ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
test_df = fetch_latest_data()

if test_df.empty:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
    exit()

us_stock = ['AAPL', 'NVDA', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'AVGO', 'TSM', 'AMD']
thai_stock = ['ADVANC', 'INTUCH', 'TRUE', 'DITTO', 'DIF', 
           'INSET', 'JMART', 'INET', 'JAS', 'HUMAN']
test_df['Market_ID'] = test_df['StockSymbol'].apply(lambda x: "US" if x in us_stock else "TH" if x in thai_stock else None)

# Initialize scalers and encoder
scaler_features = RobustScaler()
scaler_target = RobustScaler()
ticker_encoder = LabelEncoder()
market_encoder = LabelEncoder()
# Fit and transform
test_df["Ticker_ID"] = ticker_encoder.fit_transform(test_df["StockSymbol"])
test_df['Market_ID'] = market_encoder.fit_transform(test_df['Market_ID'])
scaler_features.fit(test_df[feature_columns])
scaler_target.fit(test_df[["Close"]])

# ------------------------- 5) Predict Next Day -------------------------
def predict_next_day(model_lstm, model_gru, df, feature_columns, scaler_features, scaler_target, ticker_encoder, seq_length):
    all_predictions = []
    tickers = df['StockSymbol'].unique()

    for ticker in tickers:
        print(f"\nProcessing Ticker: {ticker}")
        df_ticker = df[df['StockSymbol'] == ticker].sort_values('Date').reset_index(drop=True)

        if len(df_ticker) < seq_length:
            print(f"‚ö†Ô∏è Not enough data for ticker {ticker}, skipping...")
            continue

        # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• seq_length ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        historical_data = df_ticker.iloc[-seq_length:]
        last_date = df_ticker.iloc[-1]["Date"]

        # ‡∏™‡πÄ‡∏Å‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        features_scaled = scaler_features.transform(historical_data[feature_columns])
        ticker_ids = historical_data["Ticker_ID"].values
        market_ids = historical_data["Market_ID"].values

        # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ LSTM ‡πÅ‡∏•‡∏∞ GRU
        try:
            # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ LSTM
            pred_price_lstm_output = model_lstm.predict(
                [features_scaled.reshape(1, seq_length, len(feature_columns)), 
                ticker_ids.reshape(1, seq_length),
                market_ids.reshape(1, seq_length)], 
                verbose=0
            )

            # ‡∏Å‡∏≥‡∏à‡∏±‡∏î list ‡∏ã‡πâ‡∏≠‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡πÅ‡∏•‡∏∞ squeeze
            if isinstance(pred_price_lstm_output, list):
                pred_price_lstm_scaled = np.squeeze(pred_price_lstm_output[0])
            else:
                pred_price_lstm_scaled = np.squeeze(pred_price_lstm_output)

            
            # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ GRU
            pred_price_gru_output = model_gru.predict(
                [features_scaled.reshape(1, seq_length, len(feature_columns)), 
                ticker_ids.reshape(1, seq_length),
                market_ids.reshape(1, seq_length)], 
                verbose=0
            )

            if isinstance(pred_price_gru_output, list):
                pred_price_gru_scaled = np.squeeze(pred_price_gru_output[0])
            else:
                pred_price_gru_scaled = np.squeeze(pred_price_gru_output)


            pred_price_lstm = scaler_target.inverse_transform(pred_price_lstm_scaled.reshape(-1, 1)).flatten()[0]
            pred_price_gru = scaler_target.inverse_transform(pred_price_gru_scaled.reshape(-1, 1)).flatten()[0]
            
            # ‡πÉ‡∏ä‡πâ Weighted Stacking ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á LSTM ‡πÅ‡∏•‡∏∞ GRU
            predicted_price = (w_lstm * pred_price_lstm) + (w_gru * pred_price_gru)

            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á
            predicted_direction = 1 if predicted_price > df_ticker.iloc[-1]['Close'] else 0

            all_predictions.append({
                'StockSymbol': ticker,
                'Date': last_date + pd.Timedelta(days=1),  # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                'Predicted_Price': predicted_price,
                'Predicted_Direction': predicted_direction
            })
            
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {ticker}: {e}")
            continue

    predictions_df = pd.DataFrame(all_predictions)
    return predictions_df

# ------------------------- 6) ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á Database -------------------------
def save_predictions_to_db(predictions_df):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏•‡∏á‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á `StockDetail`
    """
    if predictions_df.empty:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
        return

    engine = sqlalchemy.create_engine(DB_CONNECTION)
    
    with engine.connect() as connection:
        for _, row in predictions_df.iterrows():
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            check_query = f"""
                SELECT COUNT(*) FROM StockDetail 
                WHERE StockSymbol = '{row['StockSymbol']}' 
                AND Date = '{row['Date'].strftime('%Y-%m-%d')}'
            """
            exists = connection.execute(check_query).scalar()
            
            if exists > 0:
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
                update_query = f"""
                    UPDATE StockDetail
                    SET PredictionClose = {row['Predicted_Price']}, 
                        PredictionTrend = {row['Predicted_Direction']}
                    WHERE StockSymbol = '{row['StockSymbol']}' 
                    AND Date = '{row['Date'].strftime('%Y-%m-%d')}'
                """
            else:
                # ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
                insert_query = f"""
                    INSERT INTO StockDetail 
                    (StockSymbol, Date, PredictionClose, PredictionTrend)
                    VALUES (
                        '{row['StockSymbol']}', 
                        '{row['Date'].strftime('%Y-%m-%d')}', 
                        {row['Predicted_Price']}, 
                        {row['Predicted_Direction']}
                    )
                """
                update_query = insert_query
            
            try:
                connection.execute(update_query)
            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {row['StockSymbol']}: {e}")
                continue
        
        connection.commit()
    
    print("\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

# ------------------------- 7) RUN -------------------------
if __name__ == "__main__":
    print("\nüîÆ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ...")
    predictions_df = predict_next_day(
        model_lstm, model_gru, test_df, feature_columns, 
        scaler_features, scaler_target, ticker_encoder, SEQ_LENGTH
    )
    
    if not predictions_df.empty:
        print("\n‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:")
        print(predictions_df[['StockSymbol', 'Date', 'Predicted_Price', 'Predicted_Direction']])
        # save_predictions_to_db(predictions_df)
    else:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")