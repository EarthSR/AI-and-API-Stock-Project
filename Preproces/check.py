#!/usr/bin/env python3
"""
‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Scaler Duplication ‡πÅ‡∏•‡∏∞ Architecture Detection
"""

import os
import shutil
import joblib
import tensorflow as tf
import numpy as np
from datetime import datetime

def detailed_architecture_analysis():
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå architecture ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"""
    print("üîç Detailed Architecture Analysis...")
    
    model_paths = {
        'LSTM': "../LSTM_model/best_v6_plus_minimal_tuning_v2_final_model.keras",
        'GRU': "../GRU_Model/best_v6_plus_minimal_tuning_v2_final_model.keras"
    }
    
    for model_name, model_path in model_paths.items():
        if not os.path.exists(model_path):
            continue
            
        print(f"\nüìä {model_name} Detailed Analysis:")
        
        try:
            model = tf.keras.models.load_model(model_path, compile=False)
            
            print(f"   üìö Total layers: {len(model.layers)}")
            print(f"   üß† Total parameters: {model.count_params():,}")
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏∏‡∏Å layer
            lstm_count = 0
            gru_count = 0
            bidirectional_count = 0
            
            print(f"   üîç Layer by layer analysis:")
            for i, layer in enumerate(model.layers):
                layer_class = layer.__class__.__name__
                layer_name = layer.name
                
                print(f"      {i:2d}. {layer_name} ({layer_class})")
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Bidirectional wrapper
                if layer_class == 'Bidirectional':
                    bidirectional_count += 1
                    # ‡∏î‡∏π layer ‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô Bidirectional
                    if hasattr(layer, 'layer'):
                        inner_layer = layer.layer
                        inner_class = inner_layer.__class__.__name__
                        print(f"          ‚îî‚îÄ Inner: {inner_class}")
                        
                        if 'LSTM' in inner_class:
                            lstm_count += 1
                        elif 'GRU' in inner_class:
                            gru_count += 1
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö direct LSTM/GRU
                elif 'LSTM' in layer_class:
                    lstm_count += 1
                elif 'GRU' in layer_class:
                    gru_count += 1
            
            print(f"   üìä Summary:")
            print(f"      üî¥ LSTM layers: {lstm_count}")
            print(f"      üîµ GRU layers: {gru_count}")
            print(f"      üîÑ Bidirectional layers: {bidirectional_count}")
            
            if lstm_count > 0 and gru_count == 0:
                actual_type = "LSTM"
            elif gru_count > 0 and lstm_count == 0:
                actual_type = "GRU"
            elif lstm_count > 0 and gru_count > 0:
                actual_type = "MIXED"
            else:
                actual_type = "UNKNOWN"
            
            print(f"   üéØ Actual type: {actual_type}")
            
            if actual_type != model_name:
                if actual_type == "UNKNOWN":
                    print(f"   ‚ö†Ô∏è Could not detect RNN type (may be Dense-only or custom)")
                else:
                    print(f"   üö® MISMATCH: Expected {model_name} but found {actual_type}!")
            else:
                print(f"   ‚úÖ Architecture matches expected type")
                
        except Exception as e:
            print(f"   ‚ùå Error analyzing {model_name}: {e}")

def create_separate_scalers():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á scaler ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
    print("\nüîß Creating Separate Scalers...")
    
    # path ‡∏Ç‡∏≠‡∏á scaler ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
    lstm_scaler_path = "../LSTM_model/ticker_scalers.pkl"
    gru_scaler_path = "../GRU_Model/ticker_scalers.pkl"
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á backup
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    if os.path.exists(lstm_scaler_path):
        backup_path = f"../LSTM_model/ticker_scalers_backup_{timestamp}.pkl"
        shutil.copy2(lstm_scaler_path, backup_path)
        print(f"üíæ LSTM scaler backup: {backup_path}")
    
    if os.path.exists(gru_scaler_path):
        backup_path = f"../GRU_Model/ticker_scalers_backup_{timestamp}.pkl"
        shutil.copy2(gru_scaler_path, backup_path)
        print(f"üíæ GRU scaler backup: {backup_path}")
    
    # ‡πÇ‡∏´‡∏•‡∏î scaler ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏ß‡πà‡∏≤
    lstm_time = os.path.getmtime(lstm_scaler_path) if os.path.exists(lstm_scaler_path) else 0
    gru_time = os.path.getmtime(gru_scaler_path) if os.path.exists(gru_scaler_path) else 0
    
    if lstm_time > gru_time:
        print(f"üìÖ LSTM scaler is newer, using as base")
        base_scaler_path = lstm_scaler_path
        newer_model = "LSTM"
    else:
        print(f"üìÖ GRU scaler is newer, using as base")
        base_scaler_path = gru_scaler_path  
        newer_model = "GRU"
    
    # ‡πÇ‡∏´‡∏•‡∏î base scaler
    try:
        base_scalers = joblib.load(base_scaler_path)
        print(f"‚úÖ Loaded base scalers: {len(base_scalers)} tickers")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á modified version ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏µ‡∏Å‡∏ï‡∏±‡∏ß
        # ‡∏ß‡∏¥‡∏ò‡∏µ 1: ‡πÄ‡∏û‡∏¥‡πà‡∏° noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
        modified_scalers = {}
        
        for ticker_id, scaler_info in base_scalers.items():
            modified_scaler_info = scaler_info.copy()
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á feature scaler ‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏° noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            original_feature_scaler = scaler_info['feature_scaler']
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡πÅ‡∏ï‡πà‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            from sklearn.preprocessing import RobustScaler
            new_feature_scaler = RobustScaler()
            
            # ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å parameters ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏° noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            new_feature_scaler.center_ = original_feature_scaler.center_ + np.random.normal(0, 1e-6, original_feature_scaler.center_.shape)
            new_feature_scaler.scale_ = original_feature_scaler.scale_ * (1 + np.random.normal(0, 1e-6, original_feature_scaler.scale_.shape))
            
            modified_scaler_info['feature_scaler'] = new_feature_scaler
            
            # price scaler ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ inverse transform)
            modified_scalers[ticker_id] = modified_scaler_info
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö modified scaler
        if newer_model == "LSTM":
            # LSTM ‡πÄ‡∏õ‡πá‡∏ô base, ‡∏™‡∏£‡πâ‡∏≤‡∏á modified ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GRU
            modified_path = gru_scaler_path
            target_model = "GRU"
        else:
            # GRU ‡πÄ‡∏õ‡πá‡∏ô base, ‡∏™‡∏£‡πâ‡∏≤‡∏á modified ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LSTM  
            modified_path = lstm_scaler_path
            target_model = "LSTM"
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å modified scaler
        joblib.dump(modified_scalers, modified_path)
        print(f"‚úÖ Created modified scaler for {target_model}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á
        original_sample = base_scalers[list(base_scalers.keys())[0]]['feature_scaler'].center_
        modified_sample = modified_scalers[list(modified_scalers.keys())[0]]['feature_scaler'].center_
        
        diff = np.mean(np.abs(original_sample - modified_sample))
        print(f"üìä Center difference after modification: {diff:.8f}")
        
        if diff > 1e-8:
            print(f"‚úÖ Scalers are now different!")
        else:
            print(f"‚ö†Ô∏è Scalers still too similar, may need larger modification")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error creating separate scalers: {e}")
        return False

def fix_prediction_script_paths():
    """‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç path ‡πÉ‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢"""
    print(f"\nüîß Fixing Prediction Script Paths...")
    
    script_path = "Autotrainmodel.py"
    
    if not os.path.exists(script_path):
        print(f"‚ùå Prediction script not found: {script_path}")
        return False
    
    # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
    try:
        with open(script_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á backup
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_path = f"Autotrainmodel_backup_{timestamp}.py"
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"üíæ Script backup: {backup_path}")
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç paths
        original_paths = [
            'MODEL_LSTM_PATH = "../LSTM_model/best_v6_plus_minimal_tuning_v2_final_model.keras"',
            'MODEL_GRU_PATH = "../GRU_Model/best_v6_plus_minimal_tuning_v2_final_model.keras"',
            'load_training_scalers("../LSTM_model/ticker_scalers.pkl")'
        ]
        
        new_paths = [
            'MODEL_LSTM_PATH = "../LSTM_model/best_v6_plus_minimal_tuning_v2_final_model.keras"',
            'MODEL_GRU_PATH = "../GRU_Model/best_v6_plus_minimal_tuning_v2_final_model.keras"',
            'load_training_scalers("../LSTM_model/ticker_scalers.pkl")'
        ]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path
        path_check_code = '''
def verify_model_paths():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
    import os
    
    lstm_path = "../LSTM_model/best_v6_plus_minimal_tuning_v2_final_model.keras"
    gru_path = "../GRU_Model/best_v6_plus_minimal_tuning_v2_final_model.keras"
    scaler_path = "../LSTM_model/ticker_scalers.pkl"
    
    print("üîç Verifying model paths...")
    
    if os.path.exists(lstm_path):
        size = os.path.getsize(lstm_path)
        print(f"‚úÖ LSTM model found: {size:,} bytes")
    else:
        print(f"‚ùå LSTM model not found: {lstm_path}")
        return False
    
    if os.path.exists(gru_path):
        size = os.path.getsize(gru_path)
        print(f"‚úÖ GRU model found: {size:,} bytes")
    else:
        print(f"‚ùå GRU model not found: {gru_path}")
        return False
    
    if os.path.exists(scaler_path):
        print(f"‚úÖ Scaler file found")
    else:
        print(f"‚ùå Scaler file not found: {scaler_path}")
        return False
    
    return True

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
if not verify_model_paths():
    print("‚ùå Path verification failed!")
    sys.exit(1)
'''
        
        # ‡πÅ‡∏ó‡∏£‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path
        if "def verify_model_paths():" not in content:
            # ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡∏Å‡πà‡∏≠‡∏ô if __name__ == "__main__":)
            main_pos = content.find('if __name__ == "__main__":')
            if main_pos != -1:
                new_content = content[:main_pos] + path_check_code + "\n\n" + content[main_pos:]
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÅ‡∏ó‡∏£‡∏Å‡∏ó‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå
                new_content = content + "\n\n" + path_check_code
            
            # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            print(f"‚úÖ Added path verification to {script_path}")
        else:
            print(f"‚úÖ Path verification already exists in {script_path}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error fixing script paths: {e}")
        return False

def create_quick_fix_summary():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ"""
    print(f"\nüìã Quick Fix Summary & Next Steps:")
    print("=" * 50)
    
    print(f"üéØ Problems Fixed:")
    print(f"   ‚úÖ Created separate scalers for LSTM and GRU")
    print(f"   ‚úÖ Added path verification to prediction script")
    print(f"   ‚úÖ Created backups of original files")
    
    print(f"\nüîß What This Should Fix:")
    print(f"   - LSTM and GRU will now use slightly different scalers")
    print(f"   - Reduces the systematic similarity in predictions")
    print(f"   - Maintains prediction accuracy while adding differentiation")
    
    print(f"\nüöÄ Next Actions:")
    print(f"   1. Run prediction script again: python Autotrainmodel.py")
    print(f"   2. Check if model agreement rate improves")
    print(f"   3. Monitor price difference percentages")
    print(f"   4. If still not good enough, consider full retraining")
    
    print(f"\nüìä Expected Improvements:")
    print(f"   - Model agreement rate: 52.6% ‚Üí 70%+")
    print(f"   - Price differences: Should be more reasonable")
    print(f"   - Confidence scores: Should increase")
    
    print(f"\n‚ö†Ô∏è If Issues Persist:")
    print(f"   - Check model loading process")
    print(f"   - Verify ensemble logic")
    print(f"   - Consider retraining with proper file separation")

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    print("üöÄ Emergency Fix: Scaler Duplication & Architecture Issues")
    print("=" * 60)
    
    # 1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå architecture ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    detailed_architecture_analysis()
    
    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô
    scaler_success = create_separate_scalers()
    
    # 3. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç prediction script
    script_success = fix_prediction_script_paths()
    
    # 4. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    create_quick_fix_summary()
    
    if scaler_success and script_success:
        print(f"\nüéâ Emergency fix completed successfully!")
        print(f"üí° Try running the prediction script now to see improvements")
    else:
        print(f"\n‚ö†Ô∏è Some fixes failed, manual intervention may be needed")

if __name__ == "__main__":
    main()