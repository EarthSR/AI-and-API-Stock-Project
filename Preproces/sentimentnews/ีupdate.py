import pandas as pd
from sqlalchemy import create_engine, text
import mysql.connector
from concurrent.futures import ThreadPoolExecutor
import threading
import time

DB_CONFIG = {
    "host": "localhost",
    "user": "root", 
    "password": "1234",
    "database": "TradeMine",
    "autocommit": False,
    "connect_timeout": 60,
    "read_timeout": 600,
    "write_timeout": 600,
    "pool_size": 10,  # ‡πÄ‡∏û‡∏¥‡πà‡∏° pool size
    "pool_reset_session": False
}

def get_db_connection():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà"""
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        return conn
    except mysql.connector.Error as e:
        print(f"‚ùå ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return None

def optimize_mysql_settings(cursor):
    """‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ MySQL ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß"""
    optimize_queries = [
        "SET SESSION innodb_lock_wait_timeout = 300",
        "SET SESSION lock_wait_timeout = 300", 
        "SET SESSION sql_mode = ''",
        "SET SESSION autocommit = 0",
        "SET SESSION innodb_buffer_pool_size = 2147483648",  # 2GB
        "SET SESSION bulk_insert_buffer_size = 268435456",   # 256MB
        "SET SESSION innodb_flush_log_at_trx_commit = 2",    # ‡∏•‡∏î‡∏Å‡∏≤‡∏£ sync
        "SET SESSION sync_binlog = 0",                       # ‡∏õ‡∏¥‡∏î binlog sync
        "SET SESSION foreign_key_checks = 0",               # ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö foreign key
        "SET SESSION unique_checks = 0"                     # ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö unique ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    ]
    
    for query in optimize_queries:
        try:
            cursor.execute(query)
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤: {query} - {e}")

def merge_backup_optimized():
    """‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß"""
    start_time = time.time()
    
    conn = get_db_connection()
    if not conn:
        return
    
    cursor = conn.cursor()
    
    try:
        print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ MySQL
        optimize_mysql_settings(cursor)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        cursor.execute("SELECT COUNT(*) FROM News")
        current_count = cursor.fetchone()[0]
        print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {current_count:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏≠‡πà‡∏≤‡∏ô backup file ‡πÅ‡∏ö‡∏ö chunked ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
        print("üìÅ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô backup file...")
        backup_df = pd.read_csv('news_202507172300.csv', low_memory=False)
        backup_df = backup_df.fillna('')
        print(f"üìÅ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• backup: {len(backup_df):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏î‡∏∂‡∏á existing URLs ‡πÅ‡∏ö‡∏ö efficient
        print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß...")
        cursor.execute("SELECT URL FROM News WHERE URL IS NOT NULL AND URL != ''")
        existing_urls = set(row[0] for row in cursor.fetchall())
        print(f"üîç URL ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß: {len(existing_urls):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        new_data = backup_df[
            (~backup_df['URL'].isin(existing_urls)) & 
            (backup_df['URL'] != '') & 
            (backup_df['URL'].notna())
        ]
        print(f"üÜï ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏ó‡∏£‡∏Å: {len(new_data):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß ===
        if len(new_data) > 0:
            insert_new_data_fast(cursor, conn, new_data)
        
        # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß ===
        update_existing_data_fast(cursor, conn, backup_df)
        
        # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
        cursor.execute("SET SESSION foreign_key_checks = 1")
        cursor.execute("SET SESSION unique_checks = 1")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        cursor.execute("SELECT COUNT(*) FROM News")
        final_count = cursor.fetchone()[0]
        
        elapsed_time = time.time() - start_time
        added_count = final_count - current_count
        
        print("\n" + "=" * 60)
        print("üéâ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£")
        print("=" * 60)
        print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: {current_count:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï: {final_count:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        print(f"üÜï ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô: +{added_count:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        print(f"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {elapsed_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        print("=" * 60)
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        conn.rollback()
        import traceback
        traceback.print_exc()
        
    finally:
        cursor.close()
        conn.close()
        print("‚úÖ ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

def insert_new_data_fast(cursor, conn, new_data):
    """‡πÅ‡∏ó‡∏£‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß"""
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏ó‡∏£‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß...")
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö bulk insert
    data_tuples = []
    for _, row in new_data.iterrows():
        title = str(row['Title']) if pd.notna(row['Title']) else ''
        url = str(row['URL']) if pd.notna(row['URL']) else ''
        content = str(row['Content']) if pd.notna(row['Content']) else ''
        published_date = str(row['PublishedDate']) if pd.notna(row['PublishedDate']) else None
        img = str(row['Img']) if pd.notna(row['Img']) else ''
        source = str(row['Source']) if pd.notna(row['Source']) else ''
        sentiment = str(row['Sentiment']) if pd.notna(row['Sentiment']) else ''
        confidence_score = float(row['ConfidenceScore']) if pd.notna(row['ConfidenceScore']) and row['ConfidenceScore'] != '' else None
        
        if url and url != 'nan':
            data_tuples.append((title, url, content, published_date, img, source, sentiment, confidence_score))
    
    if len(data_tuples) == 0:
        print("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏ó‡∏£‡∏Å")
        return
    
    # ‡πÉ‡∏ä‡πâ LOAD DATA LOCAL INFILE ‡∏´‡∏£‡∏∑‡∏≠ bulk insert
    insert_query = """
    INSERT IGNORE INTO News (Title, URL, Content, PublishedDate, Img, Source, Sentiment, ConfidenceScore)
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
    """
    
    # ‡πÅ‡∏ó‡∏£‡∏Å‡πÅ‡∏ö‡∏ö batch ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
    batch_size = 2000  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î batch
    total_inserted = 0
    
    for i in range(0, len(data_tuples), batch_size):
        try:
            batch = data_tuples[i:i + batch_size]
            cursor.executemany(insert_query, batch)
            conn.commit()
            total_inserted += len(batch)
            
            percentage = (total_inserted / len(data_tuples)) * 100
            print(f"üìù INSERT: {total_inserted:,}/{len(data_tuples):,} ({percentage:.1f}%) ‚úÖ")
            
        except Exception as e:
            print(f"‚ùå INSERT batch error: {str(e)[:50]}")
            conn.rollback()
            continue
    
    print(f"‚úÖ ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {total_inserted:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

def update_existing_data_fast(cursor, conn, backup_df):
    """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß"""
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß...")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á temporary table ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö bulk update
    cursor.execute("""
    CREATE TEMPORARY TABLE temp_updates (
        url VARCHAR(500) PRIMARY KEY,
        source VARCHAR(255),
        sentiment VARCHAR(50),
        confidence_score DECIMAL(5,4)
    )
    """)
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö temp table
    update_data = backup_df[
        (backup_df['URL'] != '') & 
        (backup_df['URL'].notna()) &
        (backup_df['Source'].notna() | backup_df['Sentiment'].notna() | backup_df['ConfidenceScore'].notna())
    ].copy()
    
    if len(update_data) == 0:
        print("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï")
        return
    
    # ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á temp table
    temp_insert_query = """
    INSERT IGNORE INTO temp_updates (url, source, sentiment, confidence_score)
    VALUES (%s, %s, %s, %s)
    """
    
    temp_tuples = []
    for _, row in update_data.iterrows():
        url = str(row['URL']) if pd.notna(row['URL']) else ''
        source = str(row['Source']) if pd.notna(row['Source']) else ''
        sentiment = str(row['Sentiment']) if pd.notna(row['Sentiment']) else ''
        confidence_score = float(row['ConfidenceScore']) if pd.notna(row['ConfidenceScore']) and row['ConfidenceScore'] != '' else None
        
        if url and url != 'nan':
            temp_tuples.append((url, source, sentiment, confidence_score))
    
    print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï: {len(temp_tuples):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    
    # ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á temp table ‡πÅ‡∏ö‡∏ö batch
    batch_size = 5000
    for i in range(0, len(temp_tuples), batch_size):
        batch = temp_tuples[i:i + batch_size]
        cursor.executemany(temp_insert_query, batch)
        conn.commit()
        
        percentage = ((i + len(batch)) / len(temp_tuples)) * 100
        print(f"üìù TEMP INSERT: {i + len(batch):,}/{len(temp_tuples):,} ({percentage:.1f}%)")
    
    # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å temp table ‡πÅ‡∏ö‡∏ö bulk
    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å temp table...")
    
    bulk_update_query = """
    UPDATE News n
    INNER JOIN temp_updates t ON n.URL = t.url
    SET 
        n.Source = CASE WHEN t.source != '' THEN t.source ELSE n.Source END,
        n.Sentiment = CASE WHEN t.sentiment != '' THEN t.sentiment ELSE n.Sentiment END,
        n.ConfidenceScore = CASE WHEN t.confidence_score IS NOT NULL THEN t.confidence_score ELSE n.ConfidenceScore END
    WHERE 
        (n.Source IS NULL OR n.Source = '' OR 
         n.Sentiment IS NULL OR n.Sentiment = '' OR 
         n.ConfidenceScore IS NULL)
    """
    
    cursor.execute(bulk_update_query)
    updated_rows = cursor.rowcount
    conn.commit()
    
    # ‡∏•‡∏ö temp table
    cursor.execute("DROP TEMPORARY TABLE temp_updates")
    
    print(f"‚úÖ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {updated_rows:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

def merge_backup_sqlalchemy_fast():
    """‡πÉ‡∏ä‡πâ SQLAlchemy ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß"""
    print("üöÄ ‡πÉ‡∏ä‡πâ SQLAlchemy ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß...")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á engine ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    engine = create_engine(
        'mysql+mysqlconnector://root:1234@localhost/TradeMine',
        pool_size=20,
        max_overflow=0,
        pool_pre_ping=True,
        pool_recycle=3600,
        echo=False,
        connect_args={
            "autocommit": False,
            "connect_timeout": 60,
            "read_timeout": 600,
            "write_timeout": 600
        }
    )
    
    try:
        with engine.connect() as conn:
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
            conn.execute(text("SET SESSION innodb_flush_log_at_trx_commit = 2"))
            conn.execute(text("SET SESSION sync_binlog = 0"))
            conn.execute(text("SET SESSION foreign_key_checks = 0"))
            conn.execute(text("SET SESSION unique_checks = 0"))
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            result = conn.execute(text("SELECT COUNT(*) FROM News"))
            current_count = result.scalar()
            print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {current_count:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            
            # ‡∏≠‡πà‡∏≤‡∏ô backup
            backup_df = pd.read_csv('news_202507172300.csv', low_memory=False)
            backup_df = backup_df.fillna('')
            print(f"üìÅ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• backup: {len(backup_df):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            
            # ‡∏î‡∏∂‡∏á existing URLs
            existing_urls_result = conn.execute(text("SELECT URL FROM News WHERE URL IS NOT NULL AND URL != ''"))
            existing_urls = set(row[0] for row in existing_urls_result)
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
            new_data = backup_df[
                (~backup_df['URL'].isin(existing_urls)) & 
                (backup_df['URL'] != '') & 
                (backup_df['URL'].notna())
            ]
            
            if len(new_data) > 0:
                print(f"üÜï ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà: {len(new_data):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                # ‡πÉ‡∏ä‡πâ to_sql ‡πÅ‡∏ö‡∏ö chunk
                new_data.to_sql('News', engine, if_exists='append', index=False, chunksize=2000, method='multi')
                print("‚úÖ ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            
            # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            update_data = backup_df[
                (backup_df['URL'].isin(existing_urls)) &
                (backup_df['URL'] != '') & 
                (backup_df['URL'].notna())
            ]
            
            if len(update_data) > 0:
                print(f"üîÑ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(update_data):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ raw SQL
                for idx, row in update_data.iterrows():
                    if idx % 1000 == 0:
                        print(f"üîÑ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß: {idx:,}/{len(update_data):,}")
                    
                    conn.execute(text("""
                    UPDATE News 
                    SET Source = :source, Sentiment = :sentiment, ConfidenceScore = :confidence
                    WHERE URL = :url AND (Source IS NULL OR Source = '' OR 
                                         Sentiment IS NULL OR Sentiment = '' OR 
                                         ConfidenceScore IS NULL)
                    """), {
                        'source': str(row['Source']) if pd.notna(row['Source']) else '',
                        'sentiment': str(row['Sentiment']) if pd.notna(row['Sentiment']) else '',
                        'confidence': float(row['ConfidenceScore']) if pd.notna(row['ConfidenceScore']) and row['ConfidenceScore'] != '' else None,
                        'url': str(row['URL'])
                    })
                
                conn.commit()
                print("‚úÖ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            
            # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
            conn.execute(text("SET SESSION foreign_key_checks = 1"))
            conn.execute(text("SET SESSION unique_checks = 1"))
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            result = conn.execute(text("SELECT COUNT(*) FROM News"))
            final_count = result.scalar()
            print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï: {final_count:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:")
    print("1. MySQL Connector ‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)")
    print("2. SQLAlchemy ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß")
    print("3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß")
    
    choice = input("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (1, 2, ‡∏´‡∏£‡∏∑‡∏≠ 3): ").strip()
    
    if choice == "1":
        merge_backup_optimized()
    elif choice == "2":
        merge_backup_sqlalchemy_fast()
    elif choice == "3":
        print("üèÅ ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß...")
        print("=" * 50)
        
        print("üöÄ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö MySQL Connector ‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á:")
        start = time.time()
        merge_backup_optimized()
        time1 = time.time() - start
        
        print(f"\n‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {time1:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        print("=" * 50)
        
    else:
        print("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1, 2, ‡∏´‡∏£‡∏∑‡∏≠ 3")