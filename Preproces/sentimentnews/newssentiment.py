import time
import pandas as pd
import mysql.connector
import torch
import os
import numpy as np
from tqdm import tqdm
import spacy
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from datasets import Dataset
from urllib.parse import urlparse

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏ò‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
PARTIAL_RESULTS_PATH = 'partial_results.csv'
FINAL_RESULTS_PATH = 'final_sentiment_results.csv'
DAILY_SUMMARY_PATH = 'daily_sentiment_summary.csv'

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
DB_CONFIG = {
    "host": "localhost",
    "user": "root", 
    "password": "1234",
    "database": "TradeMine",
    "autocommit": True,
    "connect_timeout": 60,
    "read_timeout": 300,
    "write_timeout": 300,
    "pool_size": 5,
    "pool_reset_session": False
}

def get_db_connection():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà"""
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        print("‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        return conn
    except mysql.connector.Error as e:
        print(f"‚ùå ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return None

# ‚úÖ ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó
stock_entities = {
    "ADVANC": ["ADVANC", "AIS"],
    "DIF": ["DIF"],
    "DITTO": ["DITTO"],
    "HUMAN": ["HUMAN"],
    "INET": ["INET"],
    "INSET": ["INSET"],
    "INTUCH": ["INTUCH"],
    "JAS": ["JAS"],
    "JMART": ["JMART"],
    "TRUE": ["TRUE"],
    "AAPL": ["Apple"],
    "AMD": ["AMD", "Advanced Micro Devices"],
    "AMZN": ["Amazon"],
    "AVGO": ["Broadcom"],
    "GOOGL": ["Google", "Alphabet"],
    "META": ["Meta", "Facebook"],
    "MSFT": ["Microsoft"],
    "NVDA": ["Nvidia"],
    "TSLA": ["Tesla"],
    "TSM": ["TSMC", "Taiwan Semiconductor"]
}

# ‚úÖ Context Keyword Mapping
context_mapping = [
    {"Keywords": ["mobile", "5G", "network", "internet", "broadband", "AIS", "cellular", "telecom"], "Stocks": ["ADVANC"]},
    {"Keywords": ["infrastructure fund", "telecom assets", "tower lease", "fiber optic"], "Stocks": ["DIF"]},
    {"Keywords": ["digital", "document", "scanner", "workflow", "e-document", "digital solution"], "Stocks": ["DITTO"]},
    {"Keywords": ["human resource", "HR software", "payroll", "recruitment", "employee management"], "Stocks": ["HUMAN"]},
    {"Keywords": ["cloud", "data center", "IT service", "hosting", "cloud solution", "server"], "Stocks": ["INET"]},
    {"Keywords": ["network service", "fiber optic", "infrastructure", "installation service"], "Stocks": ["INSET"]},
    {"Keywords": ["investment", "holding company", "telecom", "AIS", "INTUCH group"], "Stocks": ["INTUCH"]},
    {"Keywords": ["broadband", "fixed internet", "telecom", "Jasmine", "fiber"], "Stocks": ["JAS"]},
    {"Keywords": ["retail", "mobile store", "finance service", "Jaymart", "consumer loan", "mobile retail"], "Stocks": ["JMART"]},
    {"Keywords": ["mobile", "5G", "network", "broadband", "TRUE ID", "telecom", "True Corporation"], "Stocks": ["TRUE"]},
    {"Keywords": ["semiconductor", "chip", "foundry", "wafer", "GPU", "CPU", "manufacturing"], "Stocks": ["TSM", "NVDA", "AMD", "AVGO"]},
    {"Keywords": ["cloud", "AWS", "Azure", "Google Cloud", "data center"], "Stocks": ["AMZN", "MSFT", "GOOGL"]},
    {"Keywords": ["EV", "electric vehicle", "self-driving", "battery", "gigafactory"], "Stocks": ["TSLA", "AAPL", "NVDA"]},
    {"Keywords": ["AI", "machine learning", "chatbot", "language model"], "Stocks": ["NVDA", "MSFT", "META", "GOOGL"]}
]

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU
if torch.cuda.is_available():
    print("‚úÖ CUDA Available:", torch.cuda.is_available())
    print("‚úÖ CUDA Version:", torch.version.cuda)
    print("‚úÖ Device Name:", torch.cuda.get_device_name(0))
    device = torch.device("cuda")
else:
    print("‚ö†Ô∏è No GPU detected, using CPU instead.")
    device = torch.device("cpu")

print(f"Using device: {device}")

def load_model():
    """‡πÇ‡∏´‡∏•‡∏î FinBERT model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sentiment analysis"""
    print("Loading FinBERT model...")
    model = AutoModelForSequenceClassification.from_pretrained(
        "yiyanghkust/finbert-tone"
    ).to(device)
    tokenizer = AutoTokenizer.from_pretrained(
        "yiyanghkust/finbert-tone"
    )
    print("‚úÖ Model and tokenizer loaded successfully!")
    return model, tokenizer

def extract_source(link):
    """‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å URL"""
    try:
        domain = urlparse(link).netloc
        if domain.startswith("www."):
            domain = domain[4:]
        if domain.endswith(".com"):
            domain = domain[:-4]
        return domain
    except:
        return ""

def process_news(docs):
    """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"""
    results = []
    for doc in docs:
        matched = set()
        # NER
        for ent in doc.ents:
            if ent.label_ == "ORG" and ent.ent_id_:
                matched.add(ent.ent_id_)
        # Context
        text = doc.text.lower()
        for ctx in context_mapping:
            for keyword in ctx["Keywords"]:
                if keyword.lower() in text:
                    matched.update(ctx["Stocks"])
                    break
        results.append(", ".join(sorted(matched)) if matched else None)
    return results

def prepare_data():
    """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£ match ‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß"""
    try:
        df = pd.read_csv('News_Matched.csv')
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Source ‡πÅ‡∏•‡∏∞ Type
        df['Source'] = df['link'].apply(extract_source)
        df['Type'] = 'Database News'
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£ match ‡∏´‡∏∏‡πâ‡∏ô
        matched_df = df[df['MatchedStock'].notna() & (df['MatchedStock'] != '')]
        
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(matched_df)} ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô")
        return matched_df
    except FileNotFoundError:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå News_Matched.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ match ‡∏´‡∏∏‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô")
        return pd.DataFrame()

def convert_nan_to_none(data_list):
    """‡πÅ‡∏õ‡∏•‡∏á NaN values ‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MySQL"""
    return [[None if (isinstance(x, float) and np.isnan(x)) else x for x in row] for row in data_list]

def update_to_database(daily_sentiment):
    """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• StockDetail - ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß"""
    conn = None
    cursor = None
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà
        conn = get_db_connection()
        if not conn:
            raise Exception("Cannot connect to database")
        
        cursor = conn.cursor()
        
        # SQL query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
        update_query = """
        UPDATE StockDetail
        SET
            positive_news = %s,
            negative_news = %s,
            neutral_news = %s,
            Sentiment = %s
        WHERE StockSymbol = %s AND date = %s
        """
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
        data_to_update = []
        for _, row in daily_sentiment.iterrows():
            data_to_update.append((
                int(row['positive_news']),
                int(row['negative_news']),
                int(row['neutral_news']),
                float(row['Sentiment']),
                row['Stock'],
                row['date']
            ))
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏õ‡πá‡∏ô batch ‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô timeout
        batch_size = 100  # ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î batch
        total_rows = len(data_to_update)
        updated_count = 0
        
        for i in tqdm(range(0, total_rows, batch_size), desc="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", unit="batch"):
            batch = data_to_update[i:i+batch_size]
            
            try:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥ batch
                if not conn.is_connected():
                    print("‚ö†Ô∏è ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Ç‡∏≤‡∏î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà...")
                    conn = get_db_connection()
                    cursor = conn.cursor()
                
                cursor.executemany(update_query, batch)
                conn.commit()  # commit ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å batch ‡πÄ‡∏™‡∏£‡πá‡∏à
                updated_count += len(batch)
                print(f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• batch {i//batch_size + 1}: {len(batch)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏£‡∏ß‡∏° {updated_count}/{total_rows})")
                
            except mysql.connector.Error as e:
                print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô batch {i//batch_size + 1}: {e}")
                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà
                try:
                    if conn:
                        conn.close()
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    print("‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                except:
                    print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ")
                    raise
        
        print(f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {updated_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
        raise
    finally:
        if cursor:
            cursor.close()
        if conn and conn.is_connected():
            conn.close()
            print("‚úÖ ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

def create_daily_summary():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Daily Sentiment Summary ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"""
    print("üìà ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á Daily Sentiment Summary...")
    
    if not os.path.exists(FINAL_RESULTS_PATH):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {FINAL_RESULTS_PATH}")
        return None
    
    df = pd.read_csv(FINAL_RESULTS_PATH)
    
    # ===== STEP 2: Clean & Prepare =====
    df['MatchedStock'] = df['MatchedStock'].fillna('').str.strip()
    df['Sentiment'] = df['Sentiment'].fillna('Neutral')
    df['Confidence'] = df['Confidence'].fillna(0.0)
    df['date'] = pd.to_datetime(df['date']).dt.date

    # ‡πÅ‡∏¢‡∏Å MatchedStock ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
    df_exploded = df.assign(Stock=df['MatchedStock'].str.split(', ')).explode('Stock')
    df_exploded = df_exploded[df_exploded['Stock'] != '']
    df_exploded['Stock'] = df_exploded['Stock'].str.strip()

    print(f"Total rows after explode: {len(df_exploded)}")

    # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥
    df_exploded = df_exploded.drop_duplicates(subset=['date', 'Stock', 'Sentiment', 'Confidence'])
    print(f"Total rows after deduplication: {len(df_exploded)}")

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
    df_exploded.to_csv('exploded_data.csv', index=False)

    # ===== STEP 3: Group & Summarize =====
    daily_sentiment = df_exploded.groupby(['date', 'Stock']).agg(
        positive_news=('Sentiment', lambda x: (x == 'Positive').sum()),
        negative_news=('Sentiment', lambda x: (x == 'Negative').sum()),
        neutral_news=('Sentiment', lambda x: (x == 'Neutral').sum()),
        avg_confidence=('Confidence', 'mean'),
        total_news=('Sentiment', 'count')
    ).reset_index()

    print(f"Total rows after groupby: {len(daily_sentiment)}")

    # ===== STEP 4: Feature Engineering =====
    daily_sentiment['positive_ratio'] = daily_sentiment['positive_news'] / daily_sentiment['total_news']
    daily_sentiment['negative_ratio'] = daily_sentiment['negative_news'] / daily_sentiment['total_news']

    def calculate_net_sentiment(row):
        threshold = 0.4
        if row['positive_ratio'] > row['negative_ratio'] and row['positive_ratio'] > threshold:
            return 1
        elif row['negative_ratio'] > row['positive_ratio'] and row['negative_ratio'] > threshold:
            return -1
        else:
            return 0

    daily_sentiment['net_sentiment_score'] = daily_sentiment.apply(calculate_net_sentiment, axis=1)
    daily_sentiment['has_news'] = daily_sentiment['total_news'].apply(lambda x: 1 if x > 0 else 0)
    daily_sentiment = daily_sentiment.rename(columns={'net_sentiment_score': 'Sentiment'})
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
    daily_sentiment.to_csv(DAILY_SUMMARY_PATH, index=False)
    print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Daily Summary ‡∏ó‡∏µ‡πà {DAILY_SUMMARY_PATH}")
    
    return daily_sentiment

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πà‡∏≤‡∏ß"""
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå daily_sentiment_summary.csv ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if os.path.exists(DAILY_SUMMARY_PATH):
        print(f"‚úÖ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {DAILY_SUMMARY_PATH} ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")
        choice = input("‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏° sentiment analysis ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡∏¢‡πÑ‡∏´‡∏°? (y/n): ")
        if choice.lower() == 'y':
            try:
                daily_sentiment = pd.read_csv(DAILY_SUMMARY_PATH)
                print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {DAILY_SUMMARY_PATH} ({len(daily_sentiment)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
                print("üíæ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
                update_to_database(daily_sentiment)
                print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
                return
            except Exception as e:
                print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {e}")
                print("‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏´‡∏°‡πà...")
        else:
            print("‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏´‡∏°‡πà...")
    
    # ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ match ‡∏´‡∏∏‡πâ‡∏ô
    conn = get_db_connection()
    if not conn:
        return
    
    cursor = conn.cursor()
    query = """
    SELECT Title AS title, URL AS link, Content AS description, PublishedDate AS date, Img AS image
    FROM News 
    """
    
    try:
        cursor.execute(query)
        news_data = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        news_df = pd.DataFrame(news_data, columns=columns)

        if news_df.empty:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
            return
        
        print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß {len(news_df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö
        news_df.to_csv('News.csv', index=False, encoding='utf-8')
        
        # ‡πÇ‡∏´‡∏•‡∏î spaCy model
        try:
            nlp = spacy.load("en_core_web_trf")
            print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î spaCy Model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        except OSError:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö spaCy model 'en_core_web_trf'")
            print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢: python -m spacy download en_core_web_trf")
            return
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Entity Ruler
        if "entity_ruler" not in nlp.pipe_names:
            ruler = nlp.add_pipe("entity_ruler", before="ner")
            patterns = []
            for stock, keywords in stock_entities.items():
                for keyword in keywords:
                    patterns.append({
                        "label": "ORG",
                        "pattern": keyword,
                        "id": stock
                    })
            ruler.add_patterns(patterns)
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        texts = news_df.apply(lambda row: f"{row.get('title', '')} {row.get('description', '')}", axis=1)
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• NER + Context
        start_time = time.time()
        batch_size = 32
        
        print(f"üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• NER + Context (Batch size = {batch_size})")
        results = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="üîç Processing batches"):
            batch_texts = texts[i:i+batch_size].tolist()
            docs = list(nlp.pipe(batch_texts))
            batch_results = process_news(docs)
            results.extend(batch_results)
        
        news_df["MatchedStock"] = results
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        total_news = len(news_df)
        related_df = news_df[news_df["MatchedStock"].notnull()]
        related_news = len(related_df)
        unrelated_news = total_news - related_news
        percentage = (related_news / total_news) * 100
        
        end_time = time.time()
        elapsed = end_time - start_time
        
        print("\nüìä Stock Matching Summary")
        print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_news}")
        print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {related_news} ({percentage:.2f}%)")
        print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {unrelated_news} ({100 - percentage:.2f}%)")
        print(f"‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {elapsed / 60:.2f} ‡∏ô‡∏≤‡∏ó‡∏µ\n")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£ match
        news_df.to_csv('News_Matched.csv', index=False, encoding='utf-8')
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà match ‡∏´‡∏∏‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if related_news == 0:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment")
            return
        
        # ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: Sentiment Analysis
        print("üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment...")
        
        # ‡πÇ‡∏´‡∏•‡∏î FinBERT model
        model, tokenizer = load_model()
        
        finbert_sentiment = pipeline(
            "sentiment-analysis",
            model=model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1,
            truncation=True,
            max_length=512
        )
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sentiment analysis
        combined = prepare_data()
        
        if combined.empty:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment")
            return
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        financial_news = combined.apply(
            lambda row: f"{row.get('title', '')} {row.get('description', '')}", axis=1
        ).tolist()
        
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏Å‡πà‡∏≤
        if os.path.exists(PARTIAL_RESULTS_PATH):
            os.remove(PARTIAL_RESULTS_PATH)
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• sentiment analysis
        batch_size = 16
        results = []
        total_records = len(financial_news)
        
        print(f"üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment (Batch size = {batch_size})")
        
        with tqdm(total=total_records, desc="Processing Sentiment Analysis") as pbar:
            try:
                for i in range(0, total_records, batch_size):
                    batch_texts = financial_news[i:i+batch_size]
                    chunk_results = finbert_sentiment(batch_texts)
                    
                    # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å batch
                    for idx, result in enumerate(chunk_results):
                        global_idx = i + idx
                        sentiment = result['label']
                        confidence = result['score']
                        
                        results.append({
                            'title': combined.iloc[global_idx]['title'],
                            'description': combined.iloc[global_idx]['description'],
                            'date': combined.iloc[global_idx]['date'],
                            'link': combined.iloc[global_idx]['link'],
                            'Source': combined.iloc[global_idx]['Source'],
                            'MatchedStock': combined.iloc[global_idx]['MatchedStock'],
                            'Type': combined.iloc[global_idx]['Type'],
                            'Sentiment': sentiment,
                            'Confidence': confidence,
                            'image': combined.iloc[global_idx]['image']
                        })
                    
                    pbar.update(len(batch_texts))
                    
                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏∏‡∏Å 100 records
                    if len(results) >= 100:
                        temp_df = pd.DataFrame(results)
                        temp_df.to_csv(PARTIAL_RESULTS_PATH, mode='a', index=False, 
                                     header=not os.path.exists(PARTIAL_RESULTS_PATH))
                        results = []
                        
            except Exception as e:
                print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
            finally:
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
                if results:
                    temp_df = pd.DataFrame(results)
                    temp_df.to_csv(PARTIAL_RESULTS_PATH, mode='a', index=False, 
                                 header=not os.path.exists(PARTIAL_RESULTS_PATH))
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        if os.path.exists(PARTIAL_RESULTS_PATH):
            final_results = pd.read_csv(PARTIAL_RESULTS_PATH)
            final_results.to_csv(FINAL_RESULTS_PATH, index=False)
            
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• sentiment
            sentiment_summary = final_results['Sentiment'].value_counts()
            print(f"\nüìä Sentiment Analysis Summary")
            print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {len(final_results)}")
            for sentiment, count in sentiment_summary.items():
                percentage = (count / len(final_results)) * 100
                print(f"‚úÖ {sentiment}: {count} ({percentage:.2f}%)")
            
            print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà {FINAL_RESULTS_PATH}")
            
            # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
            if os.path.exists(PARTIAL_RESULTS_PATH):
                os.remove(PARTIAL_RESULTS_PATH)
                
            # ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Daily Summary ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            daily_sentiment = create_daily_summary()
            
            if daily_sentiment is not None:
                print("üíæ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
                update_to_database(daily_sentiment)
                
        else:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
        
        print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
        
    except mysql.connector.Error as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á SQL: {e}")
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ: {e}")
    finally:
        if cursor:
            cursor.close()
        if conn and conn.is_connected():
            conn.close()
            print("‚úÖ ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

if __name__ == "__main__":
    main()