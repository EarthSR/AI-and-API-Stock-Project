import spacy
from spacy.tokens import Span
from tqdm import tqdm
import pandas as pd
import time
import os
import io
import sys

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

if not spacy.prefer_gpu():
    print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö GPU ‡∏´‡∏£‡∏∑‡∏≠ spaCy ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GPU")
else:
    print("üöÄ ‡πÉ‡∏ä‡πâ GPU:", spacy.prefer_gpu())

# ‚úÖ ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó
stock_entities = {
    "AAPL": ["Apple"],
    "AMD": ["AMD", "Advanced Micro Devices"],
    "AMZN": ["Amazon"],
    "AVGO": ["Broadcom"],
    "GOOGL": ["Google", "Alphabet"],
    "META": ["Meta", "Facebook"],
    "MSFT": ["Microsoft"],
    "NVDA": ["Nvidia"],
    "TSLA": ["Tesla"],
    "TSM": ["TSMC", "Taiwan Semiconductor"]
}

# ‚úÖ Context Keyword Mapping
context_mapping = [
    {"Keywords": ["semiconductor", "chip", "foundry", "wafer", "GPU", "CPU", "manufacturing"], "Stocks": ["TSM", "NVDA", "AMD", "AVGO"]},
    {"Keywords": ["cloud", "AWS", "Azure", "Google Cloud", "data center"], "Stocks": ["AMZN", "MSFT", "GOOGL"]},
    {"Keywords": ["EV", "electric vehicle", "self-driving", "battery", "gigafactory"], "Stocks": ["TSLA", "AAPL", "NVDA"]},
    {"Keywords": ["AI", "machine learning", "chatbot", "language model"], "Stocks": ["NVDA", "MSFT", "META", "GOOGL"]}
]

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î spaCy Transformer Model
nlp = spacy.load("en_core_web_trf")
print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î Model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

# ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° Entity Ruler
ruler = nlp.add_pipe("entity_ruler", before="ner")
patterns = []
for stock, keywords in stock_entities.items():
    for keyword in keywords:
        patterns.append({
            "label": "ORG",
            "pattern": keyword,
            "id": stock
        })
ruler.add_patterns(patterns)

# ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß
floder_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'usa', 'News')
file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'usa', 'News', 'USA_News.csv')
df = pd.read_csv(file_path)

# ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
texts = df.apply(lambda row: f"{row.get('title', '')} {row.get('description', '')}", axis=1)

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
def process_news(docs):
    results = []
    for doc in docs:
        matched = set()
        # NER
        for ent in doc.ents:
            if ent.label_ == "ORG" and ent.ent_id_:
                matched.add(ent.ent_id_)
        # Context
        text = doc.text.lower()
        for ctx in context_mapping:
            for keyword in ctx["Keywords"]:
                if keyword.lower() in text:
                    matched.update(ctx["Stocks"])
                    break
        results.append(", ".join(sorted(matched)) if matched else None)
    return results

# ‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
start_time = time.time()
batch_size = 32

print(f"üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• NER + Context (Batch size = {batch_size})")
results = []
for doc_batch in tqdm(nlp.pipe(texts, batch_size=batch_size), total=len(texts), desc="üîç Hybrid Processing"):
    results.extend(process_news([doc_batch]))

df["MatchedStock"] = results

# ‚úÖ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
total_news = len(df)
related_df = df[df["MatchedStock"].notnull()]
unrelated_df = df[df["MatchedStock"].isnull()]
related_news = len(related_df)
unrelated_news = len(unrelated_df)
percentage = (related_news / total_news) * 100

end_time = time.time()
elapsed = end_time - start_time

print("\nüìä Hybrid Model Summary")
print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_news}")
print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {related_news} ({percentage:.2f}%)")
print(f"‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {unrelated_news} ({100 - percentage:.2f}%)")
print(f"‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {elapsed / 60:.2f} ‡∏ô‡∏≤‡∏ó‡∏µ\n")

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
related_path = os.path.join(floder_path, 'Related_News_Hybrid.csv')
unrelated_path = os.path.join(floder_path, 'Unrelated_News_Hybrid.csv')
related_df.to_csv(related_path, index=False, encoding='utf-8')
unrelated_df.to_csv(unrelated_path, index=False, encoding='utf-8')

print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà: Related_News_Hybrid.csv")
print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà: Unrelated_News_Hybrid.csv")
