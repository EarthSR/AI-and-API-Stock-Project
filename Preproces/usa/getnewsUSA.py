import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import concurrent.futures
import threading
import os
import gc
from datetime import datetime, timedelta
import sys
import mysql.connector
from dotenv import load_dotenv
from pathlib import Path
import time
# ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnicodeEncodeError (‡∏Ç‡πâ‡∏≤‡∏°‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö)
sys.stdout.reconfigure(encoding="utf-8", errors="ignore")


# ‚úÖ ‡πÉ‡∏ä‡πâ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÅ‡∏ó‡∏ô BASE_DIR
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'usa')

# ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå "Investing_Folder" ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
News_FOLDER = os.path.join(path, "News")
os.makedirs(News_FOLDER, exist_ok=True)

# ‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå CSV
output_filename = os.path.join(News_FOLDER, "USA_News.csv")

print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà: {output_filename}")

# ‚úÖ URL ‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß
base_url = "https://www.investing.com/news/stock-market-news"

# ‚úÖ Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Chrome instance
driver_lock = threading.Lock()


# ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á __del__ ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î WinError 6
def patched_del(self):
    pass

uc.Chrome.__del__ = patched_del

uc.Chrome.__del__ = patched_del
dotenv_path = Path(__file__).resolve().parents[1] / "config.env"
load_dotenv(dotenv_path)

db_config = {
    "host": os.getenv("DB_HOST"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "database": os.getenv("DB_NAME"),
    "port": os.getenv("DB_PORT")
}


conn = mysql.connector.connect(**db_config)
cursor = conn.cursor()


from datetime import datetime

def get_latest_news_date_from_database():
    cursor.execute("SELECT MAX(PublishedDate) FROM News WHERE Source = 'investing'")
    latest_date = cursor.fetchone()[0]
    if latest_date:
        # ‡∏ñ‡πâ‡∏≤ latest_date ‡πÄ‡∏õ‡πá‡∏ô datetime ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ .date() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime.date
        if isinstance(latest_date, datetime):
            latest_date = latest_date.date()
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà datetime (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô string) ‡∏Ñ‡πà‡∏≠‡∏¢‡πÅ‡∏õ‡∏•‡∏á‡∏î‡πâ‡∏ß‡∏¢ strptime
            latest_date = datetime.strptime(latest_date, "%Y-%m-%d %H:%M:%S").date()

        print(f"üóìÔ∏è ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {latest_date}")
        return latest_date
    else:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 7 ‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô")
        return (datetime.now() - timedelta(days=7)).date()  # ‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 7 ‡∏ß‡∏±‡∏ô

# ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö news_date ‡∏Å‡∏±‡∏ö latest_date
news_date = datetime.now().date()  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
latest_date = get_latest_news_date_from_database()  # ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ latest_date ‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

if news_date and news_date <= latest_date:
    print("‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ")
else:
    print("‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡∏°‡πà‡∏°‡∏≤‡∏Å‡∏û‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")




def init_driver():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Chrome driver instance ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"""
    with driver_lock:
        options = uc.ChromeOptions()
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-extensions")
        driver = uc.Chrome(options=options)
        driver._keep_alive = False
    return driver

def close_popup(driver):
    """‡∏õ‡∏¥‡∏î popup ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ"""
    try:
        close_button = WebDriverWait(driver, 3).until(
            EC.element_to_be_clickable((By.XPATH, "//svg[@data-test='sign-up-dialog-close-button']"))
        )
        close_button.click()
        print("‚úÖ Popup ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    except Exception:
        pass

def smooth_scroll_to_bottom(driver, step=300, delay=0.3):
    """‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡πâ‡∏≤‡πÜ ‡∏à‡∏ô‡∏™‡∏∏‡∏î‡∏´‡∏ô‡πâ‡∏≤"""
    last_height = driver.execute_script("return document.body.scrollHeight")
    current_position = 0

    while current_position < last_height:
        driver.execute_script(f"window.scrollTo(0, {current_position});")
        time.sleep(delay)
        current_position += step
        last_height = driver.execute_script("return document.body.scrollHeight")
    
    # ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏ä‡πâ‡∏≤
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(1.5)


def scrape_news(driver):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"""
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    articles = soup.find_all('article', {'data-test': 'article-item'})
    news_list = []
    for article in articles:
        title_tag = article.find('a', {'data-test': 'article-title-link'})
        title = title_tag.get_text(strip=True) if title_tag else 'No Title'
        link = title_tag.get("href", "No Link")
        
        img_tag = article.find('img', {'data-test': 'item-image'})
        img = img_tag.get('src', 'No Image') if img_tag else 'No Image'

        description_tag = article.find('p', {'data-test': 'article-description'})
        description = description_tag.get_text(strip=True) if description_tag else 'No Description'

        date_tag = article.find('time', {'data-test': 'article-publish-date'})
        date_str = date_tag.get("datetime", "No Date")

        news_list.append({'title': title, 'link': link, 'description': description, 'date': date_str, 'image': img})

    return news_list

def safe_quit(driver):
    """‡∏õ‡∏¥‡∏î WebDriver ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"""
    if driver:
        try:
            driver.quit()
            if driver.service:
                driver.service.stop()
            del driver
            gc.collect()
            print("‚úÖ WebDriver ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: WebDriver ‡∏õ‡∏¥‡∏î‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: {e}")

scraped_pages = set()

def scrape_page(page):
    """Scrape ‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"""
    global scraped_pages
    
    if page in scraped_pages:
        print(f"‚ö†Ô∏è ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page} ‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡πâ‡∏≤‡∏°...")
        return []
    driver = None
    try:
        driver = init_driver()
        driver.get(f"{base_url}/{page}" if page > 1 else base_url)
        smooth_scroll_to_bottom(driver)
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "article")))
        close_popup(driver)
        
        
        news = scrape_news(driver)

        print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page} ‡πÑ‡∏î‡πâ {len(news)} ‡∏Ç‡πà‡∏≤‡∏ß")
        scraped_pages.add(page)
        return news

    except Exception as e:
        print(f"‚ùå Error ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page}: {e}")
        return []

    finally:
        if driver:
            safe_quit(driver)
            driver = None  # üîí ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error ‡∏à‡∏≤‡∏Å __del__ ‡∏Ç‡∏≠‡∏á undetected_chromedriver


def save_to_csv(data, filename, write_header=False):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á CSV ‡πÇ‡∏î‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥"""
    if not data:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà ‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV")
        return
    
    df_new = pd.DataFrame(data)

    if os.path.exists(filename):
        df_existing = pd.read_csv(filename)
        if 'link' in df_existing.columns:
            existing_links = set(df_existing['link'].astype(str))
            df_new = df_new[~df_new['link'].astype(str).isin(existing_links)]
        else:
            existing_links = set()

        print(f"üõë ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ {len(existing_links)} ‡∏Ç‡πà‡∏≤‡∏ß, ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")

    if not df_new.empty:
        mode = 'w' if write_header else 'a'
        header = True if write_header else False
        df_new.to_csv(filename, index=False, encoding='utf-8', mode=mode, header=header)
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß {len(df_new)} ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏á CSV (mode={mode})")
    else:
        print("‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")


def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
    latest_date = get_latest_news_date_from_database()
    batch_size = 1
    max_pages = 7499
    all_news = []
    is_first_save = True
    stop_scraping = False
    total_articles = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
        futures = []
        for page in range(1, max_pages + 1):
            if stop_scraping:
                break

            futures.append(executor.submit(scrape_page, page))

            if len(futures) == batch_size or page == max_pages:
                for future in concurrent.futures.as_completed(futures):
                    result = future.result()
                    all_news.extend(result)

                    for item in result:
                        try:
                            news_date = datetime.strptime(item['date'], "%Y-%m-%d %H:%M:%S").date()
                        except (ValueError, TypeError):
                            news_date = None

                        if news_date and news_date <= latest_date:
                            print(f"‚èπÔ∏è ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ({latest_date}), ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡∏ô‡∏ó‡∏µ")
                            save_to_csv(all_news, output_filename, write_header=is_first_save)
                            save_to_csv(all_news, os.path.join("Combined_News.csv"), write_header=False)
                            stop_scraping = True
                            driver = init_driver()
                            safe_quit(driver)
                            driver = None 
                            break

                save_to_csv(all_news, output_filename, write_header=is_first_save)
                save_to_csv(all_news, os.path.join("Combined_News.csv"), write_header=False)
                total_articles += len(all_news)
                is_first_save = False
                all_news = []
                futures = []

if __name__ == "__main__":
    main()
