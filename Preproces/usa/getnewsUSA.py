# -*- coding: utf-8 -*-
import sys, os, time, random, threading, platform, shutil, tarfile, zipfile
from datetime import datetime, timedelta
from pathlib import Path

import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='ignore')

import pandas as pd
import requests
import mysql.connector
from dotenv import load_dotenv

from bs4 import BeautifulSoup
import concurrent.futures

from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# ===================== CONFIG PATHS =====================
ROOT = Path(__file__).resolve().parent
USA_DIR = (ROOT / '..' / 'usa').resolve()
NEWS_DIR = USA_DIR / 'News'
NEWS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_CSV = NEWS_DIR / 'USA_News.csv'

# ===================== ENV / DB =========================
dotenv_path = (ROOT / '..' / 'config.env').resolve()
load_dotenv(dotenv_path)

db_config = {
    "host": os.getenv("DB_HOST"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "database": os.getenv("DB_NAME"),
    "port": os.getenv("DB_PORT"),
    "autocommit": True
}
try:
    conn = mysql.connector.connect(**db_config)
    cursor = conn.cursor()
    print("‚úÖ DB connected")
except mysql.connector.Error as e:
    print(f"‚ùå DB connect failed: {e}")
    sys.exit(1)

# ===================== SCRAPER TARGET ===================
base_url = "https://www.investing.com/news/stock-market-news"

# ===================== UA & LOCK ========================
driver_lock = threading.Lock()
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:127.0) Gecko/20100101 Firefox/127.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:127.0) Gecko/20100101 Firefox/127.0",
    "Mozilla/5.0 (X11; Linux x86_64; rv:127.0) Gecko/20100101 Firefox/127.0"
]

# =======================================================
#           GECKODRIVER RESOLUTION (No API limit)
# =======================================================
def _gecko_direct_url(version="v0.34.0"):
    sysname = platform.system().lower()
    arch = platform.machine().lower()
    if "linux" in sysname:
        fname = f"geckodriver-{version}-linux64.tar.gz"
    elif "windows" in sysname:
        fname = f"geckodriver-{version}-win64.zip"
    elif "darwin" in sysname and arch in ("arm64", "aarch64"):
        fname = f"geckodriver-{version}-macos-aarch64.tar.gz"
    else:
        fname = f"geckodriver-{version}-macos.tar.gz"
    return f"https://github.com/mozilla/geckodriver/releases/download/{version}/{fname}"

def _extract_driver(archive_path, dest_dir):
    os.makedirs(dest_dir, exist_ok=True)
    if archive_path.endswith(".zip"):
        with zipfile.ZipFile(archive_path, "r") as zf:
            zf.extractall(dest_dir)
    else:
        with tarfile.open(archive_path, "r:gz") as tf:
            tf.extractall(dest_dir)
    exe = "geckodriver.exe" if platform.system().lower().startswith("win") else "geckodriver"
    driver_path = os.path.join(dest_dir, exe)
    if not os.path.isfile(driver_path):
        for root, _, files in os.walk(dest_dir):
            if exe in files:
                driver_path = os.path.join(root, exe)
                break
    try:
        if not platform.system().lower().startswith("win"):
            os.chmod(driver_path, 0o755)
    except Exception:
        pass
    return driver_path

def _download_direct(version, target_dir):
    url = _gecko_direct_url(version)
    os.makedirs(target_dir, exist_ok=True)
    tmp_path = os.path.join(target_dir, f"geckodriver-{version}.tmp")
    print(f"‚¨áÔ∏è Downloading geckodriver direct: {url}")
    with requests.get(url, stream=True, timeout=120) as r:
        r.raise_for_status()
        with open(tmp_path, "wb") as f:
            for chunk in r.iter_content(8192):
                if chunk:
                    f.write(chunk)
    driver_path = _extract_driver(tmp_path, target_dir)
    try:
        os.remove(tmp_path)
    except Exception:
        pass
    return driver_path

def resolve_geckodriver_path():
    # 1) Env path
    env_path = os.getenv("GECKODRIVER_PATH")
    if env_path and os.path.isfile(env_path):
        print(f"üîß Using GECKODRIVER_PATH: {env_path}")
        return env_path
    # 2) PATH
    found = shutil.which("geckodriver")
    if found:
        print(f"üîé Found geckodriver in PATH: {found}")
        return found
    # 3) Cache
    cache_dir = os.path.join(Path.home(), ".cache", "geckodriver")
    os.makedirs(cache_dir, exist_ok=True)
    exe = "geckodriver.exe" if platform.system().lower().startswith("win") else "geckodriver"
    cached = os.path.join(cache_dir, exe)
    if os.path.isfile(cached):
        print(f"üíæ Using cached geckodriver: {cached}")
        return cached
    # 4) Direct download fallback
    version = os.getenv("GECKODRIVER_VERSION", "v0.34.0")
    path = _download_direct(version, cache_dir)
    print(f"‚úÖ Installed geckodriver direct: {path}")
    return path

try:
    GECKO_PATH = resolve_geckodriver_path()
except Exception as e:
    print("‚ùå geckodriver setup failed:", e)
    sys.exit(1)

# =======================================================
#                HELPERS: DRIVER & NAVIGATION
# =======================================================
def init_driver():
    """Headless Firefox, ‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î‡∏£‡∏π‡∏õ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡∏£‡∏π‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏î‡πâ) + ‡πÑ‡∏°‡πà‡∏£‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏ô‡πâ‡∏≤"""
    with driver_lock:
        options = FirefoxOptions()
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-extensions")
        options.add_argument("--headless")
        options.add_argument(f"--user-agent={random.choice(USER_AGENTS)}")

        # ‡πÑ‡∏°‡πà‡∏£‡∏≠ page load ‡∏à‡∏ô‡∏à‡∏ö ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô timeout
        options.page_load_strategy = "none"

        service = Service(executable_path=GECKO_PATH)
        try:
            driver = webdriver.Firefox(service=service, options=options)
            driver.set_page_load_timeout(12)   # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ stop ‡πÄ‡∏≠‡∏á
            driver.set_script_timeout(20)
            time.sleep(0.8)
            print(f"Firefox version: {driver.capabilities.get('browserVersion')}")
            return driver
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏° GeckoDriver: {e}")
            return None

def smart_get(driver, url, hard_timeout=12, wait_selector=(By.TAG_NAME, "article"), wait_timeout=20):
    """
    ‡πÄ‡∏Ç‡πâ‡∏≤ URL ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏£‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏ô‡πâ‡∏≤:
    1) ‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏Å‡∏±‡∏ö driver.get()
    2) ‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏±‡πà‡∏á window.stop() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå/‡πÅ‡∏≠‡∏î
    3) ‡∏£‡∏≠ element ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÇ‡∏ú‡∏•‡πà
    """
    try:
        driver.set_page_load_timeout(hard_timeout)
        try:
            driver.get(url)
        except TimeoutException:
            try:
                driver.execute_script("window.stop();")
            except Exception:
                pass

        WebDriverWait(driver, wait_timeout).until(EC.presence_of_element_located(wait_selector))
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è smart_get fail: {e}")
        return False

def smooth_scroll_to_bottom(driver, step=200, delay=0.03, max_time=10):
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        pos, start = 0, time.time()
        while pos < last_height and (time.time() - start) < max_time:
            driver.execute_script(f"window.scrollTo(0, {pos});")
            time.sleep(delay)
            pos += step
            new_h = driver.execute_script("return document.body.scrollHeight")
            if new_h > last_height:
                last_height = new_h
        time.sleep(0.2)
    except Exception as e:
        print(f"‚ö†Ô∏è scroll issue: {e}")

def close_popup(driver):
    try:
        btn = WebDriverWait(driver, 3).until(
            EC.element_to_be_clickable((By.XPATH, "//svg[@data-test='sign-up-dialog-close-button']"))
        )
        btn.click()
        print("‚úÖ Popup closed")
    except Exception:
        pass

# =======================================================
#                   SCRAPERS (USA)
# =======================================================
def get_latest_news_date_from_database():
    try:
        cursor.execute("SELECT MAX(PublishedDate) FROM News WHERE Source = 'investing'")
        latest = cursor.fetchone()[0]
        if latest:
            if isinstance(latest, datetime):
                return latest.date()
            return datetime.strptime(latest, "%Y-%m-%d %H:%M:%S").date()
        else:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏£‡∏¥‡πà‡∏° 7 ‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô")
            return (datetime.now() - timedelta(days=7)).date()
    except mysql.connector.Error as e:
        print(f"‚ùå DB query error: {e}")
        return (datetime.now() - timedelta(days=7)).date()

def scrape_news(driver):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ list (‡∏°‡∏µ‡∏£‡∏π‡∏õ/‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢/‡πÄ‡∏ß‡∏•‡∏≤)"""
    try:
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        articles = soup.find_all('article', {'data-test': 'article-item'})
        out = []
        for a in articles:
            title_tag = a.find('a', {'data-test': 'article-title-link'})
            title = title_tag.get_text(strip=True) if title_tag else 'No Title'
            link = title_tag.get("href", "No Link") if title_tag else "No Link"

            img_tag = a.find('img', {'data-test': 'item-image'})
            img = img_tag.get('src') if img_tag and img_tag.get('src') else 'No Image'

            desc_tag = a.find('p', {'data-test': 'article-description'})
            desc = desc_tag.get_text(strip=True) if desc_tag else 'No Description'

            date_tag = a.find('time', {'data-test': 'article-publish-date'})
            date_str = date_tag.get("datetime", None) if date_tag else None
            if date_str:
                try:
                    datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S")
                except ValueError:
                    date_str = None

            out.append({
                "title": title, "link": link, "description": desc, "date": date_str, "image": img
            })
        return out
    except Exception as e:
        print(f"‚ùå parse error: {e}")
        return []

def safe_quit(driver):
    if driver:
        try:
            driver.quit()
            if getattr(driver, "service", None):
                driver.service.stop()
            del driver
        except Exception:
            pass
        print("‚úÖ WebDriver ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

scraped_pages = set()

def scrape_page(page, max_retries=3, latest_date=None):
    if page in scraped_pages:
        print(f"‚ö†Ô∏è ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page} ‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡πâ‡∏≤‡∏°...")
        return []

    url = f"{base_url}/{page}" if page > 1 else base_url
    driver = None

    for attempt in range(1, max_retries + 1):
        try:
            print(f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤ {page} (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt})")
            driver = init_driver()
            if not driver:
                raise WebDriverException("start WebDriver failed")

            ok = smart_get(driver, url, hard_timeout=12, wait_selector=(By.TAG_NAME, "article"), wait_timeout=20)
            if not ok:
                raise TimeoutException("smart_get timed out")

            smooth_scroll_to_bottom(driver)
            close_popup(driver)

            news = scrape_news(driver)
            if news:
                print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page} ‡πÑ‡∏î‡πâ {len(news)} ‡∏Ç‡πà‡∏≤‡∏ß")
                scraped_pages.add(page)
                time.sleep(2.0)
                return news
            else:
                raise WebDriverException("no articles found")

        except (TimeoutException, WebDriverException) as e:
            # Fallback: ‡∏´‡∏¢‡∏∏‡∏î‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ
            try:
                if driver:
                    driver.execute_script("window.stop();")
                    news = scrape_news(driver)
                    if news:
                        print(f"‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏î‡πâ {len(news)} ‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏°‡πâ‡∏à‡∏∞ timeout")
                        scraped_pages.add(page)
                        time.sleep(1.5)
                        return news
            except Exception:
                pass

            print(f"‚ùå Error ‡∏´‡∏ô‡πâ‡∏≤ {page} (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt}): {e}")
            if attempt == max_retries:
                print(f"‚è≥ ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏•‡∏≠‡∏á {max_retries} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page}")
                return []
            time.sleep(10 * attempt)

        except Exception as e:
            print(f"‚ùå Unexpected error ‡∏´‡∏ô‡πâ‡∏≤ {page} (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt}): {e}")
            return []

        finally:
            if driver:
                safe_quit(driver)

    return []

def save_to_csv(data, filename, write_header=False):
    if not data:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà ‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV")
        return
    df_new = pd.DataFrame(data)
    existing_links = set()
    os.makedirs(os.path.dirname(str(filename)), exist_ok=True)

    if os.path.exists(filename):
        try:
            df_existing = pd.read_csv(filename, encoding='utf-8')
            if 'link' in df_existing.columns:
                existing_links = set(df_existing['link'].astype(str))
                before = len(df_new)
                df_new = df_new[~df_new['link'].astype(str).isin(existing_links)]
                print(f"üõë ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ {len(existing_links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå | ‡∏Ñ‡∏±‡∏î‡∏≠‡∏≠‡∏Å {before - len(df_new)} ‡πÅ‡∏ñ‡∏ß")
        except Exception as e:
            print(f"‚ùå ‡∏≠‡πà‡∏≤‡∏ô CSV ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")
            return
    else:
        print(f"üìÑ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà: {filename}")

    if not df_new.empty:
        mode = 'w' if write_header else 'a'
        header = True if write_header else False
        try:
            df_new.to_csv(filename, index=False, encoding='utf-8', mode=mode, header=header)
            print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß {len(df_new)} ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏á CSV '{filename}' (mode={mode})")
        except Exception as e:
            print(f"‚ùå ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å CSV ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
    else:
        print("‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏±‡∏î‡∏ã‡πâ‡∏≥")

def main():
    latest_date = get_latest_news_date_from_database()
    batch_size = 2          # ‡∏•‡∏î parallel ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏•‡πâ‡∏° / ‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏ô block
    max_pages = 200
    all_news = []
    is_first_save = True
    stop_scraping = False
    total_articles = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
        futures = []
        for page in range(1, max_pages + 1):
            if stop_scraping:
                break
            futures.append(executor.submit(scrape_page, page, 3, latest_date))

            if len(futures) == batch_size or page == max_pages:
                time.sleep(1.0)  # ‡πÄ‡∏ß‡πâ‡∏ô‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏∞‡∏•‡∏î‡πÇ‡∏´‡∏•‡∏î
                for future in concurrent.futures.as_completed(futures):
                    result = future.result()
                    all_news.extend(result)

                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤ ‚Üí ‡∏´‡∏¢‡∏∏‡∏î
                    for item in result:
                        try:
                            if item['date']:
                                nd = datetime.strptime(item['date'], "%Y-%m-%d %H:%M:%S").date()
                            else:
                                nd = None
                        except (ValueError, TypeError):
                            nd = None
                        if nd and nd <= latest_date:
                            print(f"‚èπÔ∏è ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ({latest_date}), ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡∏ô‡∏ó‡∏µ")
                            save_to_csv(all_news, str(OUTPUT_CSV), write_header=is_first_save)
                            stop_scraping = True
                            break

                save_to_csv(all_news, str(OUTPUT_CSV), write_header=is_first_save)
                total_articles += len(all_news)
                print(f"üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ: {len(all_news)} ‡∏Ç‡πà‡∏≤‡∏ß | ‡∏™‡∏∞‡∏™‡∏°: {total_articles}")
                is_first_save = False
                all_news = []
                futures = []

    try:
        cursor.close()
        conn.close()
        print("‚úÖ ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    except mysql.connector.Error as e:
        print(f"‚ùå ‡∏õ‡∏¥‡∏î‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")

if __name__ == "__main__":
    print("Selenium version:", webdriver.__version__ if hasattr(webdriver, "__version__") else "N/A")
    print(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà: {OUTPUT_CSV}")
    main()
