import sys
import selenium
from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.firefox.options import Options as FirefoxOptions  # ‡πÉ‡∏ä‡πâ FirefoxOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
import pandas as pd
import concurrent.futures
import threading
import os
import gc
from datetime import datetime, timedelta
import mysql.connector
from dotenv import load_dotenv
from pathlib import Path
import time
import random
from webdriver_manager.firefox import GeckoDriverManager

# ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnicodeEncodeError
sys.stdout.reconfigure(encoding="utf-8", errors="ignore")

# ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô Selenium
print(f"Selenium version: {selenium.__version__}")

# ‚úÖ ‡πÉ‡∏ä‡πâ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'usa')
News_FOLDER = os.path.join(path, "News")
os.makedirs(News_FOLDER, exist_ok=True)

# ‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå CSV
output_filename = os.path.join(News_FOLDER, "USA_News.csv")
print(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà: {output_filename}")

# ‚úÖ URL ‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß
base_url = "https://www.investing.com/news/stock-market-news"

# ‚úÖ Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Firefox instance
driver_lock = threading.Lock()

# ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ User-Agent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏∏‡πà‡∏°
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:127.0) Gecko/20100101 Firefox/127.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:127.0) Gecko/20100101 Firefox/127.0"
]

# ‡πÇ‡∏´‡∏•‡∏î environment variables
dotenv_path = Path(__file__).resolve().parents[1] / "config.env"
load_dotenv(dotenv_path)

db_config = {
    "host": os.getenv("DB_HOST"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "database": os.getenv("DB_NAME"),
    "port": os.getenv("DB_PORT")
}

try:
    conn = mysql.connector.connect(**db_config)
    cursor = conn.cursor()
except mysql.connector.Error as e:
    print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
    sys.exit(1)

def get_latest_news_date_from_database():
    """‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    try:
        cursor.execute("SELECT MAX(PublishedDate) FROM News WHERE Source = 'investing'")
        latest_date = cursor.fetchone()[0]
        if latest_date:
            if isinstance(latest_date, datetime):
                latest_date = latest_date.date()
            else:
                latest_date = datetime.strptime(latest_date, "%Y-%m-%d %H:%M:%S").date()
            print(f"üóìÔ∏è ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {latest_date}")
            return latest_date
        else:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 7 ‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô")
            return (datetime.now() - timedelta(days=7)).date()
    except mysql.connector.Error as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
        return (datetime.now() - timedelta(days=7)).date()

def init_driver():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Firefox driver instance ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"""
    with driver_lock:
        options = FirefoxOptions()
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-extensions")
        options.add_argument("--headless")  # ‡πÉ‡∏ä‡πâ headless mode ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        options.add_argument(f"--user-agent={random.choice(USER_AGENTS)}")
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ proxy ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÉ‡∏´‡πâ uncomment
        # options.add_argument("--proxy-server=http://your_proxy:port")
        try:
            service = Service(GeckoDriverManager().install())
            driver = webdriver.Firefox(service=service, options=options)
            driver.set_page_load_timeout(30)
            time.sleep(2)
            print(f"Firefox version: {driver.capabilities['browserVersion']}")
            return driver
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏° GeckoDriver: {e}")
            return None

def close_popup(driver):
    """‡∏õ‡∏¥‡∏î popup ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ"""
    try:
        close_button = WebDriverWait(driver, 3).until(
            EC.element_to_be_clickable((By.XPATH, "//svg[@data-test='sign-up-dialog-close-button']"))
        )
        close_button.click()
        print("‚úÖ Popup ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    except Exception:
        pass

def smooth_scroll_to_bottom(driver, step=200, delay=0.03):
    """‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡πâ‡∏≤‡πÜ ‡∏à‡∏ô‡∏™‡∏∏‡∏î‡∏´‡∏ô‡πâ‡∏≤"""
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        current_position = 0
        max_scroll_time = 10
        start_time = time.time()
        while current_position < last_height and (time.time() - start_time) < max_scroll_time:
            driver.execute_script(f"window.scrollTo(0, {current_position});")
            time.sleep(delay)
            current_position += step
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height > last_height:
                last_height = new_height
        time.sleep(0.2)
    except Exception as e:
        print(f"‚ö†Ô∏è ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: {e}")

def scrape_news(driver):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"""
    try:
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        articles = soup.find_all('article', {'data-test': 'article-item'})
        news_list = []
        for article in articles:
            title_tag = article.find('a', {'data-test': 'article-title-link'})
            title = title_tag.get_text(strip=True) if title_tag else 'No Title'
            link = title_tag.get("href", "No Link")
            img_tag = article.find('img', {'data-test': 'item-image'})
            img = img_tag.get('src', 'No Image') if img_tag else 'No Image'
            description_tag = article.find('p', {'data-test': 'article-description'})
            description = description_tag.get_text(strip=True) if description_tag else 'No Description'
            date_tag = article.find('time', {'data-test': 'article-publish-date'})
            date_str = date_tag.get("datetime", None) if date_tag else None
            if date_str:
                try:
                    datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S")
                except ValueError:
                    date_str = None
            news_list.append({'title': title, 'link': link, 'description': description, 'date': date_str, 'image': img})
        return news_list
    except Exception as e:
        print(f"‚ùå ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Ñ‡∏£‡∏õ‡∏Ç‡πà‡∏≤‡∏ß: {e}")
        return []

def safe_quit(driver):
    """‡∏õ‡∏¥‡∏î WebDriver ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"""
    if driver:
        try:
            driver.quit()
            if driver.service:
                driver.service.stop()
            del driver
            gc.collect()
            print("‚úÖ WebDriver ‡∏õ‡∏¥‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: WebDriver ‡∏õ‡∏¥‡∏î‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: {e}")

scraped_pages = set()

def scrape_page(page, max_retries=3):
    """Scrape ‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£ retry"""
    global scraped_pages
    if page in scraped_pages:
        print(f"‚ö†Ô∏è ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page} ‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡πâ‡∏≤‡∏°...")
        return []
    driver = None
    for attempt in range(1, max_retries + 1):
        try:
            print(f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤ {page} (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt})")
            driver = init_driver()
            if not driver:
                raise WebDriverException("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏° WebDriver ‡πÑ‡∏î‡πâ")
            driver.get(f"{base_url}/{page}" if page > 1 else base_url)
            try:
                status_code = driver.execute_script("return window.performance.getEntriesByType('navigation')[0].responseStatus")
                if status_code != 200:
                    print(f"‚ùå ‡∏´‡∏ô‡πâ‡∏≤ {page} ‡∏Ñ‡∏∑‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ HTTP {status_code}")
                    return []
            except Exception:
                print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö HTTP status ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ {page}")
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "article")))
            smooth_scroll_to_bottom(driver)
            close_popup(driver)
            news = scrape_news(driver)
            print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page} ‡πÑ‡∏î‡πâ {len(news)} ‡∏Ç‡πà‡∏≤‡∏ß")
            scraped_pages.add(page)
            time.sleep(5)
            return news
        except (TimeoutException, WebDriverException) as e:
            print(f"‚ùå Error ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page} (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt}): {e}")
            if attempt == max_retries:
                print(f"‚è≥ ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏•‡∏≠‡∏á {max_retries} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page}")
                return []
            time.sleep(15 * attempt)
        except Exception as e:
            print(f"‚ùå Error ‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page} (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt}): {e}")
            return []
        finally:
            if driver:
                safe_quit(driver)
    return []

def save_to_csv(data, filename, write_header=False):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á CSV ‡πÇ‡∏î‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥"""
    if not data:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà ‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV")
        return
    try:
        df_new = pd.DataFrame(data)
        existing_links = set()
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        if os.path.exists(filename):
            try:
                df_existing = pd.read_csv(filename, encoding='utf-8')
                if 'link' in df_existing.columns:
                    existing_links = set(df_existing['link'].astype(str))
                    df_new = df_new[~df_new['link'].astype(str).isin(existing_links)]
                print(f"üõë ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ {len(existing_links)} ‡∏Ç‡πà‡∏≤‡∏ß")
            except Exception as e:
                print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏î‡πâ: {e}")
                return
        else:
            print(f"üìÑ ‡πÑ‡∏ü‡∏•‡πå CSV '{filename}' ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà")
        if not df_new.empty:
            mode = 'w' if write_header else 'a'
            header = True if write_header else False
            try:
                df_new.to_csv(filename, index=False, encoding='utf-8', mode=mode, header=header)
                print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß {len(df_new)} ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏á CSV '{filename}' (mode={mode})")
            except Exception as e:
                print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡πÑ‡∏î‡πâ: {e}")
        else:
            print("‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô save_to_csv: {e}")

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
    latest_date = get_latest_news_date_from_database()
    batch_size = 4
    max_pages = 200  
    all_news = []
    is_first_save = True
    stop_scraping = False
    total_articles = 0
    with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
        futures = []
        for page in range(1, max_pages + 1):
            if stop_scraping:
                break
            futures.append(executor.submit(scrape_page, page))
            if len(futures) == batch_size or page == max_pages:
                for future in concurrent.futures.as_completed(futures):
                    result = future.result()
                    all_news.extend(result)
                    for item in result:
                        try:
                            if item['date']:
                                news_date = datetime.strptime(item['date'], "%Y-%m-%d %H:%M:%S").date()
                            else:
                                news_date = None
                        except (ValueError, TypeError):
                            news_date = None
                        if news_date and news_date <= latest_date:
                            print(f"‚èπÔ∏è ‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ({latest_date}), ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡∏ô‡∏ó‡∏µ")
                            save_to_csv(all_news, output_filename, write_header=is_first_save)              
                            stop_scraping = True
                            break
                save_to_csv(all_news, output_filename, write_header=is_first_save)
                total_articles += len(all_news)
                print(f"üìä ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total_articles} ‡∏Ç‡πà‡∏≤‡∏ß")
                is_first_save = False
                all_news = []
                futures = []
    try:
        cursor.close()
        conn.close()
        print("‚úÖ ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    except mysql.connector.Error as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")

if __name__ == "__main__":
    main()