# -*- coding: utf-8 -*-
import os, sys, io
import pandas as pd
from tqdm import tqdm
import mysql.connector
from dotenv import load_dotenv

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# ====== ENV / DB CONFIG ======
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'config.env')
load_dotenv(path)
db_config = {
    "host": os.getenv("DB_HOST"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "database": os.getenv("DB_NAME"),
    "charset": "utf8mb4"
}
if not all(db_config.values()):
    print("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MySQL")
    sys.exit(1)

# ====== TUNING ======
CONNECT_TIMEOUT = 120
SESSION_WAIT_TIMEOUT = 28800
SESSION_INTERACTIVE_TIMEOUT = 28800
SESSION_NET_READ_TIMEOUT = 120
SESSION_NET_WRITE_TIMEOUT = 120

BATCH_NEWS = 2000        # ‡πÉ‡∏™‡πà‡∏Ç‡πà‡∏≤‡∏ß‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏•‡∏∞‡∏Å‡∏µ‡πà‡πÄ‡∏£‡∏Ñ‡∏Ñ‡∏≠‡∏£‡πå‡∏î
BATCH_STOCK = 5000       # ‡πÉ‡∏™‡πà‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå NewsStock ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏•‡∏∞‡∏Å‡∏µ‡πà‡πÅ‡∏ñ‡∏ß
CSV_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'usa', 'News', 'USA_News_Hybrid.csv')

# ====== HELPERS ======
def safe(v, default=None):
    return v if pd.notna(v) else default

def chunked(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

# ====== LOAD CSV (‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ) ======
usecols = ["title","description","date","Sentiment","Confidence","link","Source","image","MatchedStock"]
df = pd.read_csv(CSV_PATH, usecols=[c for c in usecols if c in pd.read_csv(CSV_PATH, nrows=0).columns])
df = df[df["title"].notna()]
df = df[df["link"].notna()]  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ URL
# map ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå -> ‡∏ï‡∏≤‡∏£‡∏≤‡∏á
df = df.rename(columns={
    "title": "Title",
    "description": "Content",
    "date": "PublishedDate",
    "Sentiment": "Sentiment",
    "Confidence": "ConfidenceScore",
    "link": "URL",
    "Source": "Source",
    "image": "Img"
})
# ‡πÄ‡∏î‡∏î‡∏π‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå (URL ‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏≤‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)
df = df.sort_index().drop_duplicates(subset=["URL"], keep="first")

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° list of dict ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ iterrows
rows = df.to_dict(orient="records")

# ====== CONNECT DB ======
conn = mysql.connector.connect(**db_config, connection_timeout=CONNECT_TIMEOUT)
cur = conn.cursor()

# ‡∏Ç‡∏¢‡∏≤‡∏¢ timeout ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ session ‡∏ô‡∏µ‡πâ
cur.execute(f"SET SESSION wait_timeout = {SESSION_WAIT_TIMEOUT}")
cur.execute(f"SET SESSION interactive_timeout = {SESSION_INTERACTIVE_TIMEOUT}")
cur.execute(f"SET SESSION net_read_timeout = {SESSION_NET_READ_TIMEOUT}")
cur.execute(f"SET SESSION net_write_timeout = {SESSION_NET_WRITE_TIMEOUT}")

# (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πà‡∏á SELECT IN (...) ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ (ignore error ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
for ddl in [
    "CREATE INDEX idx_news_url ON News (URL(255))",
    "CREATE INDEX idx_newsstk ON NewsStock (NewsID, StockSymbol)"
]:
    try:
        cur.execute(ddl)
        conn.commit()
    except Exception:
        conn.rollback()

# ====== SQL ======
SQL_INSERT_NEWS = """
INSERT INTO News (Title, Source, Sentiment, ConfidenceScore, PublishedDate, Content, URL, Img)
VALUES (%s,%s,%s,%s,%s,%s,%s,%s)
"""
# ‡πÉ‡∏ä‡πâ SELECT mapping ‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡πâ‡∏≠‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î per-row query
def select_news_ids_by_urls(urls):
    if not urls:
        return {}
    placeholders = ",".join(["%s"] * len(urls))
    cur.execute(f"SELECT URL, NewsID FROM News WHERE URL IN ({placeholders})", urls)
    return dict(cur.fetchall())  # {URL: NewsID}

SQL_INSERT_NEWS_STOCK = "INSERT INTO NewsStock (NewsID, StockSymbol) VALUES (%s, %s)"

inserted_news = 0
inserted_links = 0

# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÅ‡∏ó‡∏£‡∏Å NewsStock ‡∏ã‡πâ‡∏≥‡πÉ‡∏ô "‡∏£‡∏±‡∏ô‡∏ô‡∏µ‡πâ" (‡πÅ‡∏°‡πâ DB ‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ UNIQUE)
seen_pairs = set()

# ====== MAIN PIPELINE ======
for batch in tqdm(list(chunked(rows, BATCH_NEWS)), desc="üöÄ Upserting News (batch)"):
    urls = [safe(r.get("URL"), "") for r in batch if safe(r.get("URL"), "")]
    # ‡∏´‡∏≤ URL ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô
    existing_map = select_news_ids_by_urls(urls)

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° new_rows ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ URL ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    new_rows = []
    for r in batch:
        url = safe(r.get("URL"), "")
        if not url or url in existing_map:
            continue
        conf = safe(r.get("ConfidenceScore"), 0.0)
        try:
            conf = round(float(conf), 2)
        except Exception:
            conf = 0.0
        new_rows.append((
            safe(r.get("Title")),
            safe(r.get("Source")),
            safe(r.get("Sentiment")),
            conf,
            safe(r.get("PublishedDate")),
            safe(r.get("Content")),
            url,
            safe(r.get("Img"))
        ))

    # ‡πÉ‡∏™‡πà‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ö‡∏ö batch
    if new_rows:
        for sub in chunked(new_rows, 1000):
            cur.executemany(SQL_INSERT_NEWS, sub)
            conn.commit()
        inserted_news += len(new_rows)

    # ‡∏î‡∏∂‡∏á NewsID map ‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö (‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß + ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÅ‡∏ó‡∏£‡∏Å)
    news_map = select_news_ids_by_urls(urls)

    # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° NewsStock ‡πÄ‡∏õ‡πá‡∏ô batch ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    stock_pairs = []
    for r in batch:
        url = safe(r.get("URL"), "")
        nid = news_map.get(url)
        if not nid:
            continue
        ms = safe(r.get("MatchedStock"), "")
        if isinstance(ms, str) and ms.strip():
            for s in [x.strip() for x in ms.split(",") if x.strip()]:
                key = (nid, s)
                if key in seen_pairs:
                    continue
                seen_pairs.add(key)
                stock_pairs.append(key)

    # ‡πÉ‡∏™‡πà NewsStock ‡πÅ‡∏ö‡∏ö batch
    if stock_pairs:
        for sub in chunked(stock_pairs, BATCH_STOCK):
            cur.executemany(SQL_INSERT_NEWS_STOCK, sub)
            conn.commit()
        inserted_links += len(stock_pairs)

cur.close()
conn.close()

print(f"‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà {inserted_news:,} ‡πÄ‡∏£‡∏Ñ‡∏Ñ‡∏≠‡∏£‡πå‡∏î | ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏´‡∏∏‡πâ‡∏ô {inserted_links:,} ‡πÅ‡∏ñ‡∏ß (‡πÄ‡∏î‡∏î‡∏π‡∏Å‡πÉ‡∏ô‡∏£‡∏±‡∏ô‡∏ô‡∏µ‡πâ)")
