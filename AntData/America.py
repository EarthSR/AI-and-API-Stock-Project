import os
import time
import urllib.parse
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime

def scrape_abc_news():
    """
    à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¹ˆà¸²à¸§à¸ˆà¸²à¸ ABC News à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ Selenium + BeautifulSoup (à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§à¸ªà¸¹à¸‡)
    """
    # ğŸš€ à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Selenium à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹‚à¸«à¸¥à¸”à¹€à¸£à¹‡à¸§à¸—à¸µà¹ˆà¸ªà¸¸à¸”
    options = Options()
    options.add_argument('--headless')  # à¹„à¸¡à¹ˆà¹€à¸›à¸´à¸”à¸«à¸™à¹‰à¸²à¸•à¹ˆà¸²à¸‡à¹€à¸šà¸£à¸²à¸§à¹Œà¹€à¸‹à¸­à¸£à¹Œ
    options.add_argument('--disable-gpu')
    options.add_argument('--ignore-certificate-errors')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--blink-settings=imagesEnabled=false')  # à¸›à¸´à¸”à¸à¸²à¸£à¹‚à¸«à¸¥à¸”à¸£à¸¹à¸›à¸ à¸²à¸
    options.page_load_strategy = 'eager'  # à¹‚à¸«à¸¥à¸”à¸«à¸™à¹‰à¸²à¹„à¸§à¸‚à¸¶à¹‰à¸™

    # ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™ WebDriver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    # ğŸš€ à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸à¸·à¹‰à¸™à¸à¸²à¸™
    base_url = "https://abcnews.go.com"
    page = 1
    search_url = f'https://abcnews.go.com/search?searchtext=stock technology&sort=date&page={page}'
    driver.get(search_url)

    news_data = []
    file_name = 'D:/StockData/AI-and-API-Stock-Project/news_data/ABCNews.csv'

    # à¹‚à¸«à¸¥à¸”à¸‚à¹ˆà¸²à¸§à¸—à¸µà¹ˆà¹€à¸„à¸¢à¸šà¸±à¸™à¸—à¸¶à¸
    existing_titles = set()
    if os.path.exists(file_name):
        try:
            df_existing = pd.read_csv(file_name, usecols=['Title'])
            existing_titles = set(df_existing['Title'].astype(str))
        except Exception:
            pass  # à¸–à¹‰à¸²à¸¡à¸µà¸›à¸±à¸à¸«à¸²à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹à¸ªà¸”à¸‡ log

    try:
        while True:
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CLASS_NAME, 'ContentRoll__Headline'))
            )
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            articles = soup.select("section.ContentRoll__Item")

            if not articles:
                break  # à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹ˆà¸²à¸§à¹à¸¥à¹‰à¸§ à¸«à¸¢à¸¸à¸”

            for article in articles:
                try:
                    title_tag = article.select_one("div.ContentRoll__Headline a")
                    if not title_tag:
                        continue  # à¸‚à¹‰à¸²à¸¡à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸¥à¸´à¸‡à¸à¹Œà¸‚à¹ˆà¸²à¸§
                    title = title_tag.get_text(strip=True)
                    link = urllib.parse.urljoin(base_url, title_tag['href'])

                    if title in existing_titles or 'PHOTO:' in title or link.endswith('.jpg'):
                        continue  # à¸‚à¹‰à¸²à¸¡à¸‚à¹ˆà¸²à¸§à¸‹à¹‰à¸³à¸«à¸£à¸·à¸­à¸‚à¹ˆà¸²à¸§à¸£à¸¹à¸›à¸ à¸²à¸

                    date_tag = article.select_one("div.TimeStamp__Date")
                    date = date_tag.get_text(strip=True) if date_tag else 'No Date'

                    if 'hours ago' in date or 'minutes ago' in date:
                        date = datetime.today().strftime('%d %b %Y')
                    else:
                        date_obj = datetime.strptime(date.replace(',', ''), '%B %d %Y')
                        date = date_obj.strftime('%d %b %Y')

                    desc_tag = article.select_one("div.ContentRoll__Desc")
                    description = desc_tag.get_text(strip=True) if desc_tag else 'No Description'

                    news_data.append({
                        "Title": title.replace(',', ''),
                        "Link": link,
                        "Date": date,
                        "Description": description.replace(',', '')
                    })
                    existing_titles.add(title)

                except Exception:
                    continue  # à¸‚à¹‰à¸²à¸¡ error à¹à¸¥à¸°à¹„à¸›à¸•à¹ˆà¸­

            # ğŸ’¾ à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹ˆà¸²à¸§à¸—à¸¸à¸ à¹† 5 à¸‚à¹ˆà¸²à¸§
            if len(news_data) >= 5:
                df = pd.DataFrame(news_data)
                df.to_csv(file_name, mode='a', header=not os.path.exists(file_name), index=False)
                print(f"ğŸ’¾ Total News Saved: {len(existing_titles)}")
                news_data = []

            # ğŸš€ à¹„à¸›à¸«à¸™à¹‰à¸²à¸–à¸±à¸”à¹„à¸›
            page += 1
            next_url = f'https://abcnews.go.com/search?searchtext=Finance&sort=date&page={page}'
            driver.get(next_url)

    except Exception:
        pass  # à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹à¸ªà¸”à¸‡ error log
    finally:
        driver.quit()

        # ğŸ’¾ à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹ˆà¸²à¸§à¸—à¸µà¹ˆà¹€à¸«à¸¥à¸·à¸­
        if news_data:
            df = pd.DataFrame(news_data)
            df.to_csv(file_name, mode='a', header=not os.path.exists(file_name), index=False)
            print(f"ğŸ’¾ Total News Saved: {len(existing_titles)}")

        print("âœ… Scraping Completed.")

scrape_abc_news()
