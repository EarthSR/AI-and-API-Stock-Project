import os
import time
import urllib.parse
import pandas as pd
import requests
import pymysql
import torch
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from dotenv import load_dotenv

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .env
load_dotenv()
DB_CONFIG = {
    "host": os.getenv("DB_HOST"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "database": os.getenv("DB_NAME"),
    "port": int(os.getenv("DB_PORT"))
}

NEWS_CATEGORIES = {
    "Investment": "https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQppbnZlc3RtZW50DGNoYW5uZWxhbGlhcwEBXgEk",
    "Motoring": "https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQhtb3RvcmluZwxjaGFubmVsYWxpYXMBAV4BJA%3D%3D",
    "General": "https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQdnZW5lcmFsDGNoYW5uZWxhbGlhcwEBXgEk"
}

def is_news_exists(title, url):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    connection = pymysql.connect(**DB_CONFIG)
    cursor = connection.cursor()
    
    query = "SELECT COUNT(*) FROM News WHERE Title = %s OR URL = %s"
    cursor.execute(query, (title, url))
    result = cursor.fetchone()[0]

    cursor.close()
    connection.close()
    return result > 0  # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ True ‡∏ñ‡πâ‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß

def insert_news_to_db(df):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏á‡πÉ‡∏ô Database table: News ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥"""
    connection = pymysql.connect(**DB_CONFIG)
    cursor = connection.cursor()

    insert_query = """
        INSERT INTO News (Title, URL, PublishedDate, Content, Source, Sentiment, ConfidenceScore)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
    """

    new_entries = 0
    skipped_entries = 0

    try:
        for _, row in df.iterrows():
            if is_news_exists(row["Title"], row["URL"]):
                print(f"‚ö†Ô∏è ‡∏Ç‡πà‡∏≤‡∏ß '{row['Title']}' ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•! ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å...")
                skipped_entries += 1
                continue
            
            cursor.execute(insert_query, (
                row["Title"], row["URL"], row["PublishedDate"], row["Content"],
                row["Source"], row["Sentiment"], row["ConfidenceScore"]
            ))
            new_entries += 1

        connection.commit()
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà {new_entries} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏á‡πÉ‡∏ô Database ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!")
        print(f"‚ö†Ô∏è ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ã‡πâ‡∏≥: {skipped_entries} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

    except pymysql.Error as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
        connection.rollback()

    finally:
        cursor.close()
        connection.close()

chrome_options = Options()
chrome_options.add_argument('--headless')  
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument('--ignore-certificate-errors')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--blink-settings=imagesEnabled=false')  

def scrape_news(category, url, driver):
    """ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô DataFrame """
    print(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å {category}...")
    driver.get(url)

    news_data = []
    while True:
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, 'mk-listnew--title'))
            )
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            articles = soup.find_all('div', class_='mk-listnew--title')

            if not articles:
                print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏´‡∏°‡∏ß‡∏î {category}")
                break

            stop_scraping = False  

            for article in articles:
                try:
                    title_tag = article.find('h3').find('a')
                    title = title_tag.get_text(strip=True)
                    link = title_tag['href']

                    response = requests.get(link, timeout=5)
                    news_soup = BeautifulSoup(response.content, 'html.parser')

                    date_tag = news_soup.find('div', class_='article-info--col')
                    date_raw = date_tag.find('p', string=lambda x: x and 'PUBLISHED :' in x).get_text(strip=True).replace('PUBLISHED :', '').strip() if date_tag else 'No Date'

                    try:
                        published_date = datetime.strptime(date_raw, "%d %b %Y at %H:%M").strftime("%Y-%m-%d")
                    except Exception:
                        continue

                    if published_date < datetime.now().strftime("%Y-%m-%d"):
                        print(f"‚úÖ ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß {category} ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤")
                        stop_scraping = True
                        break  

                    news_data.append({
                        "Title": title,
                        "URL": link,          
                        "PublishedDate": published_date,    
                        "Source": "Bangkok Post"
                    })

                except Exception:
                    continue

            if stop_scraping:
                break

        except Exception:
            print(f"‚ùå ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß {category} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß")
            break

    return pd.DataFrame(news_data)

if __name__ == '__main__':
    # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• FinBERT **‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß**
    print("üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• FinBERT...")
    model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone").to("cpu")
    tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
    finbert_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=-1, truncation=True, max_length=512)
    print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!")

    # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô
    last_run_date = None  

    # ‚úÖ ‡πÄ‡∏£‡∏¥‡πà‡∏° loop ‡∏£‡∏±‡∏ô‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤
    while True:
        now = datetime.now()
        if now.hour == 0 and (last_run_date is None or last_run_date != now.date()):
            print("‚úÖ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô...")
            last_run_date = now.date()
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
            news_dataframes = [scrape_news(category, url, driver) for category, url in NEWS_CATEGORIES.items()]
            driver.quit()
            df_merged = pd.concat(news_dataframes, ignore_index=True)

            if not df_merged.empty:
                print("‚úÖ ‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥ Sentiment Analysis")
                sentiment_results = finbert_pipeline(df_merged["Title"].tolist())
                df_merged["Sentiment"] = [r["label"] for r in sentiment_results]
                df_merged["ConfidenceScore"] = [r["score"] for r in sentiment_results]
                insert_news_to_db(df_merged)

        time.sleep(60)  # ‚úÖ ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏∏‡∏Å 60 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
