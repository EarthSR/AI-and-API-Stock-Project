import os
import time
import urllib.parse
import pandas as pd
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager

def scrape_bangkok_post_selenium(query):
    # üîπ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Chrome options
    options = Options()
    options.add_argument('--headless')  # ‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå
    options.add_argument('--disable-gpu')
    options.add_argument('--ignore-certificate-errors')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--blink-settings=imagesEnabled=false')  # ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û

    # üîπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Chrome driver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    # üîπ URL ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß
    #base_url = f'https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQhidXNpbmVzcwxjaGFubmVsYWxpYXMBAV4BJA%3D%3D'
    #base_url = f'https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQppbnZlc3RtZW50DGNoYW5uZWxhbGlhcwEBXgEk'
    #base_url = f'https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQhtb3RvcmluZwxjaGFubmVsYWxpYXMBAV4BJA%3D%3D'
    base_url = f'https://search.bangkokpost.com/search/result?publishedDate=&q=&category=news&sort=newest&rows=10&refinementFilter=AQdnZW5lcmFsDGNoYW5uZWxhbGlhcwEBXgEk'
    driver.get(base_url)

    news_data = []
# üîπ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV
    #file_name = 'D:/Stock_Project/AI-and-API-Stock-Project/news_data/bangkok_post_news.csv'
    #file_name = 'D:/Stock_Project/AI-and-API-Stock-Project/news_data/bangkok_post_news2.csv'
    #file_name = 'D:/Stock_Project/AI-and-API-Stock-Project/news_data/bangkok_post_news3.csv'
    file_name = 'D:/Stock_Project/AI-and-API-Stock-Project/news_data/bangkok_post_news4.csv'

    # üîπ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß
    existing_titles = set()
    if os.path.exists(file_name):
        try:
            df_existing = pd.read_csv(file_name, usecols=['Title'])
            existing_titles = set(df_existing['Title'].astype(str))
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏î‡πâ: {e}")

    try:
        while True:
            # üîπ ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡πÇ‡∏´‡∏•‡∏î (‡∏•‡∏î Timeout ‡∏•‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CLASS_NAME, 'mk-listnew--title'))
                )
            except Exception:
                print("‚ùå Timeout: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ")
                break

            # üîπ ‡∏î‡∏∂‡∏á HTML ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            articles = soup.find_all('div', class_='mk-listnew--title')

            if not articles:
                print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß.")
                break

            # üîπ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß
            for article in articles:
                try:
                    title_tag = article.find('h3').find('a')
                    title = title_tag.get_text(strip=True)
                    link = title_tag['href']

                    # üîπ ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥
                    if title in existing_titles:
                        continue

                    # üîπ ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏à‡∏£‡∏¥‡∏á
                    if 'track/visitAndRedirect' in link:
                        url_params = urllib.parse.parse_qs(urllib.parse.urlparse(link).query)
                        real_link = url_params['href'][0]
                    else:
                        real_link = link

                    # üîπ ‡πÉ‡∏ä‡πâ requests ‡πÅ‡∏ó‡∏ô Selenium ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÇ‡∏´‡∏•‡∏î
                    try:
                        response = requests.get(real_link, timeout=5)
                        news_soup = BeautifulSoup(response.content, 'html.parser')
                    except requests.exceptions.RequestException:
                        print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πà‡∏≤‡∏ß: {title}")
                        continue

                    # üîπ ‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                    date_tag = news_soup.find('div', class_='article-info--col')
                    date = date_tag.find('p', string=lambda x: x and 'PUBLISHED :' in x).get_text(strip=True).replace('PUBLISHED :', '').strip() if date_tag else 'No Date'

                    # üîπ ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß
                    content_div = news_soup.find('div', class_='article-content')
                    paragraphs = content_div.find_all('p') if content_div else []
                    full_content = '\n'.join([p.get_text(strip=True) for p in paragraphs])

                    if not full_content:
                        full_content = 'Content not found'

                    # üîπ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡∏•‡∏á‡πÉ‡∏ô dataset
                    news_data.append({
                        "Title": title,
                        "Link": real_link,
                        "Date": date,
                        "Content": full_content
                    })
                    existing_titles.add(title)

                except Exception as e:
                    print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß: {e}")
                    continue

            # üîπ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏∏‡∏Å ‡πÜ 5 ‡∏Ç‡πà‡∏≤‡∏ß
            if len(news_data) >= 5:
                df = pd.DataFrame(news_data)
                df.to_csv(file_name, mode='a', header=not os.path.exists(file_name), index=False)
                print(f"üíæ Total News Saved: {len(existing_titles)}")
                news_data = []

            # üîπ ‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏•‡∏¥‡∏Å Next page
            next_page = soup.find('a', text='Next')
            if next_page and 'href' in next_page.attrs:
                driver.get(next_page['href'])
            else:
                break

        # üîπ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏à‡∏ö
        if news_data:
            df = pd.DataFrame(news_data)
            df.to_csv(file_name, mode='a', header=not os.path.exists(file_name), index=False)
            print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏µ‡∏Å {len(news_data)} ‡∏Ç‡πà‡∏≤‡∏ß")

    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
    finally:
        driver.quit()

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
query = 'Stock'
scrape_bangkok_post_selenium(query)
