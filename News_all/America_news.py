import os
import time
import urllib.parse
import pandas as pd
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime

def scrape_abc_news(query):
    """
    à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¹ˆà¸²à¸§à¸ˆà¸²à¸ ABC News à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ Selenium à¹à¸¥à¸° BeautifulSoup
    """
    print("ğŸ”¹ Initializing Web Scraper...")
    
    # ğŸ”¹ à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Chrome options
    options = Options()
    options.add_argument('--headless')  # à¸£à¸±à¸™à¹à¸šà¸šà¹„à¸¡à¹ˆà¹à¸ªà¸”à¸‡à¸«à¸™à¹‰à¸²à¸•à¹ˆà¸²à¸‡à¹€à¸šà¸£à¸²à¸§à¹Œà¹€à¸‹à¸­à¸£à¹Œ
    options.add_argument('--disable-gpu')
    options.add_argument('--ignore-certificate-errors')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--blink-settings=imagesEnabled=false')
    
    # ğŸ”¹ à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™ Chrome driver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    # ğŸ”¹ URL à¸‚à¸­à¸‡à¸«à¸™à¹‰à¸²à¸„à¹‰à¸™à¸«à¸²à¸‚à¹ˆà¸²à¸§
    page = 1
    base_url = f'https://abcnews.go.com/search?searchtext={query}&sort=date&page={page}'
    driver.get(base_url)
    
    print(f"ğŸ”¹ Accessing {base_url}")
    
    news_data = []
    file_name = '../news_data/ABCNews.csv'
    
    # ğŸ”¹ à¹‚à¸«à¸¥à¸”à¸‚à¹ˆà¸²à¸§à¸—à¸µà¹ˆà¹€à¸„à¸¢à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸›à¹à¸¥à¹‰à¸§
    existing_titles = set()
    if os.path.exists(file_name):
        try:
            df_existing = pd.read_csv(file_name, usecols=['Title'])
            existing_titles = set(df_existing['Title'].astype(str))
            print("âœ… Existing news loaded successfully.")
        except Exception as e:
            print(f"âš ï¸ Error loading existing CSV: {e}")
    
    try:
        while True:
            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CLASS_NAME, 'ContentRoll__Headline'))
                )
                print("âœ… Page loaded successfully.")
            except Exception:
                print("âŒ Timeout: Unable to load news on this page.")
                break
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            articles = soup.find_all('div', class_='ContentRoll__Headline')
            
            if not articles:
                print("âŒ No news articles found.")
                break
            
            for article in articles:
                try:
                    title_tag = article.find('a', class_='AnchorLink')
                    if not title_tag:
                        continue  # à¸‚à¹‰à¸²à¸¡à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸¥à¸´à¸‡à¸à¹Œà¸‚à¹ˆà¸²à¸§
                    title = title_tag.get_text(strip=True)
                    link = title_tag['href']
                    
                    if title in existing_titles or 'PHOTO:' in title or link.endswith('.jpg'):
                        continue  # à¸‚à¹‰à¸²à¸¡à¸‚à¹ˆà¸²à¸§à¸—à¸µà¹ˆà¸‹à¹‰à¸³à¸«à¸£à¸·à¸­à¹€à¸›à¹‡à¸™à¸‚à¹ˆà¸²à¸§à¸£à¸¹à¸›à¸ à¸²à¸à¸«à¸£à¸·à¸­à¹€à¸›à¹‡à¸™à¹„à¸Ÿà¸¥à¹Œà¸ à¸²à¸à¹‚à¸”à¸¢à¸•à¸£à¸‡
                    
                    real_link = urllib.parse.urljoin("https://abcnews.go.com", link)
                    
                    try:
                        driver.get(real_link)
                        WebDriverWait(driver, 15).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid="prism-article-body"]'))
                        )
                        news_soup = BeautifulSoup(driver.page_source, 'html.parser')
                    except Exception:
                        print(f"âš ï¸ Unable to fetch news: {title}")
                        continue
                    
                    date_tag = soup.find('div', class_='VZTD mLASH gpiba ')
                    if date_tag:
                        date_text = date_tag.find('div', class_='jTKbV zIIsP ZdbeE xAPpq QtiLO JQYD ').get_text(strip=True)
                    else:
                        date_text = 'No Date'
                    
                    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸«à¸²à¸à¸à¸šà¸„à¸³à¸§à¹ˆà¸² "hours ago" à¸«à¸£à¸·à¸­ "minutes ago" à¹ƒà¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸§à¸±à¸™à¸—à¸µà¹ˆ
                    if 'hours ago' in date_text or 'minutes ago' in date_text:
                        date = datetime.today().strftime('%d %b %Y')
                    else:
                        try:
                            # à¸¥à¸­à¸‡à¹à¸›à¸¥à¸‡à¸§à¸±à¸™à¸—à¸µà¹ˆ
                            date_obj = datetime.strptime(date_text.replace(',', ''), '%B %d %Y')
                            date = date_obj.strftime('%d %b %Y')
                        except ValueError:
                            # à¸«à¸²à¸à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹à¸›à¸¥à¸‡à¹„à¸”à¹‰
                            print(f"âš ï¸ Invalid date format: {date_text}. Setting date to 'No Date'.")
                            date = 'No Date'
                            
                    content_div = news_soup.find('div', {'data-testid': 'prism-article-body'})
                    paragraphs = content_div.find_all('p') if content_div else []
                    full_content = '\n'.join([p.get_text(strip=True).replace(',', ' ') for p in paragraphs])
                    
                    if not full_content:
                        full_content = 'Content not found'
                    
                    news_data.append({
                        "Title": title.replace(',', ' '),
                        "Link": real_link,
                        "Date": date,
                        "Description": full_content
                    })
                    existing_titles.add(title)
                    print(f"âœ… Scraped: {title}")
                
                except Exception as e:
                    print(f"âš ï¸ Error processing article: {e}")
                    continue
            
            # ğŸ”¹ à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹ˆà¸²à¸§à¸—à¸¸à¸ à¹† 5 à¸‚à¹ˆà¸²à¸§
            if len(news_data) >= 5:
                df = pd.DataFrame(news_data)
                df.to_csv(file_name, mode='a', header=not os.path.exists(file_name), index=False)
                print(f"ğŸ’¾ Total News Saved: {len(existing_titles)}")
                news_data = []
            
            # ğŸ”¹ à¹„à¸›à¸«à¸™à¹‰à¸²à¸–à¸±à¸”à¹„à¸›
            page += 1
            next_url = f'https://abcnews.go.com/search?searchtext={query}&sort=date&page={page}'
            driver.get(next_url)
            print(f"â¡ï¸ Navigating to next page: {next_url}")
        
        # ğŸ”¹ à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹ˆà¸²à¸§à¸—à¸µà¹ˆà¹€à¸«à¸¥à¸·à¸­
        if news_data:
            df = pd.DataFrame(news_data)
            df.to_csv(file_name, mode='a', header=not os.path.exists(file_name), index=False)
            print(f"âœ… Saved remaining {len(news_data)} news articles.")
        
    except Exception as e:
        print(f"âŒ Error occurred: {e}")
    finally:
        driver.quit()
        print("ğŸ›‘ Scraper Stopped.")

query = 'stock'
scrape_abc_news(query)
