import os
import time
import urllib.parse
import pandas as pd
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime, timedelta

def scrape_abc_news(query):
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å ABC News ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Selenium ‡πÅ‡∏•‡∏∞ BeautifulSoup
    ‡∏ñ‡πâ‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏°‡∏≤‡∏à‡∏≤‡∏Å Good Morning America ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°
    """
    print("üîπ Initializing Web Scraper...")
    
    # üîπ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Chrome options
    options = Options()
    options.add_argument('--headless')  # ‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå
    options.add_argument('--disable-gpu')
    options.add_argument('--ignore-certificate-errors')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--blink-settings=imagesEnabled=false')
    
    # üîπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Chrome driver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    # üîπ URL ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß
    page = 1
    base_url = f'https://abcnews.go.com/search?searchtext={query}&sort=date&page={page}'
    driver.get(base_url)
    
    print(f"üîπ Accessing {base_url}")
    
    news_data = []
    file_name = '../news_data/ABCNews.csv'
    
    # üîπ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß
    existing_titles = set()
    if os.path.exists(file_name):
        try:
            df_existing = pd.read_csv(file_name, usecols=['Title'])
            existing_titles = set(df_existing['Title'].astype(str))
            print("‚úÖ Existing news loaded successfully.")
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading existing CSV: {e}")
    
    try:
        while True:
            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CLASS_NAME, 'ContentRoll__Headline'))
                )
                print("‚úÖ Page loaded successfully.")
            except Exception:
                print("‚ùå Timeout: Unable to load news on this page.")
                break
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            articles = soup.find_all('div', class_='ContentRoll__Headline')
            
            if not articles:
                print("‚ùå No news articles found.")
                break
            
            for article in articles:
                try:
                    title_tag = article.find('a', class_='AnchorLink')
                    if not title_tag:
                        continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ç‡πà‡∏≤‡∏ß
                    title = title_tag.get_text(strip=True)
                    link = title_tag['href']
                    
                    if title in existing_titles or 'PHOTO:' in title or link.endswith('.jpg'):
                        continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                    
                    real_link = urllib.parse.urljoin("https://abcnews.go.com", link)
                    
                    try:
                        if "/video" in real_link:
                            print(f"‚ö†Ô∏è Detected video link: {real_link}")
                            continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
                        driver.get(real_link)
                        WebDriverWait(driver, 15).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid="prism-article-body"]'))
                        )
                        news_soup = BeautifulSoup(driver.page_source, 'html.parser')
                    except Exception:
                        print(f"‚ö†Ô∏è Unable to fetch news: {title}")
                        continue
                    
                    
                    # ‡∏ñ‡πâ‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏°‡∏≤‡∏à‡∏≤‡∏Å Good Morning America ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏õ
                    if "goodmorningamerica" in real_link:
                        print(f"‚ö†Ô∏è Skipping news from goodmorningamerica: {title}")
                        continue
                    
                    # ‡∏ñ‡πâ‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å goodmorningamerica ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ
                    date_tag = news_soup.find('time')  # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å <time> tag ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                    if not date_tag:
                        date_tag = soup.find('div', class_='TimeStamp__Date')  # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö, ‡πÉ‡∏ä‡πâ div ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
                    date_text = date_tag.get_text(strip=True) if date_tag else 'No Date'
                    
                    # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö "hours ago" ‡∏´‡∏£‡∏∑‡∏≠ "minutes ago" ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á
                    if 'hour' in date_text or 'minute' in date_text:
                        # ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤
                        time_ago = int(date_text.split()[0])  # ‡∏î‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å "X hours ago" ‡∏´‡∏£‡∏∑‡∏≠ "X minutes ago"
                        date = datetime.today() - timedelta(hours=time_ago)
                        date = date.strftime('%d %b %Y')
                    else:
                        try:
                            date_obj = datetime.strptime(date_text.replace(',', ''), '%B %d %Y')
                            date = date_obj.strftime('%d %b %Y')
                        except ValueError:
                            print(f"‚ö†Ô∏è Invalid date format: {date_text}. Setting date to 'No Date'.")
                            date = 'No Date'
                    
                    content_div = news_soup.find('div', {'data-testid': 'prism-article-body'})
                    paragraphs = content_div.find_all('p') if content_div else []
                    full_content = '\n'.join([p.get_text(strip=True).replace(',', '') for p in paragraphs])
                    
                    if not full_content:
                        full_content = 'Content not found'
                    
                    news_data.append({
                        "Title": title.replace(',', ''),
                        "Link": real_link,
                        "Date": date,
                        "Description": full_content
                    })
                    existing_titles.add(title)
                    print(f"‚úÖ Scraped: {title}")
                
                except Exception as e:
                    print(f"‚ö†Ô∏è Error processing article: {e}")
                    continue
            
            # üîπ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏∏‡∏Å ‡πÜ 5 ‡∏Ç‡πà‡∏≤‡∏ß
            if len(news_data) >= 5:
                df = pd.DataFrame(news_data)
                df.to_csv(file_name, mode='a', header=not os.path.exists(file_name), index=False)
                print(f"üíæ Total News Saved: {len(existing_titles)}")
                news_data = []
            
            # üîπ ‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            page += 1
            next_url = f'https://abcnews.go.com/search?searchtext={query}&sort=date&page={page}'
            driver.get(next_url)
            print(f"‚û°Ô∏è Navigating to next page: {next_url}")
        
        # üîπ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
        if news_data:
            df = pd.DataFrame(news_data)
            df.to_csv(file_name, mode='a', header=not os.path.exists(file_name), index=False)
            print(f"‚úÖ Saved remaining {len(news_data)} news articles.")
        
    except Exception as e:
        print(f"‚ùå Error occurred: {e}")
    finally:
        driver.quit()
        print("üõë Scraper Stopped.")

query = 'stock technology'
scrape_abc_news(query)
